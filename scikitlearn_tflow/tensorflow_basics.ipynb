{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "d141de91-bf9d-44e6-b993-bb784587696c",
    "_uuid": "96f615e02ee6de6ddb9d3f85e3affd4fe636f002"
   },
   "source": [
    "# Intro to TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from math import ceil\n",
    "from numpy.random import seed, shuffle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "001abf7d-b936-4cf5-a807-10a7c8bc7df0",
    "_uuid": "a770a6a8638c467001263bff555142c8ff063c6b"
   },
   "source": [
    "**My first computation Graph**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52\n"
     ]
    }
   ],
   "source": [
    "# Defining a graph (a default graph)\n",
    "# Every declared node is automatically added \n",
    "# to the 'default-graph'\n",
    "x = tf.Variable(3, name=\"x\")\n",
    "y = tf.Variable(5, name=\"y\")\n",
    "f = x * x * y + y + 2\n",
    "# Run the graph inside a session.\n",
    "# With the 'with' command, the session is set as the\n",
    "# default sesion\n",
    "with tf.Session() as sess:\n",
    "    # We need to initialize the variables before\n",
    "    # performing operations using them\n",
    "    x.initializer.run() # Equivalent to: tf.get_default_session().run(x.initializer)\n",
    "    y.initializer.run() # Equivalent to: tf.get_default_session().run(y.initializer)\n",
    "    result = f.eval()   # Equivalent to: tf.get_default_session().run(f)\n",
    "    \n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "338b90a2-bc73-4f4d-bba4-1a0faebee202",
    "_uuid": "a149a0d1c964cbc0c6899d0415590dd2b9a2381f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52\n"
     ]
    }
   ],
   "source": [
    "# Removing every node inside the\n",
    "# default graph\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# **CONSTRUCTION PHASE**\n",
    "x = tf.Variable(3, name=\"x\")\n",
    "y = tf.Variable(5, name=\"y\")\n",
    "f = x * x * y + y + 2\n",
    "# Add to the graph a step to initialize all variables\n",
    "# (we are not actually initializing the variables in this step)\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# **EXECUTION PHASE**\n",
    "with tf.Session() as ses:\n",
    "    init.run()\n",
    "    result = f.eval()\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b2f81670-4bcb-4eb0-b2cc-8c4086b681c2",
    "_uuid": "2a4e49456415951dc2444caec922e0aa8bef091a"
   },
   "source": [
    "### Evaluating nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_cell_guid": "e25499c1-fb86-4a68-a4bb-089f7ed6e3cb",
    "_uuid": "520a411549831ee1c2bb61102de782b9723bb3d3",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "15\n",
      "\n",
      "10\n",
      "15\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "# Whenever we evaluate a node, Tensorflow automatically\n",
    "# determines the set of nodes that it depends on.\n",
    "\n",
    "w  = tf.constant(3)\n",
    "x = w + 2\n",
    "y = x + 5\n",
    "z = x * 3\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "# **The ineficient way to evaluate a set of nodes**\n",
    "# By evaluating (y, z) (nodes) the following way, TensorFlow\n",
    "# has to compute 'w' and 'x' twice in order to obtain (x, z)\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    print(y.eval())\n",
    "    print(z.eval())\n",
    "    \n",
    "tf.reset_default_graph()\n",
    "w  = tf.constant(3)\n",
    "x = w + 2\n",
    "y = x + 5\n",
    "z = x * 3\n",
    "\n",
    "print()\n",
    "init = tf.global_variables_initializer()\n",
    "# **The proper way to evaluate a set of nodes**\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    xres, yres = sess.run([y, z])\n",
    "    print(xres)\n",
    "    print(yres)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "d2f311a6-a51e-4fe2-af22-34d940d730cc",
    "_uuid": "09e054b3fe9210414fc815e7e56cbe5196f65bd9"
   },
   "source": [
    "Operations in TensorFlow are called *ops*, `tf.constant` and `tf.Variable` are called *source ops* since they take no input. *Ops* with $n$ inputs and $m$ outputs are called *tensors*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "f1add54c-3430-45ea-b6f4-fb234d8d8896",
    "_uuid": "7f09c7db5d86a04ea651dc5561ad4dee57d87550"
   },
   "source": [
    "### Computing $\\theta^\\star$ with the normal equation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_cell_guid": "f3980145-8037-4f84-aa91-0dc149911f35",
    "_uuid": "13218dd14974bd75a0ab087e382d5c1e451b903f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[_DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 268435456)]\n",
      "[[ -3.60324050e+06]\n",
      " [ -4.29331445e+04]\n",
      " [ -4.26845234e+04]\n",
      " [  1.15434619e+03]\n",
      " [ -8.20419979e+00]\n",
      " [  1.13781357e+02]\n",
      " [ -3.83973656e+01]\n",
      " [  4.75008698e+01]\n",
      " [  4.02665078e+04]]\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "housing = pd.read_csv(\"./datasets/housing.csv\").dropna()\n",
    "m, n = housing.shape\n",
    "housing_data_bias = np.c_[np.ones((m, 1)), housing.drop([\"ocean_proximity\", \"median_house_value\"], axis=1).values]\n",
    "housing_target = housing.median_house_value.values.reshape(-1, 1)\n",
    "\n",
    "# Defining the computation graph \n",
    "X = tf.constant(housing_data_bias, dtype=tf.float32, name=\"X\")\n",
    "y = tf.constant(housing_target, dtype=tf.float32, name=\"y\")\n",
    "Xtranspose = tf.transpose(X)\n",
    "theta = tf.matrix_inverse(Xtranspose @ X) @ Xtranspose @ y\n",
    "\n",
    "with tf.Session(config=tf.ConfigProto(log_device_placement=True)) as sess:\n",
    "    print(sess.list_devices())\n",
    "    theta_star = theta.eval() # Equivalent to sess.run(theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "2db47547-c58e-43b6-b34c-d29dc7d61dde",
    "_uuid": "67630970e5a58295f0da3023bb31410b2ff7f53a"
   },
   "source": [
    "### Computing $\\theta^\\star$ with Gradient Descent\n",
    "#### Manually Setting the gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_cell_guid": "e4744113-5c34-4859-a983-aa5e742b40b1",
    "_uuid": "751ae2e3ed8bd5eca6546dcd48ce685d28f7221f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@Epoch 000, MSE: 56,117,448,704.00\n",
      "@Epoch 100, MSE: 47,701,590,016.00\n",
      "@Epoch 200, MSE: 47,639,318,528.00\n",
      "@Epoch 300, MSE: 47,633,567,744.00\n",
      "@Epoch 400, MSE: 47,632,191,488.00\n",
      "@Epoch 500, MSE: 47,631,593,472.00\n",
      "@Epoch 600, MSE: 47,631,310,848.00\n",
      "@Epoch 700, MSE: 47,631,142,912.00\n",
      "@Epoch 800, MSE: 47,631,052,800.00\n",
      "@Epoch 900, MSE: 47,630,999,552.00\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "tf.reset_default_graph()\n",
    "\n",
    "housing_data_bias_scaled = StandardScaler().fit_transform(housing_data_bias)\n",
    "\n",
    "n_epochs = 1000\n",
    "learning_rate = 0.1\n",
    "X = tf.constant(housing_data_bias_scaled, dtype=tf.float32, name=\"X\")\n",
    "y = tf.constant(housing.median_house_value.values.reshape(-1, 1), dtype=tf.float32, name=\"y\")\n",
    "theta = tf.Variable(tf.random_uniform([n - 1, 1], minval=-1, maxval=1), name=\"theta\")\n",
    "\n",
    "y_pred = tf.matmul(X, theta, name=\"predictions\")\n",
    "error = y_pred - y\n",
    "mse = tf.reduce_mean(tf.square(error), name=\"MSE\")\n",
    "# **Taking a step of gradient descent**\n",
    "# ---------------- Manual Differentiation----------------\n",
    "gradients = 2 / m * tf.matmul(tf.transpose(X), error)\n",
    "# Updating the parameters\n",
    "training_op = tf.assign(theta, theta - learning_rate * gradients)\n",
    "# ---------------- Manual Differentiation----------------\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # Step 1) Initialize all variables\n",
    "    sess.run(init)\n",
    "    # Step 2) Iterate over epochs\n",
    "    for epoch in range(n_epochs):\n",
    "        if epoch % 100 == 0:\n",
    "            # Evaluate current mse variable\n",
    "            print(f\"@Epoch {epoch:03}, MSE: {mse.eval():0,.2f}\")\n",
    "        # update the training operation: compute gradients\n",
    "        sess.run(training_op)\n",
    "    theta_star = theta.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "2db47547-c58e-43b6-b34c-d29dc7d61dde",
    "_uuid": "67630970e5a58295f0da3023bb31410b2ff7f53a"
   },
   "source": [
    "### Computing $\\theta^\\star$ with Gradient Descent\n",
    "#### Using automatic differentiation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "_cell_guid": "932de405-2061-4366-ad6a-bdd721907c96",
    "_uuid": "cd6350ae680df9075871658a1519552ac13e0e0b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@Epoch 000, MSE: 56,117,559,296.00\n",
      "@Epoch 100, MSE: 47,701,585,920.00\n",
      "@Epoch 200, MSE: 47,639,318,528.00\n",
      "@Epoch 300, MSE: 47,633,567,744.00\n",
      "@Epoch 400, MSE: 47,632,191,488.00\n",
      "@Epoch 500, MSE: 47,631,593,472.00\n",
      "@Epoch 600, MSE: 47,631,306,752.00\n",
      "@Epoch 700, MSE: 47,631,142,912.00\n",
      "@Epoch 800, MSE: 47,631,052,800.00\n",
      "@Epoch 900, MSE: 47,630,999,552.00\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "tf.reset_default_graph()\n",
    "\n",
    "housing_data_bias_scaled = StandardScaler().fit_transform(housing_data_bias)\n",
    "\n",
    "n_epochs = 1000\n",
    "learning_rate = 0.1\n",
    "X = tf.constant(housing_data_bias_scaled, dtype=tf.float32, name=\"X\")\n",
    "y = tf.constant(housing.median_house_value.values.reshape(-1, 1), dtype=tf.float32, name=\"y\")\n",
    "# Initializing the parameters to learn\n",
    "theta = tf.Variable(tf.random_uniform([n - 1, 1], minval=-1, maxval=1), name=\"theta\")\n",
    "\n",
    "y_pred = tf.matmul(X, theta, name=\"predictions\")\n",
    "error = y_pred - y\n",
    "mse = tf.reduce_mean(tf.square(error), name=\"MSE\")\n",
    "# **Taking a step of gradient descent**\n",
    "# ---------------- Automatic Differentiation----------------\n",
    "gradients = tf.gradients(mse, [theta])[0]\n",
    "training_op = tf.assign(theta, theta - learning_rate * gradients)\n",
    "# ---------------- Automatic Differentiation----------------\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # Step 1) Initialize all variables\n",
    "    sess.run(init)\n",
    "    # Step 2) Iterate over epochs\n",
    "    for epoch in range(n_epochs):\n",
    "        if epoch % 100 == 0:\n",
    "            # Evaluate current mse variable\n",
    "            print(f\"@Epoch {epoch:03}, MSE: {mse.eval():0,.2f}\")\n",
    "        # update the training operation: comppute gradients\n",
    "        sess.run(training_op)\n",
    "    theta_star = theta.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "2db47547-c58e-43b6-b34c-d29dc7d61dde",
    "_uuid": "67630970e5a58295f0da3023bb31410b2ff7f53a"
   },
   "source": [
    "### Computing $\\theta^\\star$ with Gradient Descent\n",
    "#### Using an Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_cell_guid": "a2359d21-1389-4a12-b745-443cb4b14c7e",
    "_uuid": "d05aa937551a5e50aa576086ef53e6fd9bea4b02"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@Epoch 000, MSE: 56,117,612,544.00\n",
      "@Epoch 100, MSE: 47,701,594,112.00\n",
      "@Epoch 200, MSE: 47,639,330,816.00\n",
      "@Epoch 300, MSE: 47,633,571,840.00\n",
      "@Epoch 400, MSE: 47,632,187,392.00\n",
      "@Epoch 500, MSE: 47,631,601,664.00\n",
      "@Epoch 600, MSE: 47,631,306,752.00\n",
      "@Epoch 700, MSE: 47,631,142,912.00\n",
      "@Epoch 800, MSE: 47,631,052,800.00\n",
      "@Epoch 900, MSE: 47,631,003,648.00\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "tf.reset_default_graph()\n",
    "\n",
    "housing_data_bias_scaled = StandardScaler().fit_transform(housing_data_bias)\n",
    "\n",
    "n_epochs = 1000\n",
    "learning_rate = 0.1\n",
    "X = tf.constant(housing_data_bias_scaled, dtype=tf.float32, name=\"X\")\n",
    "y = tf.constant(housing.median_house_value.values.reshape(-1, 1), dtype=tf.float32, name=\"y\")\n",
    "theta = tf.Variable(tf.random_uniform([n - 1, 1], minval=-1, maxval=1), name=\"theta\")\n",
    "\n",
    "y_pred = tf.matmul(X, theta, name=\"predictions\")\n",
    "error = y_pred - y\n",
    "mse = tf.reduce_mean(tf.square(error), name=\"MSE\")\n",
    "# **Taking a step of gradient descent**\n",
    "# ---------------- TF Optimizer ----------------\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "training_op = optimizer.minimize(mse)\n",
    "# ---------------- TF Optimizer ----------------\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # Step 1) Initialize all variables\n",
    "    sess.run(init)\n",
    "    # Step 2) Iterate over epochs\n",
    "    for epoch in range(n_epochs):\n",
    "        if epoch % 100 == 0:\n",
    "            # Evaluate current mse variable\n",
    "            print(f\"@Epoch {epoch:03}, MSE: {mse.eval():0,.2f}\")\n",
    "        # update the training operation: comppute gradients\n",
    "        sess.run(training_op)\n",
    "    theta_star = theta.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "3d2598f2-ba37-48f3-b487-0f7ef6406189",
    "_uuid": "04565e08879092c892ac6bf1b2b7d6945c6cb6d0"
   },
   "source": [
    "----\n",
    "### Computing $\\theta^\\star$ with Mini-Batch Gradient Descent\n",
    "#### The use of Placeholders for model learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "_cell_guid": "9bef903a-b472-4a3f-9d3b-b6e2f8985503",
    "_uuid": "36861c7ed1129cb749f9c3fab8e2963db5b84ffe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@Epoch 000, MSE: 49,532,727,296.00\n",
      "@Epoch 100, MSE: 46,844,350,464.00\n",
      "@Epoch 200, MSE: 47,022,866,432.00\n",
      "@Epoch 300, MSE: 47,475,621,888.00\n",
      "@Epoch 400, MSE: 47,403,692,032.00\n",
      "@Epoch 500, MSE: 47,448,813,568.00\n",
      "@Epoch 600, MSE: 46,916,739,072.00\n",
      "@Epoch 700, MSE: 47,627,169,792.00\n",
      "@Epoch 800, MSE: 47,496,564,736.00\n",
      "@Epoch 900, MSE: 47,253,815,296.00\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "batch_size = 5000\n",
    "n_epochs = 1000\n",
    "n_batches = ceil(m / batch_size)\n",
    "def fetch_batch(epoch, n_batches):\n",
    "    \"\"\"\n",
    "    Retrieve the i-th batch from a random shuffled\n",
    "    training dataset. Each epoch the training\n",
    "    dataset gets reshuffled.\n",
    "    \"\"\"\n",
    "    seed(epoch)\n",
    "    batches = np.c_[housing_data_bias_scaled, housing_target]\n",
    "    shuffle(batches)\n",
    "    batches = np.array_split(batches, n_batches)\n",
    "    for batch in batches:\n",
    "        yield batch[:, :-1], batch[:, -1].reshape(-1, 1)\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=[None, n - 1], name=\"X\")\n",
    "y = tf.placeholder(tf.float32, shape=[None, 1], name=\"y\")\n",
    "theta = tf.Variable(tf.random_uniform([n - 1, 1], minval=-1, maxval=1), name=\"theta\")\n",
    "\n",
    "y_pred = tf.matmul(X, theta)\n",
    "err = y_pred - y\n",
    "mse = tf.reduce_mean(tf.square(err), name=\"MSE\")\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "training_op = optimizer.minimize(mse)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for epoch in range(n_epochs):\n",
    "        for X_batch, y_batch in fetch_batch(epoch, n_batches):\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        if epoch % 100 == 0:\n",
    "            # Evaluate current mse variable\n",
    "            print(f\"@Epoch {epoch:03}, MSE: {mse.eval(feed_dict={X: X_batch, y: y_batch}):0,.2f}\")\n",
    "    theta_star_bgd = theta.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "3d2598f2-ba37-48f3-b487-0f7ef6406189",
    "_uuid": "04565e08879092c892ac6bf1b2b7d6945c6cb6d0"
   },
   "source": [
    "----\n",
    "### Saving and restoring models\n",
    "#### *Saving a file*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "9bef903a-b472-4a3f-9d3b-b6e2f8985503",
    "_uuid": "36861c7ed1129cb749f9c3fab8e2963db5b84ffe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@Epoch 000, MSE: 49,532,690,432.00\n",
      "@Epoch 100, MSE: 46,844,346,368.00\n",
      "@Epoch 200, MSE: 47,022,866,432.00\n",
      "@Epoch 300, MSE: 47,475,621,888.00\n",
      "@Epoch 400, MSE: 47,403,692,032.00\n",
      "@Epoch 500, MSE: 47,448,813,568.00\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "batch_size = 5000\n",
    "n_epochs = 1000\n",
    "n_batches = ceil(m / batch_size)\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=[None, n - 1], name=\"X\")\n",
    "y = tf.placeholder(tf.float32, shape=[None, 1], name=\"y\")\n",
    "theta = tf.Variable(tf.random_uniform([n - 1, 1], minval=-1, maxval=1), name=\"theta\")\n",
    "\n",
    "y_pred = tf.matmul(X, theta)\n",
    "err = y_pred - y\n",
    "mse = tf.reduce_mean(tf.square(err), name=\"MSE\")\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "training_op = optimizer.minimize(mse)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for epoch in range(n_epochs):\n",
    "        for X_batch, y_batch in fetch_batch(epoch, n_batches):\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        if epoch % 100 == 0:\n",
    "            # Checkpoint every 100 epochs\n",
    "            save_path = saver.save(sess, \"/tmp/linear_regression.ckpt\")\n",
    "            # Evaluate current mse variable\n",
    "            print(f\"@Epoch {epoch:03}, MSE: {mse.eval(feed_dict={X: X_batch, y: y_batch}):0,.2f}\")\n",
    "    theta_star_bgd = theta.eval()\n",
    "    save_path = saver.save(sess, \"/tmp/my_model_final.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "3d2598f2-ba37-48f3-b487-0f7ef6406189",
    "_uuid": "04565e08879092c892ac6bf1b2b7d6945c6cb6d0"
   },
   "source": [
    "----\n",
    "### Saving and restoring models\n",
    "#### *Restoring a File*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /tmp/my_model_final.ckpt\n",
      "@Epoch 000, MSE: 47,483,203,584.00\n",
      "@Epoch 100, MSE: 46,842,966,016.00\n",
      "@Epoch 200, MSE: 47,023,616,000.00\n",
      "@Epoch 300, MSE: 47,475,777,536.00\n",
      "@Epoch 400, MSE: 47,403,679,744.00\n",
      "@Epoch 500, MSE: 47,448,821,760.00\n",
      "@Epoch 600, MSE: 46,916,743,168.00\n",
      "@Epoch 700, MSE: 47,627,169,792.00\n",
      "@Epoch 800, MSE: 47,496,560,640.00\n",
      "@Epoch 900, MSE: 47,253,819,392.00\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "X = tf.placeholder(tf.float32, shape=[None, n - 1], name=\"X\")\n",
    "y = tf.placeholder(tf.float32, shape=[None, 1], name=\"y\")\n",
    "theta = tf.Variable(tf.random_uniform([n - 1, 1 ], minval=-1, maxval=1), name=\"theta\")\n",
    "\n",
    "y_pred = tf.matmul(X, theta, name=\"y_pred\")\n",
    "mse = tf.reduce_mean(tf.square(y_pred - y), name=\"MSE\")\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "training_op = optimizer.minimize(mse)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, \"/tmp/my_model_final.ckpt\")\n",
    "    for epoch in range(n_epochs):\n",
    "        for batch_ix in range(n_batches):\n",
    "            X_batch, y_batch = fetch_batch(epoch, batch_ix, batch_size)\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        if epoch % 100 == 0:\n",
    "            # Checkpoint every 100 epochs\n",
    "            save_path = saver.save(sess, \"/tmp/linear_regression.ckpt\")\n",
    "            # Evaluate current mse variable\n",
    "            print(f\"@Epoch {epoch:03}, MSE: {mse.eval(feed_dict={X: X_batch, y: y_batch}):0,.2f}\")\n",
    "    theta_star_bgd = theta.eval()\n",
    "    save_path = saver.save(sess, \"/tmp/my_model_final.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@Epoch 000, MSE: 54,531,899,392.00\n",
      "@Epoch 100, MSE: 47,067,054,080.00\n",
      "@Epoch 200, MSE: 47,152,381,952.00\n",
      "@Epoch 300, MSE: 47,476,203,520.00\n",
      "@Epoch 400, MSE: 47,412,080,640.00\n",
      "@Epoch 500, MSE: 47,469,412,352.00\n",
      "@Epoch 600, MSE: 46,948,286,464.00\n",
      "@Epoch 700, MSE: 47,646,089,216.00\n",
      "@Epoch 800, MSE: 47,510,441,984.00\n",
      "@Epoch 900, MSE: 47,261,286,400.00\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "learning_rate = 0.01\n",
    "now = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
    "root = \"tf_logs\"\n",
    "logdir = f\"{root}/run-{now}\"\n",
    "\n",
    "tf.reset_default_graph()\n",
    "batch_size = 5000\n",
    "n_epochs = 1000\n",
    "n_batches = ceil(m / batch_size)\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=[None, n - 1], name=\"X\")\n",
    "y = tf.placeholder(tf.float32, shape=[None, 1], name=\"y\")\n",
    "theta = tf.Variable(tf.random_uniform([n - 1, 1], minval=-1, maxval=1), name=\"theta\")\n",
    "\n",
    "y_pred = tf.matmul(X, theta)\n",
    "# Name scope: To group relating nodes into a single\n",
    "# *graphical* node\n",
    "with tf.name_scope(\"loss\") as scope:\n",
    "    err = y_pred - y\n",
    "    mse = tf.reduce_mean(tf.square(err), name=\"MSE\")\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "training_op = optimizer.minimize(mse)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "mse_summary = tf.summary.scalar(\"MSE\", mse)\n",
    "file_writer = tf.summary.FileWriter(logdir, tf.get_default_graph())\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for epoch in range(n_epochs):\n",
    "        for batch_ix in range(n_batches):\n",
    "            X_batch, y_batch = fetch_batch(epoch, batch_ix, batch_size)\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "            if batch_ix % 10 == 0:\n",
    "                summary_str = mse_summary.eval(feed_dict={X: X_batch, y:y_batch})\n",
    "                step = epoch * n_batches + batch_ix\n",
    "                file_writer.add_summary(summary_str, step )\n",
    "        if epoch % 100 == 0:\n",
    "            # Checkpoint every 100 epochs\n",
    "            save_path = saver.save(sess, \"/tmp/linear_regression.ckpt\")\n",
    "            # Evaluate current mse variable\n",
    "            print(f\"@Epoch {epoch:03}, MSE: {mse.eval(feed_dict={X: X_batch, y: y_batch}):0,.2f}\")\n",
    "    theta_star_bgd = theta.eval()\n",
    "    save_path = saver.save(sess, \"/tmp/my_model_final.ckpt\")\n",
    "file_writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modularity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "now = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
    "root = \"tf_logs\"\n",
    "logdir = f\"{root}/run-{now}\"\n",
    "\n",
    "def relu(X):\n",
    "    with tf.name_scope(\"relu\"):\n",
    "        w_shape = (int(X.get_shape()[1]), 1)\n",
    "        w = tf.Variable(tf.random_normal(w_shape), name=\"weights\")\n",
    "        b = tf.Variable(0., name=\"bias\")\n",
    "        z = tf.add(tf.matmul(X, w), b, name=\"z\")\n",
    "        return tf.maximum(z, 0., name=\"relu\")\n",
    "\n",
    "n_features = 3\n",
    "X = tf.placeholder(tf.float32, shape=[None, n_features], name=\"X\")\n",
    "relus = [relu(X) for _ in range(5)]\n",
    "output = tf.add_n(relus, name=\"output\")\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "file_writer = tf.summary.FileWriter(logdir, tf.get_default_graph())\n",
    "file_writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sharing Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "now = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
    "root = \"tf_logs\"\n",
    "logdir = f\"{root}/run-{now}\"\n",
    "\n",
    "def relu(X):\n",
    "    with tf.variable_scope(\"relu\", reuse=True):\n",
    "        threshold = tf.get_variable(\"threshold\")\n",
    "        w_shape = (int(X.get_shape()[1]), 1)\n",
    "        w = tf.Variable(tf.random_normal(w_shape), name=\"weights\")\n",
    "        b = tf.Variable(0., name=\"bias\")\n",
    "        z = tf.add(tf.matmul(X, w), b, name=\"z\")\n",
    "        return tf.maximum(z, 0., name=\"relu\")\n",
    "\n",
    "n_features = 3\n",
    "X = tf.placeholder(tf.float32, shape=[None, n_features], name=\"X\")\n",
    "with tf.variable_scope(\"relu\"):\n",
    "    threshold = tf.get_variable(\"threshold\", shape=(),\n",
    "                                initializer=tf.constant_initializer(0.0))\n",
    "relus = [relu(X) for _ in range(5)]\n",
    "output = tf.add_n(relus, name=\"output\")\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "file_writer = tf.summary.FileWriter(logdir, tf.get_default_graph())\n",
    "file_writer.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
