{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro ANNs\n",
    "## In TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.contrib.layers import fully_connected\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "mnist = input_data.read_data_sets(\"/tmp/data/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1. Building the Computation Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2018_05_25'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datetime.now().strftime(\"%Y_%m_%d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "now = datetime.now().strftime(\"%Y%m%d\")\n",
    "outfile = f\"./tf_logs/run-{now}\"\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "n_inputs = 28 * 28\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 100\n",
    "n_outputs = 10\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int32, shape=(None), name=\"y\")\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = fully_connected(X, n_hidden1, activation_fn=tf.nn.elu, \n",
    "                              scope=\"hidden1\")\n",
    "    hidden2 = fully_connected(hidden1, n_hidden2, activation_fn=tf.nn.elu,\n",
    "                              scope=\"hidden2\")\n",
    "    logits = fully_connected(hidden2, n_outputs, scope=\"outputs\",\n",
    "                             activation_fn=None)\n",
    "\n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "                labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "\n",
    "learning_rate = 0.01\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    train_step = optimizer.minimize(loss)\n",
    "    \n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "    summary_acc = tf.summary.scalar(\"Accuracy\", accuracy)\n",
    "    file_writer = tf.summary.FileWriter(outfile, tf.get_default_graph())\n",
    "    \n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2. Execution Phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@Epoch 0. Train Acc: 90.000% | Test Acc: 89.345%\n",
      "@Epoch 10. Train Acc: 98.000% | Test Acc: 94.942%\n",
      "@Epoch 20. Train Acc: 94.000% | Test Acc: 96.642%%\n",
      "@Epoch 30. Train Acc: 100.000% | Test Acc: 97.555%\n",
      "@Epoch 40. Train Acc: 100.000% | Test Acc: 98.204%\n",
      "@Epoch 50. Train Acc: 100.000% | Test Acc: 98.556%\n",
      "@Epoch 60. Train Acc: 100.000% | Test Acc: 98.887%\n",
      "@Epoch 69. Train Acc: 100.000% | Test Acc: 99.167%\r"
     ]
    }
   ],
   "source": [
    "n_epochs = 70\n",
    "batch_size = 50\n",
    "batch_iterations = mnist.train.num_examples // batch_size\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for epoch in range(n_epochs):\n",
    "        for iteration in range(batch_iterations):\n",
    "            X_batch, y_batch = mnist.train.next_batch(batch_size)\n",
    "            sess.run(train_step, feed_dict={X: X_batch, y:y_batch})\n",
    "            \n",
    "        acc_train = sess.run(accuracy, feed_dict={X:X_batch, y:y_batch})\n",
    "        acc_test = sess.run(accuracy, feed_dict={X:mnist.train.images,\n",
    "                                                 y:mnist.train.labels})\n",
    "        end = \"\\n\" if epoch % 10 == 0 else \"\\r\"\n",
    "        print(f\"@Epoch {epoch}. Train Acc: {acc_train:0.3%} | Test Acc: {acc_test:0.3%}\", end=end)\n",
    "        tboard_loss = summary_acc.eval(feed_dict={X: X_batch, y:y_batch})\n",
    "        file_writer.add_summary(tboard_loss, epoch)\n",
    "    save_path = saver.save(sess, \"./tmp/my_model_final.ckpt\")\n",
    "file_writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Normalization in TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.contrib.layers import fully_connected, batch_norm\n",
    "from tensorflow.contrib.framework import arg_scope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "n_inputs = 28 * 28\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 100\n",
    "n_outputs = 10\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int32, shape=(None), name=\"y\")\n",
    "\n",
    "is_training = tf.placeholder(tf.bool, shape=(), name=\"is_training\")\n",
    "bn_params = {\n",
    "    \"is_training\": is_training,\n",
    "    \"decay\": 0.999, # The exponential decay hyperparameter\n",
    "    \"updates_collections\": None\n",
    "}\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    ###  Argument Scopes ### \n",
    "    # In order to aviod repetition,\n",
    "    # the first parameter is a list of functions,\n",
    "    # and the other parameters will be passed to\n",
    "    # these functions automatically.\n",
    "    with arg_scope(\n",
    "        [fully_connected],\n",
    "        normalizer_fn=batch_norm,\n",
    "        normalizer_params=bn_params\n",
    "    ):\n",
    "        hidden1 = fully_connected(X, n_hidden1, activation_fn=tf.nn.elu,\n",
    "                                  scope=\"hidden1\")\n",
    "        hidden2 = fully_connected(hidden1, n_hidden2, activation_fn=tf.nn.elu,\n",
    "                                  scope=\"hidden2\")\n",
    "        logits = fully_connected(hidden2, n_outputs, activation_fn=None,\n",
    "                                 scope=\"logits\")\n",
    "\n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "        labels=y,\n",
    "        logits=logits\n",
    "    )\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "\n",
    "learning_rate = 0.01\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    train_step = optimizer.minimize(loss)\n",
    "\n",
    "with tf.name_scope(\"evaluate\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy @Epoch 0: 87.695%\n",
      "Accuracy @Epoch 10: 89.489%\n",
      "Accuracy @Epoch 20: 87.689%\n",
      "Accuracy @Epoch 30: 90.333%\n",
      "Accuracy @Epoch 40: 88.845%\n",
      "Accuracy @Epoch 50: 89.882%\n",
      "Accuracy @Epoch 60: 89.269%\n",
      "Accuracy @Epoch 69: 87.873%\r"
     ]
    }
   ],
   "source": [
    "epochs = 70\n",
    "batch_size = 50\n",
    "batch_iterations = mnist.train.num_examples // batch_size\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    for epoch in range(epochs):\n",
    "        sess.run(init)\n",
    "        for it in range(batch_iterations):\n",
    "            X_batch, y_batch = mnist.train.next_batch(batch_size)\n",
    "            sess.run(train_step,\n",
    "                     feed_dict={X:X_batch, y:y_batch, is_training:True})\n",
    "        acc_mnist = sess.run(accuracy,\n",
    "                             feed_dict={X:mnist.train.images,\n",
    "                                        y:mnist.train.labels,\n",
    "                                        is_training:False})\n",
    "        end = \"\\n\" if epoch % 10 == 0 else \"\\r\"\n",
    "        print(f\"Accuracy @Epoch {epoch}: {acc_mnist:0.3%}\", end=end)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Clipping\n",
    "In order to lessen the exploding gradients problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "n_inputs = 28 * 28\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 100\n",
    "n_outputs = 10\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int32, shape=(None), name=\"y\")\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = fully_connected(X, n_hidden1, scope=\"hidden1\")\n",
    "    hidden2 = fully_connected(hidden1, n_hidden2, scope=\"hidden2\")\n",
    "    logits = fully_connected(hidden2, n_outputs, activation_fn=None,\n",
    "                             scope=\"logits\")\n",
    "    \n",
    "#### GRADIENT CLIPPING IMPLEMENTATION ####\n",
    "# In tensorflow, the optimizer’s minimize() function\n",
    "# takes care of both computing the gradients and applying them,\n",
    "# so you must instead call the optimizer’s compute_gradients()\n",
    "# method first, then create an operation to clip the gradients\n",
    "# using the clip_by_value() function, and finally create an operation\n",
    "# to apply the clipped gradients using the optimizer’s apply_gradients() method\n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "        labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "    \n",
    "learning_rate = 0.01\n",
    "with tf.name_scope(\"train\"):\n",
    "    threshold = 1.0\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    grads_and_vars = optimizer.compute_gradients(loss)\n",
    "    capped_gvs = [(tf.clip_by_value(grad, -threshold, threshold), var)\n",
    "                  for grad, var in grads_and_vars]\n",
    "    train_step = optimizer.apply_gradients(capped_gvs)\n",
    "\n",
    "with tf.name_scope(\"evaluate\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "    \n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@Epoch 0. Test accuracy: 89.5000%\n",
      "@Epoch 10. Test accuracy: 96.5891%\n",
      "@Epoch 20. Test accuracy: 98.0891%\n",
      "@Epoch 30. Test accuracy: 98.8727%\n",
      "@Epoch 40. Test accuracy: 99.2764%\n",
      "@Epoch 50. Test accuracy: 99.6564%\n",
      "@Epoch 60. Test accuracy: 99.7618%\n",
      "@Epoch 69. Test accuracy: 99.8909%\r"
     ]
    }
   ],
   "source": [
    "n_epochs = 70\n",
    "batch_size = 50\n",
    "batch_iterations = mnist.train.num_examples // batch_size\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for epoch in range(n_epochs):\n",
    "        for it in range(batch_iterations):\n",
    "            X_batch, y_batch = mnist.train.next_batch(batch_size)\n",
    "            sess.run(train_step, feed_dict={X:X_batch, y:y_batch})\n",
    "            \n",
    "        test_acc = sess.run(accuracy,\n",
    "                            feed_dict={X:mnist.train.images,\n",
    "                                       y:mnist.train.labels})\n",
    "        end = \"\\n\" if epoch % 10 == 0 else \"\\r\"\n",
    "        print(f\"@Epoch {epoch}. Test accuracy: {test_acc:0.4%}\", end=end)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Is it okay to initialize all the weights to the same value as long as that value is selected randomly using He initialization?**  \n",
    "No, since initializing all weights to the same value would not allow backprop to break the *symmetry*, i.e., for each neuron in a given layer, their values would remain the same at evey step of the learning process.\n",
    "\n",
    "**2. Is it okay to initialize the bias terms to 0?**  \n",
    "Yes. Bias neurons do not suffer from the symmetry problem presented in other neurons.\n",
    "\n",
    "\n",
    "**3. Name three advantages of the ELU activation function over ReLU.**\n",
    "1. Smooth everywhere, which in turn helps speeding up gradient descent.\n",
    "2. Has non-zero gradient at $z < 0$, which helps preventing the dying neuron problem\n",
    "3. Considering initialization with standard normal, it helps with the vanishing gradient problem, since its average value is 0,\n",
    "\n",
    "\n",
    "**4. In which cases would you want to use each of the following activation functions: ELU, leaky ReLU (and its variants), ReLU, tanh, logistic, and softmax?**\n",
    "\n",
    "**5. What may happen if you set the momentum hyperparameter too close to 1 (e.g., 0.99999) when using a MomentumOptimizer?**  \n",
    "We expect the learning rate to shoot up, since, as $\\beta \\to 1$ and $k\\to\\infty$, $m_k \\to \\infty$\n",
    "\n",
    "\n",
    "**6. Name three ways you can produce a sparse model.**\n",
    "1. Set small weights to 0\n",
    "2. Use a strong $L_1$ regularization scheme\n",
    "3. Dual Averaging\n",
    "\n",
    "**7. Does dropout slow down training? Does it slow down inference (i.e., making predictions on new instances)?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**8. Deep Learning.**\n",
    "\n",
    "a. Build a DNN with five hidden layers of 100 neurons each, He initialization, and the ELU activation function.\n",
    "\n",
    "b. Using Adam optimization and early stopping, try training it on MNIST but only on digits 0 to 4, as we will use transfer learning for digits 5 to 9 in the next exercise. You will need a softmax output layer with five neurons, and as always make sure to save checkpoints at regular intervals and save the final model so you can reuse it later.\n",
    "\n",
    "c. Tune the hyperparameters using cross-validation and see what precision you can achieve.\n",
    "\n",
    "d. Now try adding Batch Normalization and compare the learning curves: is it converging faster than before? Does it produce a better model?  \n",
    "No, it underperforms slighly the complete feed forward neural network.\n",
    "\n",
    "e. Is the model overfitting the training set? Try adding dropout to every layer and try again. Does it help?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.contrib.layers import fully_connected, variance_scaling_initializer, batch_norm\n",
    "from tensorflow.contrib.framework import arg_scope\n",
    "import numpy as np\n",
    "from numpy.random import choice, seed\n",
    "from math import ceil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train1_mask = mnist.train.labels <= 4\n",
    "train1_y = mnist.train.labels[train1_mask]\n",
    "train1_X = mnist.train.images[train1_mask,:]\n",
    "\n",
    "seed(1643)\n",
    "n_train = sum(train1_mask)\n",
    "n_cv = ceil(0.2 * n_train)\n",
    "indices_cv = np.random.choice(range(n_train), n_cv, replace=False)\n",
    "indices_ytrain = [i for i in range(n_train) if i not in indices_cv]\n",
    "\n",
    "train1_y_cv, train1_y = train1_y[indices_cv], train1_y[indices_ytrain]\n",
    "train1_X_cv, train1_X = train1_X[indices_cv], train1_X[indices_ytrain]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy @Epoch 000... train: 53.38% | cv: 52.23%\n",
      "Accuracy @Epoch 040... train: 95.74% | cv: 95.97%\n",
      "Accuracy @Epoch 080... train: 97.01% | cv: 96.70%\n",
      "Accuracy @Epoch 120... train: 97.51% | cv: 97.25%\n",
      "Accuracy @Epoch 160... train: 97.87% | cv: 97.54%\n",
      "Accuracy @Epoch 200... train: 98.20% | cv: 97.79%\n",
      "Accuracy @Epoch 240... train: 98.50% | cv: 97.84%\n",
      "Accuracy @Epoch 280... train: 98.72% | cv: 97.88%\n",
      "Accuracy @Epoch 320... train: 98.92% | cv: 97.93%\n",
      "Accuracy @Epoch 360... train: 98.99% | cv: 97.90%\n",
      "Accuracy @Epoch 399... train: 99.08% | cv: 97.90%\r"
     ]
    }
   ],
   "source": [
    "now = datetime.now()\n",
    "outfile_train = now.strftime(\"./tf_logs/run-%Y%m%d%H%M-BN-train\")\n",
    "outfile_cv = now.strftime(\"./tf_logs/run-%Y%m%d%H%M-BN-cv\")\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "n_inputs = 28 * 28\n",
    "n_hidden = 100\n",
    "n_output = 5\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int32, shape=(None), name=\"y\")\n",
    "\n",
    "he_init = variance_scaling_initializer()\n",
    "is_training = tf.placeholder(tf.bool, shape=(), name=\"is_training\")\n",
    "bn_params = {\n",
    "    \"is_training\": is_training,\n",
    "    \"decay\": 0.999,\n",
    "    \"updates_collections\": None,    \n",
    "}\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    with arg_scope(\n",
    "        [fully_connected],\n",
    "        normalizer_fn=batch_norm,\n",
    "        normalizer_params=bn_params):\n",
    "        with arg_scope(\n",
    "            [fully_connected],\n",
    "            weights_initializer = he_init,\n",
    "            activation_fn = tf.nn.elu):\n",
    "            hidden1 = fully_connected(X, n_hidden, scope=\"hidden1\")\n",
    "            hidden2 = fully_connected(hidden1, n_hidden, scope=\"hidden2\")\n",
    "            hidden3 = fully_connected(hidden2, n_hidden, scope=\"hidden3\")\n",
    "            hidden4 = fully_connected(hidden3, n_hidden, scope=\"hidden4\")\n",
    "            hidden5 = fully_connected(hidden4, n_hidden, scope=\"hidden5\")\n",
    "        output = fully_connected(hidden5, n_output, activation_fn=None,\n",
    "                                 weights_initializer=he_init, scope=\"output\")\n",
    "\n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "        labels=y,\n",
    "        logits=output)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "\n",
    "alpha = 0.001\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.AdamOptimizer(alpha)\n",
    "    train_step = optimizer.minimize(loss)\n",
    "\n",
    "with tf.name_scope(\"metrics\"):\n",
    "    correct = tf.nn.in_top_k(output, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "    writer_train = tf.summary.FileWriter(outfile_train, tf.get_default_graph())\n",
    "    writer_test = tf.summary.FileWriter(outfile_cv, tf.get_default_graph())\n",
    "    summ_acc_train = tf.summary.scalar(\"Accuracy\", accuracy)\n",
    "    summ_acc_cv = tf.summary.scalar(\"Accuracy\", accuracy)\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "epochs = 400\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for epoch in range(epochs):\n",
    "        # Running a training step\n",
    "        sess.run(train_step, feed_dict={is_training: True, X:train1_X, y:train1_y})\n",
    "        acc = sess.run(accuracy, feed_dict ={is_training:False,\n",
    "                                             X:train1_X, y:train1_y})\n",
    "        acc_cv = sess.run(accuracy, feed_dict={is_training: False,\n",
    "                                               X:train1_X_cv, y:train1_y_cv})\n",
    "        end = \"\\n\" if epoch % 40 == 0 else \"\\r\"\n",
    "        \"\"\"Tensorboard summaries\"\"\"\n",
    "        tboard_train_loss = summ_acc_train.eval(feed_dict={is_training: False,\n",
    "                                                           X:train1_X, y:train1_y})\n",
    "        tboard_cv_loss = summ_acc_cv.eval(feed_dict={is_training: False,\n",
    "                                                     X:train1_X_cv, y:train1_y_cv})\n",
    "        writer_train.add_summary(tboard_train_loss, epoch)\n",
    "        writer_test.add_summary(tboard_cv_loss, epoch)\n",
    "        \n",
    "        print(f\"Accuracy @Epoch {epoch:03}... train: {acc:.2%} | cv: {acc_cv:0.2%}\", end=end)\n",
    "    saver.save(sess, \"./tmp/my_model_final.ckpt\")\n",
    "writer_test.close()\n",
    "writer_train.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Accuracy Comparisson under training set**\n",
    "![Training History](./images/Training_comparissons.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**9. Transfer learning.**\n",
    "\n",
    "a. Create a new DNN that reuses all the pretrained hidden layers of the previous model, freezes them, and replaces the softmax output layer with a fresh new one.\n",
    "\n",
    "b. Train this new DNN on digits 5 to 9, using only 100 images per digit, and time how long it takes. Despite this small number of examples, can you achieve high precision?\n",
    "\n",
    "c. Try caching the frozen layers, and train the model again: how much faster is it now?\n",
    "\n",
    "d. Try again reusing just four hidden layers instead of five. Can you achieve a higher precision?\n",
    "\n",
    "e. Now unfreeze the top two hidden layers and continue training: can you get the model to perform even better?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.contrib.layers import fully_connected, batch_norm, variance_scaling_initializer\n",
    "from tensorflow.contrib.framework import arg_scope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Variable 'hidden1/weights:0' shape=(784, 100) dtype=float32_ref>, <tf.Variable 'hidden1/BatchNorm/beta:0' shape=(100,) dtype=float32_ref>, <tf.Variable 'hidden2/weights:0' shape=(100, 100) dtype=float32_ref>, <tf.Variable 'hidden2/BatchNorm/beta:0' shape=(100,) dtype=float32_ref>, <tf.Variable 'hidden3/weights:0' shape=(100, 100) dtype=float32_ref>, <tf.Variable 'hidden3/BatchNorm/beta:0' shape=(100,) dtype=float32_ref>, <tf.Variable 'output/weights:0' shape=(100, 5) dtype=float32_ref>, <tf.Variable 'output/BatchNorm/beta:0' shape=(5,) dtype=float32_ref>]\n",
      "**********\n",
      "[<tf.Variable 'hidden1/weights:0' shape=(784, 100) dtype=float32_ref>, <tf.Variable 'hidden1/BatchNorm/beta:0' shape=(100,) dtype=float32_ref>, <tf.Variable 'hidden2/weights:0' shape=(100, 100) dtype=float32_ref>, <tf.Variable 'hidden2/BatchNorm/beta:0' shape=(100,) dtype=float32_ref>, <tf.Variable 'hidden3/weights:0' shape=(100, 100) dtype=float32_ref>, <tf.Variable 'hidden3/BatchNorm/beta:0' shape=(100,) dtype=float32_ref>]\n",
      "INFO:tensorflow:Restoring parameters from ./tmp/my_model_final.ckpt\n"
     ]
    },
    {
     "ename": "NotFoundError",
     "evalue": "Key hidden1/BatchNorm/beta:0 not found in checkpoint\n\t [[Node: save/RestoreV2 = RestoreV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](_arg_save/Const_0_0, save/RestoreV2/tensor_names, save/RestoreV2/shape_and_slices)]]\n\nCaused by op 'save/RestoreV2', defined at:\n  File \"/Users/gerardoduran/anaconda/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/Users/gerardoduran/anaconda/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/Users/gerardoduran/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/Users/gerardoduran/anaconda/lib/python3.6/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/Users/gerardoduran/anaconda/lib/python3.6/site-packages/ipykernel/kernelapp.py\", line 486, in start\n    self.io_loop.start()\n  File \"/Users/gerardoduran/anaconda/lib/python3.6/site-packages/tornado/platform/asyncio.py\", line 127, in start\n    self.asyncio_loop.run_forever()\n  File \"/Users/gerardoduran/anaconda/lib/python3.6/asyncio/base_events.py\", line 422, in run_forever\n    self._run_once()\n  File \"/Users/gerardoduran/anaconda/lib/python3.6/asyncio/base_events.py\", line 1432, in _run_once\n    handle._run()\n  File \"/Users/gerardoduran/anaconda/lib/python3.6/asyncio/events.py\", line 145, in _run\n    self._callback(*self._args)\n  File \"/Users/gerardoduran/anaconda/lib/python3.6/site-packages/tornado/platform/asyncio.py\", line 117, in _handle_events\n    handler_func(fileobj, events)\n  File \"/Users/gerardoduran/anaconda/lib/python3.6/site-packages/tornado/stack_context.py\", line 276, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/Users/gerardoduran/anaconda/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 450, in _handle_events\n    self._handle_recv()\n  File \"/Users/gerardoduran/anaconda/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 480, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/Users/gerardoduran/anaconda/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 432, in _run_callback\n    callback(*args, **kwargs)\n  File \"/Users/gerardoduran/anaconda/lib/python3.6/site-packages/tornado/stack_context.py\", line 276, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/Users/gerardoduran/anaconda/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/Users/gerardoduran/anaconda/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 233, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/Users/gerardoduran/anaconda/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/Users/gerardoduran/anaconda/lib/python3.6/site-packages/ipykernel/ipkernel.py\", line 208, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/Users/gerardoduran/anaconda/lib/python3.6/site-packages/ipykernel/zmqshell.py\", line 537, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/Users/gerardoduran/anaconda/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2662, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"/Users/gerardoduran/anaconda/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2785, in _run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/Users/gerardoduran/anaconda/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2903, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/Users/gerardoduran/anaconda/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2963, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-51-5b9631f5ad40>\", line 67, in <module>\n    original_saver = tf.train.Saver(reuse_vars_dict)\n  File \"/Users/gerardoduran/anaconda/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 1293, in __init__\n    self.build()\n  File \"/Users/gerardoduran/anaconda/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 1302, in build\n    self._build(self._filename, build_save=True, build_restore=True)\n  File \"/Users/gerardoduran/anaconda/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 1339, in _build\n    build_save=build_save, build_restore=build_restore)\n  File \"/Users/gerardoduran/anaconda/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 796, in _build_internal\n    restore_sequentially, reshape)\n  File \"/Users/gerardoduran/anaconda/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 449, in _AddRestoreOps\n    restore_sequentially)\n  File \"/Users/gerardoduran/anaconda/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 847, in bulk_restore\n    return io_ops.restore_v2(filename_tensor, names, slices, dtypes)\n  File \"/Users/gerardoduran/anaconda/lib/python3.6/site-packages/tensorflow/python/ops/gen_io_ops.py\", line 1030, in restore_v2\n    shape_and_slices=shape_and_slices, dtypes=dtypes, name=name)\n  File \"/Users/gerardoduran/anaconda/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/Users/gerardoduran/anaconda/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 3271, in create_op\n    op_def=op_def)\n  File \"/Users/gerardoduran/anaconda/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1650, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nNotFoundError (see above for traceback): Key hidden1/BatchNorm/beta:0 not found in checkpoint\n\t [[Node: save/RestoreV2 = RestoreV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](_arg_save/Const_0_0, save/RestoreV2/tensor_names, save/RestoreV2/shape_and_slices)]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1360\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1361\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1362\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1339\u001b[0m           return tf_session.TF_Run(session, options, feed_dict, fetch_list,\n\u001b[0;32m-> 1340\u001b[0;31m                                    target_list, status, run_metadata)\n\u001b[0m\u001b[1;32m   1341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type_arg, value_arg, traceback_arg)\u001b[0m\n\u001b[1;32m    515\u001b[0m             \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 516\u001b[0;31m             c_api.TF_GetCode(self.status.status))\n\u001b[0m\u001b[1;32m    517\u001b[0m     \u001b[0;31m# Delete the underlying status object from memory otherwise it stays alive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotFoundError\u001b[0m: Key hidden1/BatchNorm/beta:0 not found in checkpoint\n\t [[Node: save/RestoreV2 = RestoreV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](_arg_save/Const_0_0, save/RestoreV2/tensor_names, save/RestoreV2/shape_and_slices)]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-51-5b9631f5ad40>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m     \u001b[0moriginal_saver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"./tmp/my_model_final.ckpt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/tensorflow/python/training/saver.py\u001b[0m in \u001b[0;36mrestore\u001b[0;34m(self, sess, save_path)\u001b[0m\n\u001b[1;32m   1753\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0min_graph_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1754\u001b[0m       sess.run(self.saver_def.restore_op_name,\n\u001b[0;32m-> 1755\u001b[0;31m                {self.saver_def.filename_tensor_name: save_path})\n\u001b[0m\u001b[1;32m   1756\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1757\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_build_eager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuild_save\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuild_restore\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    903\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    904\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 905\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    906\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    907\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1135\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1136\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1137\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1138\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1139\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1353\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1354\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1355\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1356\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1357\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1372\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1373\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1374\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1375\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1376\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotFoundError\u001b[0m: Key hidden1/BatchNorm/beta:0 not found in checkpoint\n\t [[Node: save/RestoreV2 = RestoreV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](_arg_save/Const_0_0, save/RestoreV2/tensor_names, save/RestoreV2/shape_and_slices)]]\n\nCaused by op 'save/RestoreV2', defined at:\n  File \"/Users/gerardoduran/anaconda/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/Users/gerardoduran/anaconda/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/Users/gerardoduran/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/Users/gerardoduran/anaconda/lib/python3.6/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/Users/gerardoduran/anaconda/lib/python3.6/site-packages/ipykernel/kernelapp.py\", line 486, in start\n    self.io_loop.start()\n  File \"/Users/gerardoduran/anaconda/lib/python3.6/site-packages/tornado/platform/asyncio.py\", line 127, in start\n    self.asyncio_loop.run_forever()\n  File \"/Users/gerardoduran/anaconda/lib/python3.6/asyncio/base_events.py\", line 422, in run_forever\n    self._run_once()\n  File \"/Users/gerardoduran/anaconda/lib/python3.6/asyncio/base_events.py\", line 1432, in _run_once\n    handle._run()\n  File \"/Users/gerardoduran/anaconda/lib/python3.6/asyncio/events.py\", line 145, in _run\n    self._callback(*self._args)\n  File \"/Users/gerardoduran/anaconda/lib/python3.6/site-packages/tornado/platform/asyncio.py\", line 117, in _handle_events\n    handler_func(fileobj, events)\n  File \"/Users/gerardoduran/anaconda/lib/python3.6/site-packages/tornado/stack_context.py\", line 276, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/Users/gerardoduran/anaconda/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 450, in _handle_events\n    self._handle_recv()\n  File \"/Users/gerardoduran/anaconda/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 480, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/Users/gerardoduran/anaconda/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 432, in _run_callback\n    callback(*args, **kwargs)\n  File \"/Users/gerardoduran/anaconda/lib/python3.6/site-packages/tornado/stack_context.py\", line 276, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/Users/gerardoduran/anaconda/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/Users/gerardoduran/anaconda/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 233, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/Users/gerardoduran/anaconda/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/Users/gerardoduran/anaconda/lib/python3.6/site-packages/ipykernel/ipkernel.py\", line 208, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/Users/gerardoduran/anaconda/lib/python3.6/site-packages/ipykernel/zmqshell.py\", line 537, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/Users/gerardoduran/anaconda/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2662, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"/Users/gerardoduran/anaconda/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2785, in _run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/Users/gerardoduran/anaconda/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2903, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/Users/gerardoduran/anaconda/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2963, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-51-5b9631f5ad40>\", line 67, in <module>\n    original_saver = tf.train.Saver(reuse_vars_dict)\n  File \"/Users/gerardoduran/anaconda/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 1293, in __init__\n    self.build()\n  File \"/Users/gerardoduran/anaconda/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 1302, in build\n    self._build(self._filename, build_save=True, build_restore=True)\n  File \"/Users/gerardoduran/anaconda/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 1339, in _build\n    build_save=build_save, build_restore=build_restore)\n  File \"/Users/gerardoduran/anaconda/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 796, in _build_internal\n    restore_sequentially, reshape)\n  File \"/Users/gerardoduran/anaconda/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 449, in _AddRestoreOps\n    restore_sequentially)\n  File \"/Users/gerardoduran/anaconda/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 847, in bulk_restore\n    return io_ops.restore_v2(filename_tensor, names, slices, dtypes)\n  File \"/Users/gerardoduran/anaconda/lib/python3.6/site-packages/tensorflow/python/ops/gen_io_ops.py\", line 1030, in restore_v2\n    shape_and_slices=shape_and_slices, dtypes=dtypes, name=name)\n  File \"/Users/gerardoduran/anaconda/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/Users/gerardoduran/anaconda/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 3271, in create_op\n    op_def=op_def)\n  File \"/Users/gerardoduran/anaconda/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1650, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nNotFoundError (see above for traceback): Key hidden1/BatchNorm/beta:0 not found in checkpoint\n\t [[Node: save/RestoreV2 = RestoreV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](_arg_save/Const_0_0, save/RestoreV2/tensor_names, save/RestoreV2/shape_and_slices)]]\n"
     ]
    }
   ],
   "source": [
    "train_tlearn = \"./tf_logs/transfer-learning-train\"\n",
    "test_tlearn = \"./tf_logs/transfer-learning-test\"\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "n_inputs = 28 * 28\n",
    "n_hidden1 = 100\n",
    "n_hidden2 = 100\n",
    "n_hidden3 = 100\n",
    "n_output = 5\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int32, shape=(None), name=\"y\")\n",
    "\n",
    "he_initializer = variance_scaling_initializer()\n",
    "is_training = tf.placeholder(tf.bool, shape=(), name=\"is_training\")\n",
    "bn_params = {\n",
    "    \"is_training\": is_training,\n",
    "    \"decay\": 0.999,\n",
    "    \"updates_collections\": None\n",
    "}\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    with arg_scope(\n",
    "        [fully_connected],\n",
    "        normalizer_fn=batch_norm,\n",
    "        normalizer_params=bn_params,\n",
    "        weights_initializer=he_initializer\n",
    "    ):\n",
    "        with arg_scope(\n",
    "            [fully_connected],\n",
    "            activation_fn=tf.nn.elu\n",
    "        ):\n",
    "            hidden1 = fully_connected(X, n_hidden1, scope=\"hidden1\")\n",
    "            hidden2 = fully_connected(hidden1, n_hidden2, scope=\"hidden2\")\n",
    "            hidden3 = fully_connected(hidden2, n_hidden3, scope=\"hidden3\")\n",
    "        output = fully_connected(hidden3, n_output, activation_fn=None,\n",
    "                                 scope=\"output\")\n",
    "\n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=output)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "    \n",
    "alpha = 0.001\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.AdamOptimizer(alpha)\n",
    "    train_step = optimizer.minimize(loss)\n",
    "\n",
    "with tf.name_scope(\"metrics\"):\n",
    "    correct = tf.nn.in_top_k(output, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "    writer_train = tf.summary.FileWriter(train_tlearn, tf.get_default_graph())\n",
    "    writer_test = tf.summary.FileWriter(test_tlearn, tf.get_default_graph())\n",
    "    summ_acc_train = tf.summary.scalar(\"Accuracy\", accuracy)\n",
    "    summ_acc_cv = tf.summary.scalar(\"Accuracy\", accuracy)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "# Retrieve hidden layers\n",
    "reuse_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,\n",
    "                               scope=\"hidden[1-9]\")\n",
    "print(tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES))\n",
    "print(\"*\" * 10)\n",
    "print(reuse_vars)\n",
    "# Mapping from old nodes to new nodes (we keep the same names)\n",
    "reuse_vars_dict = {var.name: var for var in reuse_vars}\n",
    "# Saver to reuse the original model\n",
    "original_saver = tf.train.Saver(reuse_vars_dict)\n",
    "\n",
    "epochs = 100\n",
    "with tf.Session() as sess:\n",
    "    original_saver.restore(sess, \"./tmp/my_model_final.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'hidden1/weights:0': 'hidden1/weights:0',\n",
       " 'hidden1/BatchNorm/beta:0': 'hidden1/BatchNorm/beta:0',\n",
       " 'hidden2/weights:0': 'hidden2/weights:0',\n",
       " 'hidden2/BatchNorm/beta:0': 'hidden2/BatchNorm/beta:0',\n",
       " 'hidden3/weights:0': 'hidden3/weights:0',\n",
       " 'hidden3/BatchNorm/beta:0': 'hidden3/BatchNorm/beta:0'}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reuse_vars_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**10. Pretraining on an auxiliary task.**\n",
    "\n",
    "a. In this exercise you will build a DNN that compares two MNIST digit images and predicts whether they represent the same digit or not. Then you will reuse the lower layers of this network to train an MNIST classifier using very little training data. Start by building two DNNs (let’s call them DNN A and B), both similar to the one you built earlier but without the output layer: each DNN should have five hidden layers of 100 neurons each, He initialization, and ELU activation. Next, add a single output layer on top of both DNNs. You should use TensorFlow’s concat() function with axis=1 to concatenate the outputs of both DNNs along the horizontal axis, then feed the result to the output layer. This output layer should contain a single neuron using the logistic activation function.\n",
    "\n",
    "b. Split the MNIST training set in two sets: split #1 should containing 55,000 images, and split #2 should contain contain 5,000 images. Create a function that generates a training batch where each instance is a pair of MNIST images picked from split #1. Half of the training instances should be pairs of images that belong to the same class, while the other half should be images from different classes. For each pair, the training label should be 0 if the images are from the same class, or 1 if they are from different classes.\n",
    "\n",
    "c. Train the DNN on this training set. For each image pair, you can simultaneously feed the first image to DNN A and the second image to DNN B. The whole network will gradually learn to tell whether two images belong to the same class or not.\n",
    "\n",
    "d. Now create a new DNN by reusing and freezing the hidden layers of DNN A and adding a softmax output layer on with 10 neurons. Train this network on split #2 and see if you can achieve high performance despite having only 500 images per class."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
