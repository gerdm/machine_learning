{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks With TensorFlow\n",
    "## Stable implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow import variance_scaling_initializer # He-initializer\n",
    "import matplotlib.pyplot as plt\n",
    "from jupyter_tf_graph import show_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "X_train = X_train.reshape(-1, 28 * 28) / 255.0\n",
    "X_test = X_test.reshape(-1, 28 * 28) / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x504 with 6 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "np.random.seed(1643)\n",
    "img_ixs = np.random.randint(0, X_train.shape[0], 6)\n",
    "fig = plt.figure(figsize=(10, 7))\n",
    "fig.suptitle(\"Sample MNIST images\")\n",
    "for ix, img_ixs in enumerate(img_ixs):\n",
    "    ax = fig.add_subplot(3, 2, ix + 1)\n",
    "    ax.imshow(X_train[ix].reshape(28, 28))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a Fully Connected Deep Neural Network\n",
    "### FFNN V.01\n",
    "Feed forward neural network with sigmoid activation function and Gradient Descent Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "logs = \"./tf_logs/ffnn_v01\"\n",
    "\n",
    "n_inputs = 28 * 28\n",
    "n_hidden = 100\n",
    "n_outputs = 10\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int32, shape=None, name=\"y\")\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = tf.layers.dense(X, n_hidden, activation=tf.nn.sigmoid, name=\"hidden1\")\n",
    "    hidden2 = tf.layers.dense(hidden1, n_hidden, activation=tf.nn.sigmoid, name=\"hidden2\")\n",
    "    hidden3 = tf.layers.dense(hidden2, n_hidden, activation=tf.nn.sigmoid, name=\"hidden3\")\n",
    "    hidden4 = tf.layers.dense(hidden3, n_hidden, activation=tf.nn.sigmoid, name=\"hidden4\")\n",
    "    hidden5 = tf.layers.dense(hidden4, n_hidden, activation=tf.nn.sigmoid, name=\"hidden5\")\n",
    "    outputs = tf.layers.dense(hidden5, n_outputs, name=\"outputs\")\n",
    "\n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "        labels=y,\n",
    "        logits=outputs\n",
    "    )\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "    \n",
    "\n",
    "alpha = 0.001\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(alpha)\n",
    "    train_step = optimizer.minimize(loss)\n",
    "\n",
    "with tf.name_scope(\"metrics\"):\n",
    "    correct = tf.nn.in_top_k(outputs, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name=\"accuracy\")\n",
    "    tb_train_writer = tf.summary.FileWriter(logs + \"/train\", tf.get_default_graph())\n",
    "    tb_cv_writer = tf.summary.FileWriter(logs + \"/cv\", tf.get_default_graph())\n",
    "    tb_accuracy = tf.summary.scalar(\"Accuracy\", accuracy)\n",
    "    \n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@Epoch  00: Train Accuracy 009.87% | Test Accuracy 009.80%\n",
      "@Epoch  40: Train Accuracy 009.87% | Test Accuracy 009.80%\n",
      "@Epoch  80: Train Accuracy 009.87% | Test Accuracy 009.80%\n",
      "@Epoch  120: Train Accuracy 009.87% | Test Accuracy 009.80%\n",
      "@Epoch  160: Train Accuracy 009.87% | Test Accuracy 009.80%\n",
      "@Epoch  200: Train Accuracy 009.87% | Test Accuracy 009.80%\n",
      "@Epoch  240: Train Accuracy 009.87% | Test Accuracy 009.80%\n",
      "@Epoch  280: Train Accuracy 011.24% | Test Accuracy 011.35%\n",
      "@Epoch  320: Train Accuracy 011.24% | Test Accuracy 011.35%\n",
      "@Epoch  360: Train Accuracy 011.24% | Test Accuracy 011.35%\n",
      "CPU times: user 24min 36s, sys: 4min 15s, total: 28min 51s%\n",
      "Wall time: 6min 50s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "epochs = 400\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for epoch in range(epochs):\n",
    "        sess.run(train_step, feed_dict={X: X_train, y: y_train})\n",
    "        acc, tb_acc = sess.run([accuracy, tb_accuracy], feed_dict={X: X_train, y: y_train})\n",
    "        cvacc, tb_cvacc = sess.run([accuracy, tb_accuracy], feed_dict={X: X_test, y: y_test})\n",
    "        tb_train_writer.add_summary(tb_acc, epoch)\n",
    "        tb_cv_writer.add_summary(tb_cvacc, epoch)\n",
    "        \n",
    "        end = \"\\n\" if epoch % 40 == 0 else \"\\r\"\n",
    "        print(f\"@Epoch {epoch: 03}: Train Accuracy {acc:07.2%} | Test Accuracy {cvacc:07.2%}\",\n",
    "              end=end)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "### FFNN V.02\n",
    "Feed forward neural network with elu activation function and Gradient Descent Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "logs = \"./tf_logs/ffnn_v02\"\n",
    "\n",
    "n_inputs = 28 * 28\n",
    "n_hidden = 100\n",
    "n_outputs = 10\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int32, shape=None, name=\"y\")\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = tf.layers.dense(X, n_hidden, activation=tf.nn.elu, name=\"hidden1\")\n",
    "    hidden2 = tf.layers.dense(hidden1, n_hidden, activation=tf.nn.elu, name=\"hidden2\")\n",
    "    hidden3 = tf.layers.dense(hidden2, n_hidden, activation=tf.nn.elu, name=\"hidden3\")\n",
    "    hidden4 = tf.layers.dense(hidden3, n_hidden, activation=tf.nn.elu, name=\"hidden4\")\n",
    "    hidden5 = tf.layers.dense(hidden4, n_hidden, activation=tf.nn.elu, name=\"hidden5\")\n",
    "    outputs = tf.layers.dense(hidden5, n_outputs, name=\"outputs\")\n",
    "\n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "        labels=y,\n",
    "        logits=outputs\n",
    "    )\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "    \n",
    "\n",
    "alpha = 0.001\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(alpha)\n",
    "    train_step = optimizer.minimize(loss)\n",
    "\n",
    "with tf.name_scope(\"metrics\"):\n",
    "    correct = tf.nn.in_top_k(outputs, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name=\"accuracy\")\n",
    "    tb_train_writer = tf.summary.FileWriter(logs + \"/train\", tf.get_default_graph())\n",
    "    tb_cv_writer = tf.summary.FileWriter(logs + \"/cv\", tf.get_default_graph())\n",
    "    tb_accuracy = tf.summary.scalar(\"Accuracy\", accuracy)\n",
    "    \n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@Epoch  00: Train Accuracy 006.13% | Test Accuracy 006.11%\n",
      "@Epoch  40: Train Accuracy 009.70% | Test Accuracy 009.56%\n",
      "@Epoch  80: Train Accuracy 014.78% | Test Accuracy 014.49%\n",
      "@Epoch  120: Train Accuracy 021.02% | Test Accuracy 021.13%\n",
      "@Epoch  160: Train Accuracy 027.68% | Test Accuracy 028.32%\n",
      "@Epoch  200: Train Accuracy 033.88% | Test Accuracy 034.71%\n",
      "@Epoch  240: Train Accuracy 039.29% | Test Accuracy 040.41%\n",
      "@Epoch  280: Train Accuracy 043.42% | Test Accuracy 044.78%\n",
      "@Epoch  320: Train Accuracy 047.05% | Test Accuracy 048.85%\n",
      "@Epoch  360: Train Accuracy 050.14% | Test Accuracy 051.77%\n",
      "CPU times: user 26min 10s, sys: 4min 11s, total: 30min 22s%\n",
      "Wall time: 6min 56s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "epochs = 400\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for epoch in range(epochs):\n",
    "        sess.run(train_step, feed_dict={X: X_train, y: y_train})\n",
    "        acc, tb_acc = sess.run([accuracy, tb_accuracy], feed_dict={X: X_train, y: y_train})\n",
    "        cvacc, tb_cvacc = sess.run([accuracy, tb_accuracy], feed_dict={X: X_test, y: y_test})\n",
    "        tb_train_writer.add_summary(tb_acc, epoch)\n",
    "        tb_cv_writer.add_summary(tb_cvacc, epoch)\n",
    "        \n",
    "        end = \"\\n\" if epoch % 40 == 0 else \"\\r\"\n",
    "        print(f\"@Epoch {epoch: 03}: Train Accuracy {acc:07.2%} | Test Accuracy {cvacc:07.2%}\",\n",
    "              end=end)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "### FFNN V.03\n",
    "Feed forward neural network with elu activation function and ADAM Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "logs = \"./tf_logs/ffnn_v03\"\n",
    "\n",
    "n_inputs = 28 * 28\n",
    "n_hidden = 100\n",
    "n_outputs = 10\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int32, shape=None, name=\"y\")\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = tf.layers.dense(X, n_hidden, activation=tf.nn.elu, name=\"hidden1\")\n",
    "    hidden2 = tf.layers.dense(hidden1, n_hidden, activation=tf.nn.elu, name=\"hidden2\")\n",
    "    hidden3 = tf.layers.dense(hidden2, n_hidden, activation=tf.nn.elu, name=\"hidden3\")\n",
    "    hidden4 = tf.layers.dense(hidden3, n_hidden, activation=tf.nn.elu, name=\"hidden4\")\n",
    "    hidden5 = tf.layers.dense(hidden4, n_hidden, activation=tf.nn.elu, name=\"hidden5\")\n",
    "    outputs = tf.layers.dense(hidden5, n_outputs, name=\"outputs\")\n",
    "\n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "        labels=y,\n",
    "        logits=outputs\n",
    "    )\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "    \n",
    "\n",
    "alpha = 0.001\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.AdamOptimizer(alpha)\n",
    "    train_step = optimizer.minimize(loss)\n",
    "\n",
    "with tf.name_scope(\"metrics\"):\n",
    "    correct = tf.nn.in_top_k(outputs, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name=\"accuracy\")\n",
    "    tb_train_writer = tf.summary.FileWriter(logs + \"/train\", tf.get_default_graph())\n",
    "    tb_cv_writer = tf.summary.FileWriter(logs + \"/cv\", tf.get_default_graph())\n",
    "    tb_accuracy = tf.summary.scalar(\"Accuracy\", accuracy)\n",
    "    \n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@Epoch  00: Train Accuracy 025.93% | Test Accuracy 026.65%\n",
      "@Epoch  40: Train Accuracy 091.63% | Test Accuracy 091.74%\n",
      "@Epoch  80: Train Accuracy 094.57% | Test Accuracy 094.37%\n",
      "@Epoch  120: Train Accuracy 096.32% | Test Accuracy 095.71%\n",
      "@Epoch  160: Train Accuracy 097.35% | Test Accuracy 096.45%\n",
      "@Epoch  200: Train Accuracy 098.10% | Test Accuracy 096.92%\n",
      "@Epoch  240: Train Accuracy 098.75% | Test Accuracy 097.19%\n",
      "@Epoch  280: Train Accuracy 098.96% | Test Accuracy 097.31%\n",
      "@Epoch  320: Train Accuracy 099.46% | Test Accuracy 097.39%\n",
      "@Epoch  360: Train Accuracy 099.69% | Test Accuracy 097.42%\n",
      "CPU times: user 26min 11s, sys: 4min 17s, total: 30min 28s%\n",
      "Wall time: 7min 14s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "epochs = 400\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for epoch in range(epochs):\n",
    "        sess.run(train_step, feed_dict={X: X_train, y: y_train})\n",
    "        acc, tb_acc = sess.run([accuracy, tb_accuracy],\n",
    "                               feed_dict={X: X_train, y: y_train})\n",
    "        cvacc, tb_cvacc = sess.run([accuracy, tb_accuracy],\n",
    "                                   feed_dict={X: X_test, y: y_test})\n",
    "        tb_train_writer.add_summary(tb_acc, epoch)\n",
    "        tb_cv_writer.add_summary(tb_cvacc, epoch)\n",
    "        \n",
    "        end = \"\\n\" if epoch % 40 == 0 else \"\\r\"\n",
    "        print(f\"@Epoch {epoch: 03}: Train Accuracy {acc:07.2%} | Test Accuracy {cvacc:07.2%}\",\n",
    "              end=end)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "### FFNN V.04\n",
    "Feed forward neural network with elu activation function, ADAM Optimizer and Batch Norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "logs = \"./tf_logs/ffnn_v04/\"\n",
    "\n",
    "n_inputs = 28 * 28\n",
    "n_hidden = 100\n",
    "n_outputs = 10\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int32, shape=(None), name=\"y\")\n",
    "training = tf.placeholder_with_default(False, shape=(), name=\"training\")\n",
    "\n",
    "eta = 0.9\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    z1 = tf.layers.dense(X, n_hidden, name=\"z1\")\n",
    "    z1_bn = tf.layers.batch_normalization(z1, training=training, momentum=eta)\n",
    "    a1 = tf.nn.elu(z1_bn)\n",
    "    \n",
    "    z2 = tf.layers.dense(a1, n_hidden, name=\"z2\")\n",
    "    z2_bn = tf.layers.batch_normalization(z2, training=training, momentum=eta)\n",
    "    a2 = tf.nn.elu(z2_bn)\n",
    "    \n",
    "    z3 = tf.layers.dense(a2, n_hidden, name=\"z3\")\n",
    "    z3_bn = tf.layers.batch_normalization(z3, training=training, momentum=eta)\n",
    "    a3 = tf.nn.elu(z3_bn)\n",
    "    \n",
    "    z4 = tf.layers.dense(a3, n_hidden, name=\"z4\")\n",
    "    z4_bn = tf.layers.batch_normalization(z4, training=training, momentum=eta)\n",
    "    a4 = tf.nn.elu(z4)\n",
    "    \n",
    "    z5 = tf.layers.dense(a4, n_hidden, name=\"z5\")\n",
    "    z5_bn = tf.layers.batch_normalization(z5, training=training, momentum=eta)\n",
    "    a5 = tf.nn.elu(z5_bn)\n",
    "    \n",
    "    output = tf.layers.dense(a5, n_hidden, name=\"output\")\n",
    "\n",
    "\n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "        labels=y,\n",
    "        logits=output\n",
    "    )\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "\n",
    "    \n",
    "alpha = 0.001\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.AdamOptimizer(alpha)\n",
    "    train_step = optimizer.minimize(loss)\n",
    "\n",
    "    \n",
    "with tf.name_scope(\"metrics\"):\n",
    "    correct = tf.nn.in_top_k(output, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name=\"accuracy\")\n",
    "    tb_train_writer = tf.summary.FileWriter(logs + \"/train\", tf.get_default_graph())\n",
    "    tb_cv_writer = tf.summary.FileWriter(logs + \"/cv\", tf.get_default_graph())\n",
    "    tb_accuracy = tf.summary.scalar(\"Accuracy\", accuracy)\n",
    "\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@Epoch  00: Train Accuracy 005.91% | Test Accuracy 005.43%\n",
      "@Epoch  40: Train Accuracy 091.78% | Test Accuracy 092.06%\n",
      "@Epoch  80: Train Accuracy 095.16% | Test Accuracy 094.87%\n",
      "@Epoch  120: Train Accuracy 096.90% | Test Accuracy 096.05%\n",
      "@Epoch  160: Train Accuracy 097.95% | Test Accuracy 096.75%\n",
      "@Epoch  200: Train Accuracy 098.79% | Test Accuracy 097.07%\n",
      "@Epoch  240: Train Accuracy 099.32% | Test Accuracy 097.29%\n",
      "@Epoch  280: Train Accuracy 099.68% | Test Accuracy 097.37%\n",
      "@Epoch  320: Train Accuracy 099.87% | Test Accuracy 097.45%\n",
      "@Epoch  360: Train Accuracy 099.95% | Test Accuracy 097.43%\n",
      "CPU times: user 47min 35s, sys: 6min 25s, total: 54min7.43%\n",
      "Wall time: 10min 52s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Operations to compute the mean and standard deviation of the minibatch\n",
    "extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "epochs = 400\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for epoch in range(epochs):\n",
    "        sess.run([train_step, extra_update_ops], feed_dict={training: True, X: X_train, y: y_train})\n",
    "        acc, tb_acc = sess.run([accuracy, tb_accuracy],\n",
    "                               feed_dict={X: X_train, y: y_train})\n",
    "        cvacc, tb_cvacc = sess.run([accuracy, tb_accuracy],\n",
    "                                    feed_dict={X: X_test, y: y_test})\n",
    "        \n",
    "        tb_train_writer.add_summary(tb_acc, epoch)\n",
    "        tb_cv_writer.add_summary(tb_cvacc, epoch)\n",
    "        \n",
    "        end = \"\\n\" if epoch % 40 == 0 else \"\\r\"\n",
    "        print(f\"@Epoch {epoch:03}: Train Accuracy {acc:07.2%} | Test Accuracy {cvacc:07.2%}\",\n",
    "              end=end)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feed Forward Neural Network Version Comparison\n",
    "![FFNNs Comparisson](./images/ffnn_vs.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "----\n",
    "# Storing and Reusing TF Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from functools import partial\n",
    "\n",
    "def a_plus_b(a, b): return a + b\n",
    "a_plus_3 = partial(a_plus_b, b=3)\n",
    "a_plus_3(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Building and Storing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "logs = \"./tf_logs/ffnn_v05/\"\n",
    "\n",
    "n_inputs = 28 * 28\n",
    "n_hidden = 100\n",
    "n_output = 10\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int32, shape=None, name=\"y\")\n",
    "\n",
    "training = tf.placeholder_with_default(False, shape=(), name=\"training_bool\")\n",
    "\n",
    "eta = 0.9\n",
    "batch_norm = partial(tf.layers.batch_normalization,\n",
    "                     momentum=eta, training=training)\n",
    "\n",
    "def hidden_layer(inputs, units, hi, activation=tf.nn.elu):\n",
    "    \"\"\"\n",
    "    Create the hidden layer of a feed forward neural\n",
    "    network with batch norm.\n",
    "    \"\"\"\n",
    "    zi = tf.layers.dense(inputs, units, name=f\"z{hi}\")\n",
    "    zi_bn = batch_norm(zi, name=f\"z_bn{hi}\")\n",
    "    ai = activation(zi_bn, name=f\"a{hi}\")\n",
    "    \n",
    "    return ai\n",
    "\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden = hidden_layer(X, n_hidden, 1)\n",
    "    # Creating 5 ELU layers\n",
    "    for h_ix in range(2, 6):\n",
    "        hidden = hidden_layer(hidden, n_hidden, h_ix)\n",
    "    output = tf.layers.dense(hidden, n_output, name=\"output\")\n",
    "    \n",
    "\n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "        labels=y,\n",
    "        logits=output\n",
    "    )\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "\n",
    "    \n",
    "alpha = 0.005\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.AdamOptimizer(alpha)\n",
    "    train_step = optimizer.minimize(loss, name=\"train_step\")\n",
    "\n",
    "\n",
    "with tf.name_scope(\"metrics\"):\n",
    "    correct = tf.nn.in_top_k(output, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name=\"accuracy\")\n",
    "    tb_train_writer = tf.summary.FileWriter(logs + \"/train\", tf.get_default_graph())\n",
    "    tb_cv_writer = tf.summary.FileWriter(logs + \"/cv\", tf.get_default_graph())\n",
    "    tb_acc = tf.summary.scalar(\"Accuracy\", accuracy)\n",
    "    \n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@Epoch 000: Train Accuracy 043.06% | Test Accuracy 043.12%\n",
      "@Epoch 040: Train Accuracy 096.41% | Test Accuracy 095.82%\n",
      "@Epoch 080: Train Accuracy 098.79% | Test Accuracy 097.35%\n",
      "@Epoch 120: Train Accuracy 098.74% | Test Accuracy 097.15%\n",
      "@Epoch 160: Train Accuracy 099.95% | Test Accuracy 097.65%\n",
      "@Epoch 200: Train Accuracy 100.00% | Test Accuracy 097.68%\n",
      "@Epoch 240: Train Accuracy 100.00% | Test Accuracy 097.77%\n",
      "@Epoch 280: Train Accuracy 100.00% | Test Accuracy 097.81%\n",
      "@Epoch 320: Train Accuracy 100.00% | Test Accuracy 097.84%\n",
      "@Epoch 360: Train Accuracy 100.00% | Test Accuracy 097.85%\n",
      "@Epoch 399: Train Accuracy 100.00% | Test Accuracy 097.84%\r"
     ]
    }
   ],
   "source": [
    "# Operations to compute the running mean and variance.\n",
    "# ----------------------------------------------------\n",
    "# In general, *tf.GraphKeys*, is a collection of names\n",
    "# to collect and retrieve values associated with a graph\n",
    "extra_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "model_path = \"tfmodels/ffnn_v05.ckpt\"\n",
    "epochs = 400\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for epoch in range(epochs):\n",
    "        sess.run([train_step, extra_ops], feed_dict={X: X_train, y: y_train,\n",
    "                                                     training: True})\n",
    "        acc, bn_train_acc = sess.run([accuracy, tb_acc], feed_dict={X: X_train, y:y_train})\n",
    "        cvacc, bn_cv_acc = sess.run([accuracy, tb_acc], feed_dict={X: X_test, y:y_test})\n",
    "        tb_train_writer.add_summary(bn_train_acc, epoch)\n",
    "        tb_cv_writer.add_summary(bn_cv_acc, epoch)\n",
    "        end = \"\\n\" if epoch % 40 == 0 else \"\\r\"\n",
    "        print(f\"@Epoch {epoch:03}: Train Accuracy {acc:07.2%} | Test Accuracy {cvacc:07.2%}\",\n",
    "              end=end)\n",
    "    saver.save(sess, model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Restoring the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./tfmodels/ffnn_v05.ckpt\n",
      "@Epoch 000: Train Accuracy 100.00% | Test Accuracy 097.84%\n",
      "@Epoch 010: Train Accuracy 100.00% | Test Accuracy 097.84%\n",
      "@Epoch 020: Train Accuracy 100.00% | Test Accuracy 097.83%\n",
      "@Epoch 030: Train Accuracy 100.00% | Test Accuracy 097.83%\n",
      "@Epoch 039: Train Accuracy 100.00% | Test Accuracy 097.85%\r"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "model = tf.train.import_meta_graph(\"./tfmodels/ffnn_v05.ckpt.meta\")\n",
    "new_model = tf.train.Saver()\n",
    "new_model_path = \"./tfmodels/ffnn_v05_1.ckpt\"\n",
    "\n",
    "X = tf.get_default_graph().get_tensor_by_name(\"X:0\")\n",
    "y = tf.get_default_graph().get_tensor_by_name(\"y:0\")\n",
    "training = tf.get_default_graph().get_tensor_by_name(\"training_bool:0\")\n",
    "accuracy = tf.get_default_graph().get_tensor_by_name(\"metrics/accuracy:0\")\n",
    "train_step = tf.get_default_graph().get_operation_by_name(\"train/train_step\")\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "bn_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "epochs = 40\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    model.restore(sess, \"./tfmodels/ffnn_v05.ckpt\")\n",
    "    for epoch in range(epochs):\n",
    "        sess.run([train_step, bn_ops], feed_dict={X: X_train, y: y_train,\n",
    "                                                  training: True})\n",
    "        acc = sess.run(accuracy, feed_dict={X: X_train, y: y_train})\n",
    "        cvacc = sess.run(accuracy, feed_dict={X: X_test, y: y_test})\n",
    "        end = \"\\n\" if epoch % 10 == 0 else \"\\r\"\n",
    "        print(f\"@Epoch {epoch:03}: Train Accuracy {acc:07.2%} | Test Accuracy {cvacc:07.2%}\",\n",
    "              end=end)\n",
    "    new_model.save(sess, new_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "----\n",
    "# Transfer Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.random import choice, seed\n",
    "from functools import partial\n",
    "from math import ceil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_train_0_4 = y_train < 5\n",
    "y_0_4_train = y_train[map_train_0_4] \n",
    "X_0_4_train = X_train[map_train_0_4,:]\n",
    "\n",
    "map_test_0_4 = y_test < 5\n",
    "y_0_4_test = y_test[map_test_0_4]\n",
    "X_0_4_test = X_test[map_test_0_4, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Learning V.01\n",
    "We being by training a feed-forward neural network with 5 hidden layers, elu activation function and he initializiation. In order to train this NNet, we consider Adam optimization and early stopping.\n",
    "\n",
    "For educational purposes, we will consider the test-set as the validation set. Early stopping will kick in once the accuracy on the validation (test) set at epoch $t$ drops below the accuracy at epoch $t-1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "logs = \"./tf_logs/transfer/ffnn_0_4\"\n",
    "\n",
    "n_inputs = 28 * 28\n",
    "n_hidden = 100\n",
    "n_output = 5\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int32, shape=(None), name=\"y\")\n",
    "he_init = variance_scaling_initializer()\n",
    "\n",
    "hidden_layer = partial(tf.layers.dense, units=n_hidden,\n",
    "                       activation=tf.nn.elu,\n",
    "                       kernel_initializer=he_init)\n",
    "\n",
    "with tf.name_scope(\"DNN\"):\n",
    "    for hi in range(1, 6):\n",
    "        if hi == 1:\n",
    "            hidden = hidden_layer(inputs=X, name=f\"hidden_{hi}\")\n",
    "        else:\n",
    "            hidden = hidden_layer(inputs=hidden, name=f\"hidden_{hi}\")\n",
    "\n",
    "    output = hidden_layer(inputs=hidden, units=n_output,\n",
    "                          activation=None, name=\"output\")\n",
    "\n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "        labels=y,\n",
    "        logits=output)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "\n",
    "\n",
    "alpha = 0.001\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.AdamOptimizer(alpha)\n",
    "    train_step = optimizer.minimize(loss, name=\"train_step\")\n",
    "\n",
    "\n",
    "with tf.name_scope(\"metrics\"):\n",
    "    correct = tf.nn.in_top_k(output, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name=\"accuracy\")\n",
    "    ### Writer Configuration ###\n",
    "    writer_train = tf.summary.FileWriter(logs + \"/train\", tf.get_default_graph())\n",
    "    writer_test = tf.summary.FileWriter(logs + \"/test\", tf.get_default_graph())\n",
    "    writer_accuracy = tf.summary.scalar(\"Accuracy\", accuracy)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "model_saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch @000 Train Acc: 063.78% | Test Acc: 062.87% | ∆%: 100.0000% | Test loss: 147.022% |\n",
      "Epoch @020 Train Acc: 095.47% | Test Acc: 096.26% | ∆%: 003.2442% | Test loss: 012.223% |\n",
      "Epoch @040 Train Acc: 097.14% | Test Acc: 097.65% | ∆%: 001.2901% | Test loss: 007.678% |\n",
      "Epoch @060 Train Acc: 097.72% | Test Acc: 098.03% | ∆%: 001.3796% | Test loss: 006.510% | Stag: *\n",
      "Epoch @080 Train Acc: 098.10% | Test Acc: 098.15% | ∆%: 001.0023% | Test loss: 005.961% | Stag: *\n",
      "Epoch @188 Train Acc: 099.42% | Test Acc: 098.74% | ∆%: 017.4311% | Test loss: 004.557% | Stag: ******\n",
      "Epoch @200 Train Acc: 099.66% | Test Acc: 098.87% | ∆%: 003.8576% | Test loss: 003.959% | Stag: ******\n",
      "Epoch @201 Train Acc: 099.59% | Test Acc: 098.85% | ∆%: 009.2648% | Test loss: 004.165% | Stag: ******\n",
      "Epoch @202 Train Acc: 099.68% | Test Acc: 098.87% | ∆%: 000.8255% | Test loss: 003.844% | Stag: *******\n",
      "Epoch @203 Train Acc: 099.65% | Test Acc: 098.95% | ∆%: 000.1273% | Test loss: 003.817% | Stag: *******\n",
      "Epoch @204 Train Acc: 099.69% | Test Acc: 098.93% | ∆%: 000.5236% | Test loss: 003.792% | Stag: ********\n",
      "Epoch @205 Train Acc: 099.67% | Test Acc: 098.83% | ∆%: 004.9144% | Test loss: 004.000% | Stag: ********\n",
      "Epoch @206 Train Acc: 099.68% | Test Acc: 098.85% | ∆%: 004.9584% | Test loss: 004.001% | Stag: *********\n",
      "Epoch @207 Train Acc: 099.72% | Test Acc: 098.93% | ∆%: 000.0686% | Test loss: 003.815% | Stag: *********\n",
      "Epoch @208 Train Acc: 099.69% | Test Acc: 098.91% | ∆%: 000.1199% | Test loss: 003.817% | Stag: **********\n",
      "Epoch @209 Train Acc: 099.73% | Test Acc: 098.93% | ∆%: 001.4499% | Test loss: 003.867% | Stag: **********\n",
      "Early Stopping...\n",
      "The loss change rate was of 3.93860%\n"
     ]
    }
   ],
   "source": [
    "epochs = 400\n",
    "current_test_loss = 10 ** 10\n",
    "max_stag_threshold = 20\n",
    "current_stag = 0\n",
    "mean_stag_val = 0\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for epoch in range(epochs):\n",
    "        sess.run(train_step, feed_dict={X: X_0_4_train, y: y_0_4_train})\n",
    "        train_acc, writer_train_acc = sess.run([accuracy, writer_accuracy],\n",
    "                                               feed_dict={X: X_0_4_train, y: y_0_4_train})\n",
    "        test_acc, writer_test_acc = sess.run([accuracy, writer_accuracy],\n",
    "                                              feed_dict={X: X_0_4_test, y: y_0_4_test})\n",
    "        \n",
    "        test_loss = sess.run(loss, feed_dict={X: X_0_4_test, y: y_0_4_test})\n",
    "        # Add elements to summary\n",
    "        writer_test.add_summary(writer_test_acc, global_step=epoch)\n",
    "        writer_train.add_summary(writer_train_acc, global_step=epoch)\n",
    "        # We consider an stagnation if, for 20 steps, the test set does not\n",
    "        # move either way more than 2% from the current value\n",
    "        delta_test_loss = abs(test_loss / current_test_loss - 1)\n",
    "        if delta_test_loss < 0.01 or test_loss > current_test_loss:\n",
    "            if current_stag >= max_stag_threshold:\n",
    "                print(f\"Early Stopping...\\nThe loss change rate was of {mean_stag_val / max_stag_threshold:0.5%}\")\n",
    "                break\n",
    "            else:\n",
    "                current_stag += 1\n",
    "                mean_stag_val += delta_test_loss\n",
    "                end = \"\\n\" if current_stag > 10 else \"\\r\"\n",
    "                print((f\"Epoch @{epoch:03} Train Acc: {train_acc:07.02%} \"\n",
    "                       f\"| Test Acc: {test_acc:07.02%} | ∆%: {delta_test_loss:09.4%} \"\n",
    "                       f\"| Test loss: {test_loss:08.3%} | Stag: {'*' * ceil(current_stag / 2)}\"), end=end)\n",
    "        else:\n",
    "            current_stag = 0\n",
    "            mean_stag_val = 0\n",
    "            end = \"\\n\" if epoch % 20 == 0 else \"\\r\"\n",
    "            print((f\"Epoch @{epoch:03} Train Acc: {train_acc:07.02%} \"\n",
    "                   f\"| Test Acc: {test_acc:07.02%} | ∆%: {delta_test_loss:09.4%} \"\n",
    "                   f\"| Test loss: {test_loss:08.3%} |\"), end=end)\n",
    "            current_test_loss = test_loss\n",
    "\n",
    "    model_saver.save(sess, \"./tfmodels/nnet_0_4_v01.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Learning V.02\n",
    "In order to find a better preforming 5-layer Deep Neural Network, we proceed by creating a `DNN` class with homogeneous number of neurons in each of the hidden layers. We will make use of use of `BaseEstimator` and `ClassifierMixin` in order to cross-validate the performance and pick a better model.\n",
    "\n",
    "**NOTE:** This implementation does not take into account a batch size in order to fit the model. This simple class is to be used with training data that fits into memory. (See [tf.data](https://www.tensorflow.org/programmers_guide/datasets) for an optimized way to fit models in TensorFlow via batch-GD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.exceptions import NotFittedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_train_0_4 = y_train < 5\n",
    "y_0_4_train = y_train[map_train_0_4] \n",
    "X_0_4_train = X_train[map_train_0_4,:]\n",
    "\n",
    "y_0_4_cv, y_0_4_train = y_0_4_train[:5000], y_0_4_train[5000:]\n",
    "X_0_4_cv, X_0_4_train = X_0_4_train[:5000], X_0_4_train[5000:]\n",
    "\n",
    "map_test_0_4 = y_test < 5\n",
    "y_0_4_test = y_test[map_test_0_4]\n",
    "X_0_4_test = X_test[map_test_0_4, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DnnClassifier(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, n_hidden=5, n_neurons=100, optimizer=tf.train.AdamOptimizer,\n",
    "                 learning_rate=0.01, activation_fn=tf.nn.elu,\n",
    "                 initializer=tf.variance_scaling_initializer(),\n",
    "                 bn_momentum=None, dropout_rate=None, seed_state=None):\n",
    "        self.n_hidden = n_hidden\n",
    "        self.n_neurons = n_neurons\n",
    "        self.optimizer = optimizer\n",
    "        self.learning_rate = learning_rate\n",
    "        self.activation_fn = activation_fn\n",
    "        self.initializer = initializer # default He-initializiation\n",
    "        self.bn_momentum = bn_momentum\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.seed_state = seed_state\n",
    "        self._session = None\n",
    "    \n",
    "    def _dnn(self, inputs):\n",
    "        \"\"\"\n",
    "        Build hidden layers with optional support for batch normalization and dropout\n",
    "        \"\"\"\n",
    "        for hi in range(self.n_hidden):\n",
    "            # Dropout is applied *after* the activation function,\n",
    "            # if it is not None, this first step is applying dropout\n",
    "            # to the last seen layer (X being the first 'input' layer)\n",
    "            if self.dropout_rate:\n",
    "                inputs = tf.layers.dropout(inputs, rate=self.dropout_rate, training=self._training)\n",
    "                \n",
    "            inputs = tf.layers.dense(inputs, self.n_neurons, activation=self.activation_fn,\n",
    "                                     kernel_initializer=self.initializer,\n",
    "                                     name=f\"z_{hi + 1}\")\n",
    "            \n",
    "            if self.bn_momentum:\n",
    "                inputs = tf.layers.batch_normalization(inputs, momentum=self.bn_momentum,\n",
    "                                                       training=self._training)\n",
    "            inputs = self.activation_fn(inputs, name=f\"a_{hi + 1}\")\n",
    "        \n",
    "        return inputs\n",
    "    \n",
    "    def _build_graph(self, n_inputs, n_outputs):\n",
    "        y = tf.placeholder(tf.int32, shape=(None), name=\"y\")\n",
    "        \n",
    "        if self.bn_momentum or self.dropout_rate:\n",
    "            self._training = tf.placeholder_with_default(False, shape=(), name=\"training\") \n",
    "        else:\n",
    "            self._training = None\n",
    "            \n",
    "        with tf.name_scope(\"DNN\"):\n",
    "            X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "            final_hidden_layer = self._dnn(X)\n",
    "            logits = tf.layers.dense(final_hidden_layer, n_outputs,\n",
    "                                     kernel_initializer=self.initializer,\n",
    "                                     name=\"logits\")\n",
    "            y_proba = tf.nn.softmax(logits, name=\"y_proba\")\n",
    "            \n",
    "        with tf.name_scope(\"loss\"):\n",
    "            xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "                logits=logits,\n",
    "                labels=y)\n",
    "            loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "            \n",
    "        with tf.name_scope(\"train\"):\n",
    "            optimizer = self.optimizer(self.learning_rate)\n",
    "            train_step = optimizer.minimize(loss, name=\"train_step\")\n",
    "            \n",
    "        with tf.name_scope(\"metrics\"):\n",
    "            correct = tf.nn.in_top_k(logits, y, 1)\n",
    "            accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "        \n",
    "        init = tf.global_variables_initializer()\n",
    "        saver = tf.train.Saver()\n",
    "        \n",
    "        self._X = X\n",
    "        self._y = y\n",
    "        self._y_proba = y_proba\n",
    "        self._loss = loss\n",
    "        self._train_step = train_step\n",
    "        self._accuracy = accuracy\n",
    "        self._init = init\n",
    "        self._saver = saver\n",
    "        \n",
    "    def close_session(self):\n",
    "        if self._session:\n",
    "            self._session.close()\n",
    "\n",
    "    def _get_model_parameters(self):\n",
    "        \"\"\"\n",
    "        Get all variable parameters\n",
    "        \"\"\"\n",
    "        with self._graph.as_default():\n",
    "            glob_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES)\n",
    "        \n",
    "        return {glob_var.op.name: value for glob_var, value\n",
    "                in zip(glob_vars, self._session.run(glob_vars))}\n",
    "    \n",
    "    def _restore_model_parameters(self, model_params):\n",
    "        glob_vars_names = list(model_params.keys())\n",
    "        assign_ops = {\n",
    "            glob_var_name: self._graph.get_operation_by_name(glob_var_name + \"/Assign\")\n",
    "            for glob_var_name in glob_vars_names\n",
    "        }\n",
    "        init_values = {gvar_name: assign_op.inputs[1]\n",
    "                       for gvar_name, assign_op in assign_ops.items()}\n",
    "        feed_dict = {init_values[glob_var_name]: model_params[glob_var_name]\n",
    "                     for glob_var_name in glob_vars_names}\n",
    "        \n",
    "        self._session.run(assign_ops, feed_dict=feed_dict)\n",
    "        \n",
    "    def fit(self, X, y, epochs, X_cv=None, y_cv=None):\n",
    "        self.close_session()\n",
    "        \n",
    "        self.classes_ = np.unique(y) # Retrieve unique indices and sort\n",
    "        n_inputs = X.shape[1]\n",
    "        n_outputs = len(self.classes_)\n",
    "        \n",
    "        # Convert labels to a sorted index from 0 to n_classes - 1\n",
    "        self.class_to_index_ = {label: index for label, index\n",
    "                                in enumerate(self.classes_)}\n",
    "        y = np.array([self.class_to_index_[label] for label in y],\n",
    "                     dtype=np.int32)\n",
    "        \n",
    "        # ******** Early Stopping utility parameters ********\n",
    "        max_epochs_without_progress = 20\n",
    "        epochs_without_progress = 0\n",
    "        best_loss = np.infty\n",
    "        best_params = None\n",
    "                \n",
    "        # Defining the graph inside the class.\n",
    "        # By calling the _build_graph method\n",
    "        self._graph = tf.Graph()\n",
    "        with self._graph.as_default():\n",
    "            self._build_graph(n_inputs, n_outputs)\n",
    "            # Extra operations for batch normalization (if needed)\n",
    "            extra_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "        \n",
    "        self._session = tf.Session(graph=self._graph)\n",
    "        with self._session.as_default() as sess:\n",
    "            sess.run(self._init)\n",
    "            for epoch in range(epochs):\n",
    "                # Creating the Feed-Dict dictionary\n",
    "                feed_dict = {self._X: X, self._y: y}                \n",
    "                if self._training is not None:\n",
    "                    feed_dict[self._training] = True\n",
    "                \n",
    "                if self.bn_momentum is not None:\n",
    "                    sess.run([self._train_step, extra_ops], feed_dict=feed_dict)\n",
    "                else:\n",
    "                    sess.run(self._train_step, feed_dict=feed_dict)\n",
    "                    \n",
    "                end = \"\\n\" if epoch % (epochs // 10) == 0 else \"\\r\"\n",
    "                if X_cv is not None and y_cv is not None:\n",
    "                    loss_cv, acc_cv = sess.run([self._loss, self._accuracy],\n",
    "                                               feed_dict={self._X: X_cv, self._y: y_cv})\n",
    "                    if loss_cv < best_loss:\n",
    "                        best_params = self._get_model_parameters()\n",
    "                        best_loss = loss_cv\n",
    "                        epochs_without_progress = 0\n",
    "                    else:\n",
    "                        epochs_without_progress += 1\n",
    "                    \n",
    "                    print((f\"@E{epoch}\\tCV Loss: {loss_cv:0.3f}\\tCV Acc: {acc_cv:0.3%}\"\n",
    "                           f\"\\tEWP:{epochs_without_progress:02}\"), end=end)\n",
    "                    \n",
    "                    if epochs_without_progress > max_epochs_without_progress:\n",
    "                        print(\"\\n\\n...Early Stopping\")\n",
    "                        break\n",
    "                    \n",
    "                else:\n",
    "                    acc_train = sess.run(self._accuracy, feed_dict)\n",
    "                    print(f\"@E{epoch}, training accuracy: {acc_train:07.3%}\",\n",
    "                          end=end)\n",
    "            \n",
    "            # If early stopping was used, rollback to the best model found\n",
    "            if best_params is not None:\n",
    "                self._restore_model_parameters(best_params)\n",
    "            \n",
    "            return self\n",
    "        \n",
    "    def predict_proba(self, X):\n",
    "        if self._session is None:\n",
    "            raise NotFittedError(f\"This {self.__class__.__name__} instance is not fitted yet\")\n",
    "\n",
    "        with self._session.as_default() as sess:\n",
    "            y_proba_result = sess.run(self._y_proba, feed_dict={self._X: X})\n",
    "\n",
    "        return y_proba_result\n",
    "\n",
    "    def predict(self, X):\n",
    "        class_indices = np.argmax(self.predict_proba(X), axis=1)\n",
    "        return np.array([self.classes_[label] for label in class_indices],\n",
    "                        dtype=np.int32)\n",
    "\n",
    "    def save(self, path):\n",
    "        self._saver(self._session, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@E0\tCV Loss: 1.300\tCV Acc: 51.540%\tEWP:00\n",
      "@E40\tCV Loss: 0.071\tCV Acc: 97.840%\tEWP:00\n",
      "@E80\tCV Loss: 0.059\tCV Acc: 98.380%\tEWP:03\n",
      "@E111\tCV Loss: 0.064\tCV Acc: 98.360%\tEWP:21\n",
      "\n",
      "...Early Stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DnnClassifier(activation_fn=<function elu at 0x11206bf28>, bn_momentum=None,\n",
       "       dropout_rate=None,\n",
       "       initializer=<tensorflow.python.ops.init_ops.VarianceScaling object at 0x184c627cf8>,\n",
       "       learning_rate=0.01, n_hidden=5, n_neurons=100,\n",
       "       optimizer=<class 'tensorflow.python.training.adam.AdamOptimizer'>,\n",
       "       seed_state=None)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dnn01 = DnnClassifier()\n",
    "dnn01.fit(X_0_4_train, y_0_4_train, 400, X_0_4_cv, y_0_4_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.989492119089317"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(y_0_4_test, dnn01.predict(X_0_4_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n",
      "[CV] n_neurons=50, n_hidden=7, learning_rate=0.001, dropout_rate=0.4, activation_fn=<function relu at 0x112085d08> \n",
      "@E0\tCV Loss: 1.606\tCV Acc: 22.960%\tEWP:00\n",
      "@E150\tCV Loss: 0.342\tCV Acc: 78.420%\tEWP:00\n",
      "@E300\tCV Loss: 0.095\tCV Acc: 97.700%\tEWP:01\n",
      "@E320\tCV Loss: 0.097\tCV Acc: 97.740%\tEWP:21\n",
      "\n",
      "...Early Stopping\n",
      "[CV]  n_neurons=50, n_hidden=7, learning_rate=0.001, dropout_rate=0.4, activation_fn=<function relu at 0x112085d08>, total= 1.4min\n",
      "[CV] n_neurons=50, n_hidden=7, learning_rate=0.001, dropout_rate=0.4, activation_fn=<function relu at 0x112085d08> \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  1.4min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@E0\tCV Loss: 1.609\tCV Acc: 22.080%\tEWP:00\n",
      "@E150\tCV Loss: 0.394\tCV Acc: 77.300%\tEWP:00\n",
      "@E260\tCV Loss: 0.358\tCV Acc: 78.480%\tEWP:21\n",
      "\n",
      "...Early Stopping\n",
      "[CV]  n_neurons=50, n_hidden=7, learning_rate=0.001, dropout_rate=0.4, activation_fn=<function relu at 0x112085d08>, total= 1.2min\n",
      "[CV] n_neurons=50, n_hidden=7, learning_rate=0.001, dropout_rate=0.4, activation_fn=<function relu at 0x112085d08> \n",
      "@E0\tCV Loss: 1.606\tCV Acc: 32.160%\tEWP:00\n",
      "@E150\tCV Loss: 0.363\tCV Acc: 78.720%\tEWP:00\n",
      "@E300\tCV Loss: 0.089\tCV Acc: 97.740%\tEWP:05\n",
      "@E411\tCV Loss: 0.075\tCV Acc: 98.080%\tEWP:21\n",
      "\n",
      "...Early Stopping\n",
      "[CV]  n_neurons=50, n_hidden=7, learning_rate=0.001, dropout_rate=0.4, activation_fn=<function relu at 0x112085d08>, total= 1.9min\n",
      "[CV] n_neurons=200, n_hidden=11, learning_rate=0.1, dropout_rate=0.2, activation_fn=<function relu at 0x112085d08> \n",
      "@E0\tCV Loss: 243379632.000\tCV Acc: 19.380%\tEWP:00\n",
      "@E108\tCV Loss: 1.608\tCV Acc: 22.020%\tEWP:21\n",
      "\n",
      "...Early Stopping\n",
      "[CV]  n_neurons=200, n_hidden=11, learning_rate=0.1, dropout_rate=0.2, activation_fn=<function relu at 0x112085d08>, total= 2.1min\n",
      "[CV] n_neurons=200, n_hidden=11, learning_rate=0.1, dropout_rate=0.2, activation_fn=<function relu at 0x112085d08> \n",
      "@E0\tCV Loss: 2485434880.000\tCV Acc: 19.380%\tEWP:00\n",
      "@E105\tCV Loss: 1.608\tCV Acc: 22.020%\tEWP:21\n",
      "\n",
      "...Early Stopping\n",
      "[CV]  n_neurons=200, n_hidden=11, learning_rate=0.1, dropout_rate=0.2, activation_fn=<function relu at 0x112085d08>, total= 2.0min\n",
      "[CV] n_neurons=200, n_hidden=11, learning_rate=0.1, dropout_rate=0.2, activation_fn=<function relu at 0x112085d08> \n",
      "@E0\tCV Loss: 289719136.000\tCV Acc: 20.140%\tEWP:00\n",
      "@E60\tCV Loss: 1.609\tCV Acc: 22.020%\tEWP:2101\n",
      "\n",
      "...Early Stopping\n",
      "[CV]  n_neurons=200, n_hidden=11, learning_rate=0.1, dropout_rate=0.2, activation_fn=<function relu at 0x112085d08>, total= 1.2min\n",
      "[CV] n_neurons=100, n_hidden=5, learning_rate=0.001, dropout_rate=0.5, activation_fn=<function elu at 0x11206bf28> \n",
      "@E0\tCV Loss: 1.515\tCV Acc: 71.400%\tEWP:00\n",
      "@E150\tCV Loss: 0.090\tCV Acc: 97.400%\tEWP:00\n",
      "@E300\tCV Loss: 0.064\tCV Acc: 98.220%\tEWP:00\n",
      "@E450\tCV Loss: 0.055\tCV Acc: 98.440%\tEWP:08\n",
      "@E595\tCV Loss: 0.050\tCV Acc: 98.580%\tEWP:21\n",
      "\n",
      "...Early Stopping\n",
      "[CV]  n_neurons=100, n_hidden=5, learning_rate=0.001, dropout_rate=0.5, activation_fn=<function elu at 0x11206bf28>, total= 3.4min\n",
      "[CV] n_neurons=100, n_hidden=5, learning_rate=0.001, dropout_rate=0.5, activation_fn=<function elu at 0x11206bf28> \n",
      "@E0\tCV Loss: 1.497\tCV Acc: 62.240%\tEWP:00\n",
      "@E150\tCV Loss: 0.089\tCV Acc: 97.360%\tEWP:00\n",
      "@E300\tCV Loss: 0.063\tCV Acc: 98.140%\tEWP:00\n",
      "@E419\tCV Loss: 0.056\tCV Acc: 98.460%\tEWP:21\n",
      "\n",
      "...Early Stopping\n",
      "[CV]  n_neurons=100, n_hidden=5, learning_rate=0.001, dropout_rate=0.5, activation_fn=<function elu at 0x11206bf28>, total= 2.4min\n",
      "[CV] n_neurons=100, n_hidden=5, learning_rate=0.001, dropout_rate=0.5, activation_fn=<function elu at 0x11206bf28> \n",
      "@E0\tCV Loss: 1.525\tCV Acc: 66.500%\tEWP:00\n",
      "@E150\tCV Loss: 0.084\tCV Acc: 97.600%\tEWP:00\n",
      "@E300\tCV Loss: 0.064\tCV Acc: 98.100%\tEWP:07\n",
      "@E436\tCV Loss: 0.057\tCV Acc: 98.460%\tEWP:21\n",
      "\n",
      "...Early Stopping\n",
      "[CV]  n_neurons=100, n_hidden=5, learning_rate=0.001, dropout_rate=0.5, activation_fn=<function elu at 0x11206bf28>, total= 2.5min\n",
      "[CV] n_neurons=300, n_hidden=13, learning_rate=0.001, dropout_rate=0.4, activation_fn=<function relu at 0x112085d08> \n",
      "@E0\tCV Loss: 1.609\tCV Acc: 22.020%\tEWP:00\n",
      "@E44\tCV Loss: 2.101\tCV Acc: 40.480%\tEWP:21\n",
      "\n",
      "...Early Stopping\n",
      "[CV]  n_neurons=300, n_hidden=13, learning_rate=0.001, dropout_rate=0.4, activation_fn=<function relu at 0x112085d08>, total= 1.5min\n",
      "[CV] n_neurons=300, n_hidden=13, learning_rate=0.001, dropout_rate=0.4, activation_fn=<function relu at 0x112085d08> \n",
      "@E0\tCV Loss: 1.609\tCV Acc: 22.020%\tEWP:00\n",
      "@E150\tCV Loss: 0.115\tCV Acc: 97.620%\tEWP:07\n",
      "@E248\tCV Loss: 0.063\tCV Acc: 98.700%\tEWP:21\n",
      "\n",
      "...Early Stopping\n",
      "[CV]  n_neurons=300, n_hidden=13, learning_rate=0.001, dropout_rate=0.4, activation_fn=<function relu at 0x112085d08>, total= 8.2min\n",
      "[CV] n_neurons=300, n_hidden=13, learning_rate=0.001, dropout_rate=0.4, activation_fn=<function relu at 0x112085d08> \n",
      "@E0\tCV Loss: 1.609\tCV Acc: 22.020%\tEWP:00\n",
      "@E73\tCV Loss: 1.477\tCV Acc: 49.680%\tEWP:21\n",
      "\n",
      "...Early Stopping\n",
      "[CV]  n_neurons=300, n_hidden=13, learning_rate=0.001, dropout_rate=0.4, activation_fn=<function relu at 0x112085d08>, total= 2.6min\n",
      "[CV] n_neurons=50, n_hidden=13, learning_rate=0.001, dropout_rate=0.2, activation_fn=<function elu at 0x11206bf28> \n",
      "@E0\tCV Loss: 1.592\tCV Acc: 46.160%\tEWP:00\n",
      "@E150\tCV Loss: 0.094\tCV Acc: 97.980%\tEWP:00\n",
      "@E258\tCV Loss: 0.076\tCV Acc: 98.340%\tEWP:21\n",
      "\n",
      "...Early Stopping\n",
      "[CV]  n_neurons=50, n_hidden=13, learning_rate=0.001, dropout_rate=0.2, activation_fn=<function elu at 0x11206bf28>, total= 1.7min\n",
      "[CV] n_neurons=50, n_hidden=13, learning_rate=0.001, dropout_rate=0.2, activation_fn=<function elu at 0x11206bf28> \n",
      "@E0\tCV Loss: 1.585\tCV Acc: 56.700%\tEWP:00\n",
      "@E150\tCV Loss: 0.096\tCV Acc: 97.960%\tEWP:03\n",
      "@E287\tCV Loss: 0.076\tCV Acc: 98.500%\tEWP:21\n",
      "\n",
      "...Early Stopping\n",
      "[CV]  n_neurons=50, n_hidden=13, learning_rate=0.001, dropout_rate=0.2, activation_fn=<function elu at 0x11206bf28>, total= 1.9min\n",
      "[CV] n_neurons=50, n_hidden=13, learning_rate=0.001, dropout_rate=0.2, activation_fn=<function elu at 0x11206bf28> \n",
      "@E0\tCV Loss: 1.588\tCV Acc: 49.520%\tEWP:00\n",
      "@E150\tCV Loss: 0.092\tCV Acc: 98.060%\tEWP:04\n",
      "@E210\tCV Loss: 0.083\tCV Acc: 98.240%\tEWP:21\n",
      "\n",
      "...Early Stopping\n",
      "[CV]  n_neurons=50, n_hidden=13, learning_rate=0.001, dropout_rate=0.2, activation_fn=<function elu at 0x11206bf28>, total= 1.4min\n",
      "[CV] n_neurons=300, n_hidden=7, learning_rate=0.001, dropout_rate=0.3, activation_fn=<function elu at 0x11206bf28> \n",
      "@E0\tCV Loss: 1.322\tCV Acc: 85.740%\tEWP:00\n",
      "@E150\tCV Loss: 0.046\tCV Acc: 98.680%\tEWP:00\n",
      "@E262\tCV Loss: 0.043\tCV Acc: 98.920%\tEWP:21\n",
      "\n",
      "...Early Stopping\n",
      "[CV]  n_neurons=300, n_hidden=7, learning_rate=0.001, dropout_rate=0.3, activation_fn=<function elu at 0x11206bf28>, total= 5.6min\n",
      "[CV] n_neurons=300, n_hidden=7, learning_rate=0.001, dropout_rate=0.3, activation_fn=<function elu at 0x11206bf28> \n",
      "@E0\tCV Loss: 1.320\tCV Acc: 87.520%\tEWP:00\n",
      "@E150\tCV Loss: 0.044\tCV Acc: 98.720%\tEWP:08\n",
      "@E191\tCV Loss: 0.040\tCV Acc: 98.840%\tEWP:21\n",
      "\n",
      "...Early Stopping\n",
      "[CV]  n_neurons=300, n_hidden=7, learning_rate=0.001, dropout_rate=0.3, activation_fn=<function elu at 0x11206bf28>, total= 4.1min\n",
      "[CV] n_neurons=300, n_hidden=7, learning_rate=0.001, dropout_rate=0.3, activation_fn=<function elu at 0x11206bf28> \n",
      "@E0\tCV Loss: 1.316\tCV Acc: 84.620%\tEWP:00\n",
      "@E150\tCV Loss: 0.054\tCV Acc: 98.720%\tEWP:08\n",
      "@E224\tCV Loss: 0.048\tCV Acc: 98.920%\tEWP:21\n",
      "\n",
      "...Early Stopping\n",
      "[CV]  n_neurons=300, n_hidden=7, learning_rate=0.001, dropout_rate=0.3, activation_fn=<function elu at 0x11206bf28>, total= 4.9min\n",
      "[CV] n_neurons=50, n_hidden=13, learning_rate=0.005, dropout_rate=0.2, activation_fn=<function elu at 0x11206bf28> \n",
      "@E0\tCV Loss: 1.539\tCV Acc: 43.860%\tEWP:00\n",
      "@E150\tCV Loss: 0.071\tCV Acc: 98.540%\tEWP:08\n",
      "@E177\tCV Loss: 0.076\tCV Acc: 98.520%\tEWP:21\n",
      "\n",
      "...Early Stopping\n",
      "[CV]  n_neurons=50, n_hidden=13, learning_rate=0.005, dropout_rate=0.2, activation_fn=<function elu at 0x11206bf28>, total= 1.2min\n",
      "[CV] n_neurons=50, n_hidden=13, learning_rate=0.005, dropout_rate=0.2, activation_fn=<function elu at 0x11206bf28> \n",
      "@E0\tCV Loss: 1.581\tCV Acc: 31.020%\tEWP:00\n",
      "@E150\tCV Loss: 0.073\tCV Acc: 98.440%\tEWP:01\n",
      "@E229\tCV Loss: 0.063\tCV Acc: 98.580%\tEWP:21\n",
      "\n",
      "...Early Stopping\n",
      "[CV]  n_neurons=50, n_hidden=13, learning_rate=0.005, dropout_rate=0.2, activation_fn=<function elu at 0x11206bf28>, total= 1.5min\n",
      "[CV] n_neurons=50, n_hidden=13, learning_rate=0.005, dropout_rate=0.2, activation_fn=<function elu at 0x11206bf28> \n",
      "@E0\tCV Loss: 1.521\tCV Acc: 47.320%\tEWP:00\n",
      "@E150\tCV Loss: 0.078\tCV Acc: 98.360%\tEWP:14\n",
      "@E157\tCV Loss: 0.077\tCV Acc: 98.280%\tEWP:21\n",
      "\n",
      "...Early Stopping\n",
      "[CV]  n_neurons=50, n_hidden=13, learning_rate=0.005, dropout_rate=0.2, activation_fn=<function elu at 0x11206bf28>, total= 1.1min\n",
      "[CV] n_neurons=250, n_hidden=7, learning_rate=0.0001, dropout_rate=0.6, activation_fn=<function relu at 0x112085d08> \n",
      "@E0\tCV Loss: 1.610\tCV Acc: 13.380%\tEWP:00\n",
      "@E150\tCV Loss: 1.321\tCV Acc: 42.960%\tEWP:00\n",
      "@E300\tCV Loss: 0.524\tCV Acc: 86.860%\tEWP:00\n",
      "@E450\tCV Loss: 0.217\tCV Acc: 94.580%\tEWP:00\n",
      "@E600\tCV Loss: 0.173\tCV Acc: 96.060%\tEWP:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@E750\tCV Loss: 0.153\tCV Acc: 96.720%\tEWP:14\n",
      "@E900\tCV Loss: 0.137\tCV Acc: 97.140%\tEWP:20\n",
      "@E901\tCV Loss: 0.137\tCV Acc: 97.160%\tEWP:21\n",
      "\n",
      "...Early Stopping\n",
      "[CV]  n_neurons=250, n_hidden=7, learning_rate=0.0001, dropout_rate=0.6, activation_fn=<function relu at 0x112085d08>, total=15.8min\n",
      "[CV] n_neurons=250, n_hidden=7, learning_rate=0.0001, dropout_rate=0.6, activation_fn=<function relu at 0x112085d08> \n",
      "@E0\tCV Loss: 1.609\tCV Acc: 24.220%\tEWP:00\n",
      "@E150\tCV Loss: 1.305\tCV Acc: 42.760%\tEWP:00\n",
      "@E300\tCV Loss: 0.496\tCV Acc: 89.080%\tEWP:00\n",
      "@E450\tCV Loss: 0.207\tCV Acc: 94.880%\tEWP:00\n",
      "@E600\tCV Loss: 0.162\tCV Acc: 96.400%\tEWP:00\n",
      "@E653\tCV Loss: 0.157\tCV Acc: 96.700%\tEWP:21\n",
      "\n",
      "...Early Stopping\n",
      "[CV]  n_neurons=250, n_hidden=7, learning_rate=0.0001, dropout_rate=0.6, activation_fn=<function relu at 0x112085d08>, total=11.9min\n",
      "[CV] n_neurons=250, n_hidden=7, learning_rate=0.0001, dropout_rate=0.6, activation_fn=<function relu at 0x112085d08> \n",
      "@E0\tCV Loss: 1.609\tCV Acc: 20.180%\tEWP:00\n",
      "@E150\tCV Loss: 1.331\tCV Acc: 41.500%\tEWP:00\n",
      "@E300\tCV Loss: 0.624\tCV Acc: 78.800%\tEWP:00\n",
      "@E450\tCV Loss: 0.228\tCV Acc: 94.180%\tEWP:00\n",
      "@E600\tCV Loss: 0.170\tCV Acc: 96.180%\tEWP:00\n",
      "@E750\tCV Loss: 0.143\tCV Acc: 97.040%\tEWP:00\n",
      "@E878\tCV Loss: 0.131\tCV Acc: 97.220%\tEWP:21\n",
      "\n",
      "...Early Stopping\n",
      "[CV]  n_neurons=250, n_hidden=7, learning_rate=0.0001, dropout_rate=0.6, activation_fn=<function relu at 0x112085d08>, total=15.2min\n",
      "[CV] n_neurons=200, n_hidden=11, learning_rate=0.1, dropout_rate=0.4, activation_fn=<function relu at 0x112085d08> \n",
      "@E0\tCV Loss: 13699617.000\tCV Acc: 20.140%\tEWP:00\n",
      "@E39\tCV Loss: 1.610\tCV Acc: 22.020%\tEWP:21\n",
      "\n",
      "...Early Stopping\n",
      "[CV]  n_neurons=200, n_hidden=11, learning_rate=0.1, dropout_rate=0.4, activation_fn=<function relu at 0x112085d08>, total=  48.6s\n",
      "[CV] n_neurons=200, n_hidden=11, learning_rate=0.1, dropout_rate=0.4, activation_fn=<function relu at 0x112085d08> \n",
      "@E0\tCV Loss: 419003584.000\tCV Acc: 19.380%\tEWP:00\n",
      "@E81\tCV Loss: 1.608\tCV Acc: 22.020%\tEWP:21\n",
      "\n",
      "...Early Stopping\n",
      "[CV]  n_neurons=200, n_hidden=11, learning_rate=0.1, dropout_rate=0.4, activation_fn=<function relu at 0x112085d08>, total= 1.5min\n",
      "[CV] n_neurons=200, n_hidden=11, learning_rate=0.1, dropout_rate=0.4, activation_fn=<function relu at 0x112085d08> \n",
      "@E0\tCV Loss: 13865692.000\tCV Acc: 19.380%\tEWP:00\n",
      "@E78\tCV Loss: 1.608\tCV Acc: 22.020%\tEWP:21\n",
      "\n",
      "...Early Stopping\n",
      "[CV]  n_neurons=200, n_hidden=11, learning_rate=0.1, dropout_rate=0.4, activation_fn=<function relu at 0x112085d08>, total= 1.5min\n",
      "[CV] n_neurons=200, n_hidden=7, learning_rate=0.005, dropout_rate=0.5, activation_fn=<function relu at 0x112085d08> \n",
      "@E0\tCV Loss: 1.610\tCV Acc: 30.520%\tEWP:00\n",
      "@E142\tCV Loss: 0.059\tCV Acc: 98.560%\tEWP:21\n",
      "\n",
      "...Early Stopping\n",
      "[CV]  n_neurons=200, n_hidden=7, learning_rate=0.005, dropout_rate=0.5, activation_fn=<function relu at 0x112085d08>, total= 1.8min\n",
      "[CV] n_neurons=200, n_hidden=7, learning_rate=0.005, dropout_rate=0.5, activation_fn=<function relu at 0x112085d08> \n",
      "@E0\tCV Loss: 1.605\tCV Acc: 20.140%\tEWP:00\n",
      "@E150\tCV Loss: 0.067\tCV Acc: 98.340%\tEWP:09\n",
      "@E216\tCV Loss: 0.066\tCV Acc: 98.320%\tEWP:21\n",
      "\n",
      "...Early Stopping\n",
      "[CV]  n_neurons=200, n_hidden=7, learning_rate=0.005, dropout_rate=0.5, activation_fn=<function relu at 0x112085d08>, total=28.0min\n",
      "[CV] n_neurons=200, n_hidden=7, learning_rate=0.005, dropout_rate=0.5, activation_fn=<function relu at 0x112085d08> \n",
      "@E0\tCV Loss: 1.609\tCV Acc: 20.140%\tEWP:00\n",
      "@E150\tCV Loss: 0.047\tCV Acc: 98.760%\tEWP:08\n",
      "@E196\tCV Loss: 0.045\tCV Acc: 98.820%\tEWP:21\n",
      "\n",
      "...Early Stopping\n",
      "[CV]  n_neurons=200, n_hidden=7, learning_rate=0.005, dropout_rate=0.5, activation_fn=<function relu at 0x112085d08>, total=20.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  30 out of  30 | elapsed: 151.4min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@E0\tCV Loss: 1.298\tCV Acc: 78.780%\tEWP:00\n",
      "@E150\tCV Loss: 0.044\tCV Acc: 98.760%\tEWP:02\n",
      "@E201\tCV Loss: 0.038\tCV Acc: 98.900%\tEWP:21\n",
      "\n",
      "...Early Stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=None, error_score='raise',\n",
       "          estimator=DnnClassifier(activation_fn=<function elu at 0x11206bf28>, bn_momentum=None,\n",
       "       dropout_rate=None,\n",
       "       initializer=<tensorflow.python.ops.init_ops.VarianceScaling object at 0x184c627cf8>,\n",
       "       learning_rate=0.01, n_hidden=5, n_neurons=100,\n",
       "       optimizer=<class 'tensorflow.python.training.adam.AdamOptimizer'>,\n",
       "       seed_state=None),\n",
       "          fit_params=None, iid=True, n_iter=10, n_jobs=1,\n",
       "          param_distributions={'n_hidden': [5, 7, 11, 13], 'n_neurons': [50, 100, 200, 250, 300], 'learning_rate': [0.1, 0.001, 0.005, 0.0001], 'activation_fn': [<function elu at 0x11206bf28>, <function relu at 0x112085d08>], 'dropout_rate': [0.2, 0.3, 0.4, 0.5, 0.6]},\n",
       "          pre_dispatch='2*n_jobs', random_state=None, refit=True,\n",
       "          return_train_score='warn', scoring=None, verbose=2)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "choser_params = {\n",
    "    \"n_hidden\": [5, 7, 11, 13],\n",
    "    \"n_neurons\": [50, 100, 200, 250, 300],\n",
    "    \"learning_rate\": [0.1, 0.001, 0.005, 0.0001],\n",
    "    \"activation_fn\": [tf.nn.elu, tf.nn.relu],\n",
    "    \"dropout_rate\": [0.2, 0.3, 0.4, 0.5, 0.6]\n",
    "}\n",
    "\n",
    "dnn02 = RandomizedSearchCV(DnnClassifier(), choser_params, verbose=2)\n",
    "dnn02.fit(X_0_4_train, y_0_4_train, **{\"X_cv\": X_0_4_cv, \"y_cv\": y_0_4_cv,  \"epochs\":1500})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
