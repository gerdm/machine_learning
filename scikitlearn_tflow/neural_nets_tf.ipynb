{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks With TensorFlow\n",
    "## Stable implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow import variance_scaling_initializer # He-initializer\n",
    "import matplotlib.pyplot as plt\n",
    "from jupyter_tf_graph import show_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "X_train = X_train.reshape(-1, 28 * 28) / 255.0\n",
    "X_test = X_test.reshape(-1, 28 * 28) / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAcUAAAHOCAYAAADpBhJHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAIABJREFUeJzt3Xm4lXW5//HP7VZRGQRiCBDZlQTSsZO2FaeSUJzSwMrUFMH55BBdB1PCOpKpEZXH43QSJ3AI5ZQCqb8U+anoTyU2OAChoilCTJsIcSAQvH9/rC+d/X3cew17rb2Gvd6v6/La+7Om52azuW+f57ueZ5m7CwAASDuVugAAAMoFQxEAgIChCABAwFAEACBgKAIAEDAUAQAIGIqAJDObYGb3lrqO1mJm483s9lLXAZQ7hiJKyswON7PnzOxdM9tgZv/PzA4sdV25MLO3zWyrmXVL3P6SmbmZ1YY8JeSDGj1mHzPzRvkpMzu3UR5vZm+Z2ftmttLMHgi3Lwm3vW9m283sH43y+GSN7n6tu5+bvB1AjKGIkjGzTpIelnSjpK6S+kj6qaQtpayrhd6SdNqOYGb7Sdq9icdtkHR1Ni9oZqMkjZR0lLt3kFQnaY4kufsX3L1DuP0ZSRfvyO5+bX5/FKB6MRRRSp+XJHef5u7b3X2zuz/u7q9Ikpl9zsz+r5n9zczWm9l9ZtZ5x5PDHtoPzewVM/vAzO4ws55m9n/M7D0ze8LMuoTH1oa9tPPNbJWZrTazsc0VZmYHhz3YjWb2spkNyfBnuUfSmY3yKEl3N/G4qZK+aGZHZPHzOVDSY+7+Zvg5rXH3yVk87xMaHx5u9LM4y8xWmNnfzezfzOzA8LPcaGY3NXpupr+HA8zsxfAz/x8ze8DMrm50/wlhr3lj+Jl+sdF9l5vZX8NzXzOzI1vy5wMKhaGIUnpd0nYzm2pmx+0YYI2YpJ9L6i1pX0l9JU1IPOZbkoYpNWBPlPR/JI2X1E2p3+/vJx7/NUn9JR0taZyZHZUsysz6SHpEqT26rpIulfR7M+ue5s/ygqROZravmdVIOkVSU2uUH0q6VtI1aV6r8WueGQZ/XXjdQhqs1M/iFEnXS7pC0lGSviDpO40Gd7N/D2a2q6SHJE1R6mc1TdJJOzZgZgdIulPSBZI+JelWSbPMrJ2ZDZB0saQD3b2jpGMkvV3gPyOQE4YiSsbdN0k6XJJLuk1Sg5nNMrOe4f433H22u29x9wZJ10lK7mHd6O5r3f2vSh1GnOfuL7r7FqWa9f6Jx//U3T9w90WS7lKjQ56NnCHpUXd/1N0/dvfZkuolHZ/hj7Rjb3GYpFcl/bWZx90qaW8zOy7di7n7vZIuUWpYPC1pnZmNy1BDLn7m7v9w98clfSBpmruva/Sz3D/Uke7v4WBJO0u6wd0/cvcHJf2p0TbOk3Sru88LRwOmKnV4/GBJ2yW1kzTIzHZx97d37BUDpcJQREm5+1J3H+3ue0n6F6X2Rq6XJDPrYWb3h8Nrm5Ta8+qWeIm1jb7f3ETukHj8ikbfLw/bS+on6eRwuG+jmW1Uanj3yvDHuUfSdyWNVtOHTiVJYWD/LPxn6V7Q3e9z96MkdZb0b5KuMrNjMtSRrax+dhn+HnpL+qvHnyzQ+GfcT9LYxM+yr6Te7v6GpB8otde5Lmyjqb8PoGgYiigb7v6qUofh/iXc9HOl9iK/6O6dlNqDSztEstC30fd7S1rVxGNWSLrH3Ts3+q+9u0/MUP9ypd5wc7ykBzPUcZekPdXoUGOG1/7I3f9H0iv6359PsaT7e1gtqY+ZNf57afwzXiHpmsTPcg93nyZJ7v5bdz9cqeHpkn7R2n8YIB2GIkrGzAaa2Vgz2yvkvkodznwhPKSjpPclbQzrfD8swGZ/YmZ7mNkXJJ0l6YEmHnOvpBPN7BgzqzGz3cxsyI46MzhH0lB3/yDdg9x9m1J7SJc39xgzG21mXzezjma2Uzjc+gVJ87Koo5DS/T08r9Rh0IvNbGczGy7poEb33ybp38xssKW0b/RnGmBmQ82snaR/KLV3ur04fySgaQxFlNJ7Sr3ZY56ZfaDUMFwsace7Qn8q6QBJ7yr1xpdMe1/ZeFrSG0qd2vCrsJ4WcfcVkoYr9YadBqX2dn6oLP69uPub7l6fZS3TlNrTas6mUMM7kjZKmiTpe+7+bJavXyjN/j24+1ZJ31TqfwY2KrUX+bDCaTXhZ3GepJsk/V2pn/3o8PR2kiZKWi9pjaQeSv15gZIxPmQY1cBSJ9C/JWmXsJeGVmJm8yT9xt3vKnUtQK7YUwSQFzM7wsw+HQ6fjpL0RUl/LHVdQEvsXOoCAFS8AZKmK/Vu1Tclfdvd0x0WBsoWh08BAAg4fAoAQMBQBAAgYCgCABAwFAEACBiKAAAEDEUAAAKGIgAAAUMRAICAoQgAQMBQBAAgYCgCABAwFAEACBiKAAAEDEUAAAKGIgAAAUMRAICAoQgAQMBQBAAgYCgCABAwFAEACBiKAAAEDEUAAAKGIgAAAUMRAICAoQgAQMBQBAAgYCgCABAwFAEACBiKAAAEDEUAAAKGIgAAAUMRAICAoQgAQMBQBAAgYCgCABDkNRTN7Fgze83M3jCzcYUqCgCKgR6GJHP3lj3RrEbS65KGSVopab6k09z9z809p1u3bl5bW9ui7aH0FixYsN7du5e6DqAQcu1h9K/Klm3/2jmPbRwk6Q13/4skmdn9koZLanYo1tbWqr6+Po9NopTMbHmpawAKKKceRv+qbNn2r3wOn/aRtKJRXhluSxZyvpnVm1l9Q0NDHpsDgILK2MPoX9Unn6FoTdz2iWOx7j7Z3evcva57d468ASgbGXsY/av65DMUV0rq2yjvJWlVfuUAQNHQw/AJ+QzF+ZL6m9lnzGxXSadKmlWYsgCg1dHD8AktfqONu28zs4slPSapRtKd7r6kYJUBQCuih6Ep+bz7VO7+qKRHC1QLABQVPQxJXNEGAICAoQgAQMBQBAAgYCgCABAwFAEACBiKAAAEDEUAAAKGIgAAQV4n7wMA2qYFCxZE+aabbory1KlTozxq1KgoX3LJJVE+4IADClhd62FPEQCAgKEIAEDAUAQAIGBNMQfbt2+P8rvvvpvT85PH5D/88MMov/baa1G++eabo3zppZdGedq0aVHebbfdojxu3LgoX3nlldkXC6CqvPTSS1E+6qijorxp06Yom8Wf0Xz33XdHeebMmVHesGFDviUWBXuKAAAEDEUAAAKGIgAAQVWtKb7zzjtR3rp1a5Sfe+65KD/77LNR3rhxY5R/97vfFbA6qW/fvlFOnufz0EMPRbljx45R/td//dcoH3HEEQWsDkBb8qc//SnK3/rWt6KcfM9Ecg2xU6dOUd51112jvH79+ig///zzUf7yl7+c9vmlwp4iAAABQxEAgIChCABA0KbXFF988cUoDx06NMq5nmdYaDU1NVG++uqro9y+ffson3766VHu3bt3lLt06RLlAQMG5FsigAqVPA964cKFUT7jjDOivGrVqpxev3///lG+7LLLonzKKadE+bDDDotyst+NHz8+p+23FvYUAQAIGIoAAAQMRQAAgja9ptivX78od+vWLcqFXlMcPHhwlJNrfE8++WSUk+fljBw5sqD1AKheF1xwQZR/+9vfFvT1k5+3+P7770c5eZ70U089FeVFixYVtJ5CYU8RAICAoQgAQMBQBAAgaNNril27do3yL3/5yyj/4Q9/iPL+++8f5e9///tpX/9LX/pSlJ944okoJ88zXLx4cZRvuOGGtK8PANlKrvE9/PDDUXb3tM8fMmRIlE844YQoJz/PNXmedLJ/ZnpPRaZ6SoU9RQAAAoYiAABBxqFoZnea2TozW9zotq5mNtvMloWvXdK9BgCUCj0MubBMx3XN7KuS3pd0t7v/S7htkqQN7j7RzMZJ6uLul2faWF1dndfX1xeg7MLYtGlTlJOfT5g8z+f222+P8r333hvl7373uwWsrvyY2QJ3ryt1HUAuCtXDyq1/vfTSS1H+2te+FuVkf0s6/vjjozxt2rQoZzqv8Nxzz41y9+7d025vp53ifbDkey6efvrpKB9wwAFpXy9X2favjHuK7j5X0obEzcMlTQ3fT5U0IucKAaAI6GHIRUvXFHu6+2pJCl97NPdAMzvfzOrNrL6hoaGFmwOAgsqqh9G/qk+rv9HG3Se7e52712XavQaAckL/qj4tPU9xrZn1cvfVZtZL0rpCFlUsnTp1Snv/nnvumfb+5BrjqaeeGuXkMXQAZaPietjrr78e5UmTJkU5eS3n5BDv1atXlEeNGhXlDh06RDl5nmIy5yv5eY+/+tWvolzoa7Vmq6Vde5akHT/RUZJmFqYcACgKehialM0pGdMkPS9pgJmtNLNzJE2UNMzMlkkaFjIAlB16GHKR8fCpu5/WzF1HFrgWACg4ehhy0aavfZqvCRMmRDl5bcHkeTzJa58effTRrVEWgCqwZcuWKCevPfrII49EOfkeibvvvjvKdXXxKXqbN2/Ot8SCWrFiRalLkMRl3gAA+CeGIgAAAUMRAICANcU0ktfmu+2226KcvDbfeeedF+XktQiTx/QvuuiiKJtZi+oE0PYsXLgwysk1xKSZM+OzSo444oiC11QN2FMEACBgKAIAEDAUAQAIWFPMwec+97koT5kyJcpnnXVWlJPnCSXzBx98EOUzzzwzyslrFQKoHv/+7/8e5eRn3w4ZMiTK5b6GmOmzezPdXyzsKQIAEDAUAQAIGIoAAASsKebhpJNOivI+++wT5bFjx0Y5eW3UH/3oR1Fevnx5lK+44ooo9+nTp0V1Aih/Dz/8cJRfeumlKCfPY/7GN77R6jUVUrL+ZP7Sl75UzHKaxZ4iAAABQxEAgIChCABAwJpiAe23335Rnj59epT/8Ic/RHn06NFR/s1vfhPlZcuWRXn27Nl5VgigXCU/33Dr1q1R7tGjR5RPOeWUVq8pF8nPf0x+Hm3SkUfGn/E8ceLEQpfUIuwpAgAQMBQBAAgYigAABKwptqLOnTtHeeTIkVE+99xzo/zRRx9Fee7cuVF+6qmnopy89iGAtmu33XaLcqmvjZxcQ7z66qujPGnSpCj37ds3ysnzuDt06FDA6lqOPUUAAAKGIgAAAUMRAICANcUCeuWVV6L8u9/9Lsrz58+PcnINMWnQoEFR/upXv5pHdQAqWamvdZq8FmtyzfCBBx6I8vDhw6P84IMPtk5hBcaeIgAAAUMRAICAoQgAQMCaYg5ee+21KN94441RTh4zX7NmTU6vv/PO8V9H8jyknXbi/2GAtsrd0+YZM2ZE+b/+679atZ7rrrsuyj/72c+i/O6770b5jDPOiPLdd9/dOoW1MrosAAABQxEAgCDjUDSzvmb2pJktNbMlZjYm3N7VzGab2bLwtUvrlwsA2aN/IVfZrClukzTW3ReaWUdJC8xstqTRkua4+0QzGydpnKTLW6/U1pdcA/ztb38b5ZtuuinKb7/9dl7bO/DAA6N8xRVXRLnU5yUBbUDF9C8zS5uT/en73/9+lM8+++wof+pTn4ryCy+8EOV77rknyi+//HKUV6xYEeV+/fpF+dhjj43yhRdeqLYg456iu69294Xh+/ckLZXUR9JwSVPDw6ZKGtFaRQJAS9C/kKuc1hTNrFbS/pLmSerp7qul1C+epB7NPOd8M6s3s/qGhob8qgWAFqJ/IRtZD0Uz6yDp95J+4O6bsn2eu0929zp3r+vevXtLagSAvNC/kK2szlM0s12U+oW6z913nIy31sx6uftqM+slaV1rFVkoa9eujfKSJUuifPHFF0f51VdfzWt7gwcPjvJll10W5eS1ATkPESi8ttK/tm3bFuWbb745yslrLe+5555Rfv3113Pa3qGHHhrloUOHRvmqq67K6fUqRTbvPjVJd0ha6u6Nz+acJWlU+H6UpJmFLw8AWo7+hVxls6d4mKSRkhaZ2Y7LpI+XNFHSdDM7R9I7kk5unRIBoMXoX8hJxqHo7s9KsmbuPrKw5QBA4dC/kKs2de3TDRs2RPmCCy6IcvLzwN588828tnfYYYdFeezYsVE+5phjorz77rvntT0AbdchhxwS5YMOOijKf/rTn9I+P3keY/I9FEndunWL8qmnnhrl1r62arninR0AAAQMRQAAAoYiAABBRa0pzps3L8qTJk2K8vz586O8cuXKvLa3xx57RDl5rcHktUrbt2+f1/YAVK+99torysnPZ7311lujnPx8w0zGjBkT5e9973tR7t+/f06v11axpwgAQMBQBAAgYCgCABBU1JriQw89lDZnMmjQoCifeOKJUa6pqYnypZdeGuXOnTvntD0AaKlevXpFecKECWkzCoM9RQAAAoYiAAABQxEAgKCi1hQnTpyYNgMAkA/2FAEACBiKAAAEDEUAAAKGIgAAAUMRAICAoQgAQMBQBAAgYCgCABAwFAEACBiKAAAEDEUAAAJz9+JtzKxB0nJJ3SStL9qGc0d9Tevn7t1LsF2g5OhfBVPW/auoQ/GfGzWrd/e6om84S9QHoDnl/u+P+vLD4VMAAAKGIgAAQamG4uQSbTdb1AegOeX+74/68lCSNUUAAMoRh08BAAgYigAABEUdimZ2rJm9ZmZvmNm4Ym67OWZ2p5mtM7PFjW7ramazzWxZ+NqlRLX1NbMnzWypmS0xszHlVB9Qbcqth5Vz/wq1VFwPK9pQNLMaSTdLOk7SIEmnmdmgYm0/jSmSjk3cNk7SHHfvL2lOyKWwTdJYd99X0sGSLgo/s3KpD6gaZdrDpqh8+5dUgT2smHuKB0l6w93/4u5bJd0vaXgRt98kd58raUPi5uGSpobvp0oaUdSiAndf7e4Lw/fvSVoqqU+51AdUmbLrYeXcv6TK7GHFHIp9JK1olFeG28pRT3dfLaX+UiX1KHE9MrNaSftLmqcyrA+oApXSw8qyP1RKDyvmULQmbuN8kCyYWQdJv5f0A3ffVOp6gCpFD2uhSuphxRyKKyX1bZT3krSqiNvPxVoz6yVJ4eu6UhViZrso9ct0n7s/WG71AVWkUnpYWfWHSuthxRyK8yX1N7PPmNmukk6VNKuI28/FLEmjwvejJM0sRRFmZpLukLTU3a9rdFdZ1AdUmUrpYWXTHyqxhxX7o6OOl3S9pBpJd7r7NUXbeDPMbJqkIUp9nMlaSVdKmiFpuqS9Jb0j6WR3Ty5mF6O2wyU9I2mRpI/DzeOVOiZf8vqAalNuPayc+1eor+J6GJd5AwAg4Io2AAAEDEUAAAKGIgAAAUMRAIAgr6FYbhfHBYBc0MOQ1OJ3n4aL474uaZhSJ7XOl3Sau/+5cOUBQOugh6EpO+fx3H9eHFeSzGzHxXGb/YXq1q2b19bW5rFJlNKCBQvWu3v3UtcBFEhOPYz+Vdmy7V/5DMWmLo47ON0TamtrVV9fn8cmUUpmtrzUNQAFlFMPo39Vtmz7Vz5rilldHNfMzjezejOrb2hoyGNzAFBQGXsY/av65DMUs7o4rrtPdvc6d6/r3p0jbwDKRsYeRv+qPvkMxUq5OC4ANIUehk9o8Zqiu28zs4slPab/vTjukoJVBgCtiB6GpuTzRhu5+6OSHi1QLQBQVPQwJHFFGwAAAoYiAAABQxEAgIChCABAwFAEACBgKAIAEDAUAQAIGIoAAAQMRQAAAoYiAAABQxEAgIChCABAkNcFwVFcc+bMifLpp58e5aeffjrKAwYMaPWaAECSrr766ij/x3/8R5Td48+gf+qpp6J8xBFHtEpduWJPEQCAgKEIAEDAUAQAIKioNcW5c+dG+W9/+1uUTzrppGKWU3Tz58+Pcl1dXYkqAVDtpkyZEuWJEydGuaamJsrbt2+Pspm1Sl35Yk8RAICAoQgAQMBQBAAgqKg1xeR5LcuWLYtyW1tT/Pjjj6P81ltvRfmdd96JcvI8IABoLcuXL4/yli1bSlRJYbGnCABAwFAEACBgKAIAEFTUmuLUqVOjfOihh5aokuJYvXp1lCdPnhzlkSNHRnngwIGtXhOA6vTEE09E+YYbbkj7+GQ/evjhh6Pcs2fPwhRWYOwpAgAQMBQBAAgYigAABBW1ppg8b6+tO/fcc9Pe379//yJVAqDaPPvss1EePXp0lDdt2pT2+T/84Q+j3K9fv4LU1drYUwQAIGAoAgAQZByKZnanma0zs8WNbutqZrPNbFn42qV1ywSAlqGHIRfZrClOkXSTpLsb3TZO0hx3n2hm40K+vNDFvfLKK1Feu3ZtoTdR1jZu3Jj2/mHDhhWpEqCiTVGJelglS54XvmrVqrSPHzJkSJTPPPPMQpdUFBn3FN19rqQNiZuHS9rxE5sqaUSB6wKAgqCHIRctXVPs6e6rJSl87VG4kgCg1dHD0KRWf6ONmZ1vZvVmVt/Q0NDamwOAgqF/VZ+Wnqe41sx6uftqM+slaV1zD3T3yZImS1JdXV1OH/j36KOPRnnz5s0tKLVyJNdM33777bSP79OnTytWA7RpWfWwfPpXpVm/fn2U77jjjijX1NREuXPnzlH+8Y9/3DqFFVlL9xRnSRoVvh8laWZhygGAoqCHoUnZnJIxTdLzkgaY2UozO0fSREnDzGyZpGEhA0DZoYchFxkPn7r7ac3cdWSBawGAgqOHIRdlfe3T1157Le39X/jCF4pUSXFceumlUV6zZk2UBwwYEOWOHTu2ek0A2qbkexa++c1v5vT8Sy65JMpDhw7Nt6SywGXeAAAIGIoAAAQMRQAAgrJeU8zkwAMPLHUJaSU/b+yPf/xjlO+9994oP/7442lfL3keUPI8IQDIVrIfLVq0KO3jjzwyfl/SmDFjCl5TOWBPEQCAgKEIAEDAUAQAIKjoNcUNG5KfBpObl19+Ocoff/xxlOfMmRPllStXRnnr1q1Rvu+++9K+3u677x7lwYMHR7ldu3ZR/uijj6JcV1cnAGiJGTNmRHncuHFpH/+Vr3wlysnPV9xzzz0LU1iZYU8RAICAoQgAQMBQBAAgKOs1xeQanJlF+YILLojytddem9PrJ9cU3eOPS9tll12ivMcee0R53333jfLZZ58d5S9/+ctRHjJkSJR79uwZ5b322ivKyc+PHDhwoAAgG/le2/Szn/1slJP9qq1iTxEAgIChCABAwFAEACAo6zXFW265Jcr9+vWL8nPPPZfX6++9995RHj58eJQHDRoU5YMPPjiv7SVNnjw5yuvWrYty8pg+AGTrF7/4RZRrampyen6m8xjbKvYUAQAIGIoAAAQMRQAAgrJeU0y6/PLLS11CQSWvrZr07W9/u0iVAKh0L730UpQfe+yxnJ7/jW98I8oDBgzIu6ZKxJ4iAAABQxEAgIChCABAUFFritVmxIgRpS4BQIU4+uijo/z3v/897eOTn+ea/LzEasWeIgAAAUMRAICAoQgAQMCaIgC0AevXr49ypmudXnTRRVHu0KFDwWuqROwpAgAQMBQBAAgyDkUz62tmT5rZUjNbYmZjwu1dzWy2mS0LX7u0frkAkD36F3KVzZriNklj3X2hmXWUtMDMZksaLWmOu080s3GSxklqWxcnLbFly5ZF+ZBDDilRJUDFarP966yzzoqyu0d5+/btaZ9/6KGHFrymtiDjnqK7r3b3heH79yQtldRH0nBJO872nCqJM80BlBX6F3KV05qimdVK2l/SPEk93X21lPrFk9Sj0MUBQKHQv5CNrIeimXWQ9HtJP3D3TTk873wzqzez+oaGhpbUCAB5oX8hW1mdp2hmuyj1C3Wfuz8Ybl5rZr3cfbWZ9ZK0rqnnuvtkSZMlqa6uzpt6DJr28ccfl7oEoOK1lf6V/LzE2bNnR9nMotyuXbsoX3jhhVHu2bNnAatrO7J596lJukPSUne/rtFdsySNCt+PkjSz8OUBQMvRv5CrbPYUD5M0UtIiM9vxvyrjJU2UNN3MzpH0jqSTW6dEAGgx+hdyknEouvuzkqyZu48sbDkAUDj0L+SKa5+Wseeffz7Ko0ePLk0hAEpu48aNUV67dm3ax/fu3TvKv/71rwteU1vEZd4AAAgYigAABAxFAAAChiIAAAFDEQCAgKEIAEDAUAQAIOA8xRI67rjjojx9+vQSVQKg3A0cODDKyc9DfOaZZ4pZTpvFniIAAAFDEQCAgKEIAEDAmmIJJa9lyrVNATTn05/+dJSffvrpElXStrGnCABAwFAEACBgKAIAEDAUAQAIGIoAAAQMRQAAAoYiAAABQxEAgIChCABAwFAEACBgKAIAEJi7F29jZg2SlkvqJml90TacO+prWj93716C7QIlR/8qmLLuX0Udiv/cqFm9u9cVfcNZoj4AzSn3f3/Ulx8OnwIAEDAUAQAISjUUJ5dou9miPgDNKfd/f9SXh5KsKQIAUI44fAoAQMBQBAAgKOpQNLNjzew1M3vDzMYVc9vNMbM7zWydmS1udFtXM5ttZsvC1y4lqq2vmT1pZkvNbImZjSmn+oBqU249rJz7V6il4npY0YaimdVIulnScZIGSTrNzAYVa/tpTJF0bOK2cZLmuHt/SXNCLoVtksa6+76SDpZ0UfiZlUt9QNUo0x42ReXbv6QK7GHF3FM8SNIb7v4Xd98q6X5Jw4u4/Sa5+1xJGxI3D5c0NXw/VdKIohYVuPtqd18Yvn9P0lJJfcqlPqDKlF0PK+f+JVVmDyvmUOwjaUWjvDLcVo56uvtqKfWXKqlHieuRmdVK2l/SPJVhfUAVqJQeVpb9oVJ6WDGHojVxG+eDZMHMOkj6vaQfuPumUtcDVCl6WAtVUg8r5lBcKalvo7yXpFVF3H4u1ppZL0kKX9eVqhAz20WpX6b73P3BcqsPqCKV0sPKqj9UWg8r5lCcL6m/mX3GzHaVdKqkWUXcfi5mSRoVvh8laWYpijAzk3SHpKXufl2ju8qiPqDKVEoPK5v+UIk9rNgfHXW8pOsl1Ui6092vKdrGm2Fm0yQNUerjTNZKulLSDEnTJe0t6R1JJ7t7cjG7GLUdLukZSYskfRxuHq/UMfmS1wdUm3LrYeXcv0J9FdfDuMwbAAABV7QBACBgKAIAEDAUAQAIGIoAAAR5DcVyuzguAOSCHoakFr/7NFwc93VJw5Q6qXW+pNPc/c+FKw8AWgc9DE3ZOY/n/vPiuJJkZjsujtvsL1Q+cLmjAAANHklEQVS3bt28trY2j02ilBYsWLDe3buXug6gQHLqYfSvypZt/8pnKDZ1cdzB6Z5QW1ur+vr6PDaJUjKz5aWuASignHoY/auyZdu/8llTzOriuGZ2vpnVm1l9Q0NDHpsDgILK2MPoX9Unn6GY1cVx3X2yu9e5e1337hx5A1A2MvYw+lf1yWcoVsrFcQGgKfQwfEKL1xTdfZuZXSzpMf3vxXGXFKwyAGhF9DA0JZ832sjdH5X0aIFqAYCioochiSvaAAAQMBQBAAgYigAABAxFAAAChiIAAAFDEQCAgKEIAEDAUAQAIGAoAgAQMBQBAAgYigAABAxFAAAChiIAAEFen5IBAGib3nvvvSi///77UX7kkUeivG7duiiPHTs2yu3atStgda2HPUUAAAKGIgAAAUMRAICANUUAqEJvvfVWlCdNmhTl559/PsqLFi3K6fXXrFkT5RtuuCGn55cKe4oAAAQMRQAAAoYiAAABa4o5mDdvXpTvueeeKM+dOzfKixcvTvt6v/71r6Pcu3fvKD/zzDNRHjlyZJQHDx6c9vUBVK9XX301ytdff32U77333ihv3rw5yu4e5b333jvKHTt2jPKf//znKE+fPj3KF154YZQHDhzYVNklx54iAAABQxEAgIChCABAwJpiGg888ECUx4wZE+WGhoYoJ4/BDxkyJMrr16+P8qWXXpp2+8nXSz7//vvvT/t8AG3Xu+++G+XLL788ysn+tWnTppxe//Of/3yUH3vssShv3bo1ysk1wmR/TPavcsWeIgAAAUMRAICAoQgAQFDVa4rbtm2L8vz586N83nnnRfmDDz6I8hFHHBHln/zkJ1E+/PDDo7xly5Yof+c734ly8ph9Ul1dXdr7AVSPhx56KMq33XZbXq+3zz77RHn27NlR7tu3b5SXLVuW1/bKFXuKAAAEGYeimd1pZuvMbHGj27qa2WwzWxa+dmndMgGgZehhyEU2e4pTJB2buG2cpDnu3l/SnJABoBxNET0MWcq4pujuc82sNnHzcElDwvdTJT0l6XJVmOS1/84555y0jz/66KOjnDwPqFOnTmmfn3x8pjXE5DH8UaNGpX08gE9qqz0seW3RTGpra6N80EEHRfkXv/hFlJP9Jyl5bdW2oqVrij3dfbUkha89ClcSALQ6ehia1OpvtDGz882s3szqk1c4AIByRv+qPi0dimvNrJckha/rmnugu0929zp3r+vevXsLNwcABZVVD6N/VZ+Wnqc4S9IoSRPD15kFq6gV/fjHP47ytddeG2Uzi/JFF10U5auvvjrKmdYQk6655pqcHn/DDTdEmX+UQMFUZA9r7Pbbb4/y5MmTo5x8D0TyPMQePfI7Yrx27dq8nl+usjklY5qk5yUNMLOVZnaOUr9Iw8xsmaRhIQNA2aGHIRfZvPv0tGbuOrLAtQBAwdHDkAuuaAMAQNCmr3161VVXRTm5htiuXbsoH3PMMVFOnrez++67p93eP/7xjyg//vjjUV6+fHmUk5+XmLx26vDhw9NuD0D16t27d5QnTJhQ1O0/99xzRd1esbCnCABAwFAEACBgKAIAELSpNcWNGzdG+ZZbboly8jzE5BrijBkzctreG2+8EeXTTz89yvX19Wmff/LJJ0f5sssuy2n7ANBSyfOgk58Xm3zPQ7J/Ll68WOkcdthhUT7kkENyLbEk2FMEACBgKAIAEDAUAQAI2tSa4tatW6Oc6ar2yWPq69bF1wS+6667ojxzZnx5xCVLlkT5vffei3LyGPxOO8X/D3LGGWdEuX379mnrBYDmfPjhh1FO9qfkeduPPPJI2tfLtKaYlDxvMtk/a2pq0j6/XLCnCABAwFAEACBgKAIAELSpNcVdd901ysnPC0uuGdbW1kY50zHzpD59+kQ5+fmKq1atinK3bt2ifOKJJ+a0PQDV66OPPoryiy++GOVvfetbUU72nz322CPKyTXAQw89NMp//OMfo5w8jzFp+/btUX7wwQejPGbMmCgn+3W5YE8RAICAoQgAQMBQBAAgaFNrip07d45y8lqmJ5xwQpT/9re/RXmfffaJcvLzDEePHh3lrl27RvnUU0+NcvKYfvJ+AGhO8rzr5BrfSSedlPb5yc9X/NrXvhblww8/PMobNmyI8tChQ6O8aNGitNtLvmdj3LhxUd57772jPGLEiCgnP9+2VNhTBAAgYCgCABAwFAEACNrUmmLS4MGDo5zpWqi5mjt3bpSffvrpKCfPe/zsZz9b0O0DaDuS5yFeeeWVUZ40aVLa5x933HFRvuSSS6KcfM9Fsh8ef/zxUX7llVeinFzzS37+a3LNMXmt6O9+97tRHjZsWNrX69Kli9LZf//9097fUuwpAgAQMBQBAAgYigAABG16TbG1bd68OcrJNcRk5jxFADskrxX6k5/8JMq//OUvo9yhQ4co//znP4/yaaedFuXkGuL8+fOjnFxzXLhwYZQ///nPR/m///u/o5w873HTpk1Rfu6556J83333RXnWrFlRTq4xJiXPc3zrrbfSPr6l2FMEACBgKAIAEDAUAQAIWFPMwzHHHFPqEgBUqMmTJ0c5uYbYvn37KN96661RPvroo6P8wgsvRPmuu+6K8qOPPhrl5HsikudFnnXWWVHu27ev0kl+nuyxxx6bNk+bNi3KyTXHpP/8z/9Me3+hsKcIAECQcSiaWV8ze9LMlprZEjMbE27vamazzWxZ+Jr+8gMAUGT0L+Qqmz3FbZLGuvu+kg6WdJGZDZI0TtIcd+8vaU7IAFBO6F/Iibl7bk8wmynppvDfEHdfbWa9JD3l7gPSPbeurs7r6+tbXGy5eeyxx6KcvPZg8jzFNWvWRLl79+6tU1grMbMF7l5X6jqAliqn/tWrV68oJz+PMHmt0YEDB0b5ww8/jPKyZcty2v5Pf/rTKP/oRz+Kck1NTU6vV+6y7V85rSmaWa2k/SXNk9TT3VdLUvjaI/cyAaA46F/IRtZD0cw6SPq9pB+4+6ZMj2/0vPPNrN7M6gv9KRUAkA36F7KV1VA0s12U+oW6z90fDDevDYcdFL6ua+q57j7Z3evcva7SDhcCqHz0L+Qi43mKlloYu0PSUne/rtFdsySNkjQxfJ3ZxNPbtDfffLPUJQBIo5z716c//ekoJ9cUt2zZEuWXX3457et9/etfj/JXv/rVKI8YMSLKtbW1UW5ra4gtlc3J+4dJGilpkZm9FG4br9Qv03QzO0fSO5JObp0SAaDF6F/IScah6O7PSrJm7j6ysOUAQOHQv5ArrmgDAEDAtU/z8JWvfCXKuZ7zCaB6zZ07N8ozZsyIcvLzDXv0iM8aOfvss6PcpUt8UZ5dd9013xKrEnuKAAAEDEUAAAKGIgAAAWuKedhvv/2i3L9//ygnz2NMZk4GBqpXx44dozxy5Mi0GcXBniIAAAFDEQCAgKEIAEDAmmIBjR8/PsrnnHNO2vtvuummKA8aNKh1CgMAZIU9RQAAAoYiAAABQxEAgIA1xQL65je/GeX7778/yrNnz47yhAkTonzXXXdFuX379oUrDgCQEXuKAAAEDEUAAAKGIgAAAWuKBdSpU6coT58+PcpXXHFFlG+55ZYoJ9cYOW8RAIqLPUUAAAKGIgAAAUMRAICANcVWlFxjvPHGG9NmAEBpsacIAEDAUAQAIGAoAgAQmLsXb2NmDZKWS+omaX3RNpw76mtaP3fvXoLtAiVH/yqYsu5fRR2K/9yoWb271xV9w1miPgDNKfd/f9SXHw6fAgAQMBQBAAhKNRQnl2i72aI+AM0p939/1JeHkqwpAgBQjjh8CgBAUNShaGbHmtlrZvaGmY0r5rabY2Z3mtk6M1vc6LauZjbbzJaFr11KVFtfM3vSzJaa2RIzG1NO9QHVptx6WDn3r1BLxfWwog1FM6uRdLOk4yQNknSamZXDBwZOkXRs4rZxkua4e39Jc0IuhW2Sxrr7vpIOlnRR+JmVS31A1SjTHjZF5du/pArsYcXcUzxI0hvu/hd33yrpfknDi7j9Jrn7XEkbEjcPlzQ1fD9V0oiiFhW4+2p3Xxi+f0/SUkl9yqU+oMqUXQ8r5/4lVWYPK+ZQ7CNpRaO8MtxWjnq6+2op9ZcqqUeJ65GZ1UraX9I8lWF9QBWolB5Wlv2hUnpYMYeiNXEbb33Ngpl1kPR7ST9w902lrgeoUvSwFqqkHlbMobhSUt9GeS9Jq4q4/VysNbNekhS+ritVIWa2i1K/TPe5+4PlVh9QRSqlh5VVf6i0HlbMoThfUn8z+4yZ7SrpVEmzirj9XMySNCp8P0rSzFIUYWYm6Q5JS939ukZ3lUV9QJWplB5WNv2hEntYsT8l43hJ10uqkXSnu19TtI03w8ymSRqi1JXb10q6UtIMSdMl7S3pHUknu3tyMbsYtR0u6RlJiyR9HG4er9Qx+ZLXB1Sbcuth5dy/Qn0V18O4og0AAAFXtAEAIGAoAgAQMBQBAAgYigAABAxFAAAChiIAAAFDEQCAgKEIAEDw/wHydvV+otDqYAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 720x504 with 6 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "np.random.seed(1643)\n",
    "img_ixs = np.random.randint(0, X_train.shape[0], 6)\n",
    "fig = plt.figure(figsize=(10, 7))\n",
    "fig.suptitle(\"Sample MNIST images\")\n",
    "for ix, img_ixs in enumerate(img_ixs):\n",
    "    ax = fig.add_subplot(3, 2, ix + 1)\n",
    "    ax.imshow(X_train[ix].reshape(28, 28), cmap=\"gray_r\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a Fully Connected Deep Neural Network\n",
    "### FFNN V.01\n",
    "Feed forward neural network with sigmoid activation function and Gradient Descent Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "logs = \"./tf_logs/ffnn_v01\"\n",
    "\n",
    "n_inputs = 28 * 28\n",
    "n_hidden = 100\n",
    "n_outputs = 10\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int32, shape=None, name=\"y\")\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = tf.layers.dense(X, n_hidden, activation=tf.nn.sigmoid, name=\"hidden1\")\n",
    "    hidden2 = tf.layers.dense(hidden1, n_hidden, activation=tf.nn.sigmoid, name=\"hidden2\")\n",
    "    hidden3 = tf.layers.dense(hidden2, n_hidden, activation=tf.nn.sigmoid, name=\"hidden3\")\n",
    "    hidden4 = tf.layers.dense(hidden3, n_hidden, activation=tf.nn.sigmoid, name=\"hidden4\")\n",
    "    hidden5 = tf.layers.dense(hidden4, n_hidden, activation=tf.nn.sigmoid, name=\"hidden5\")\n",
    "    outputs = tf.layers.dense(hidden5, n_outputs, name=\"outputs\")\n",
    "\n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "        labels=y,\n",
    "        logits=outputs\n",
    "    )\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "    \n",
    "\n",
    "alpha = 0.001\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(alpha)\n",
    "    train_step = optimizer.minimize(loss)\n",
    "\n",
    "with tf.name_scope(\"metrics\"):\n",
    "    correct = tf.nn.in_top_k(outputs, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name=\"accuracy\")\n",
    "    tb_train_writer = tf.summary.FileWriter(logs + \"/train\", tf.get_default_graph())\n",
    "    tb_cv_writer = tf.summary.FileWriter(logs + \"/cv\", tf.get_default_graph())\n",
    "    tb_accuracy = tf.summary.scalar(\"Accuracy\", accuracy)\n",
    "    \n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@Epoch  00: Train Accuracy 009.87% | Test Accuracy 009.80%\n",
      "@Epoch  40: Train Accuracy 009.87% | Test Accuracy 009.80%\n",
      "@Epoch  80: Train Accuracy 009.87% | Test Accuracy 009.80%\n",
      "@Epoch  120: Train Accuracy 009.87% | Test Accuracy 009.80%\n",
      "@Epoch  160: Train Accuracy 009.87% | Test Accuracy 009.80%\n",
      "@Epoch  200: Train Accuracy 009.87% | Test Accuracy 009.80%\n",
      "@Epoch  240: Train Accuracy 009.87% | Test Accuracy 009.80%\n",
      "@Epoch  280: Train Accuracy 011.24% | Test Accuracy 011.35%\n",
      "@Epoch  320: Train Accuracy 011.24% | Test Accuracy 011.35%\n",
      "@Epoch  360: Train Accuracy 011.24% | Test Accuracy 011.35%\n",
      "CPU times: user 24min 36s, sys: 4min 15s, total: 28min 51s%\n",
      "Wall time: 6min 50s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "epochs = 400\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for epoch in range(epochs):\n",
    "        sess.run(train_step, feed_dict={X: X_train, y: y_train})\n",
    "        acc, tb_acc = sess.run([accuracy, tb_accuracy], feed_dict={X: X_train, y: y_train})\n",
    "        cvacc, tb_cvacc = sess.run([accuracy, tb_accuracy], feed_dict={X: X_test, y: y_test})\n",
    "        tb_train_writer.add_summary(tb_acc, epoch)\n",
    "        tb_cv_writer.add_summary(tb_cvacc, epoch)\n",
    "        \n",
    "        end = \"\\n\" if epoch % 40 == 0 else \"\\r\"\n",
    "        print(f\"@Epoch {epoch: 03}: Train Accuracy {acc:07.2%} | Test Accuracy {cvacc:07.2%}\",\n",
    "              end=end)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "### FFNN V.02\n",
    "Feed forward neural network with elu activation function and Gradient Descent Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "logs = \"./tf_logs/ffnn_v02\"\n",
    "\n",
    "n_inputs = 28 * 28\n",
    "n_hidden = 100\n",
    "n_outputs = 10\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int32, shape=None, name=\"y\")\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = tf.layers.dense(X, n_hidden, activation=tf.nn.elu, name=\"hidden1\")\n",
    "    hidden2 = tf.layers.dense(hidden1, n_hidden, activation=tf.nn.elu, name=\"hidden2\")\n",
    "    hidden3 = tf.layers.dense(hidden2, n_hidden, activation=tf.nn.elu, name=\"hidden3\")\n",
    "    hidden4 = tf.layers.dense(hidden3, n_hidden, activation=tf.nn.elu, name=\"hidden4\")\n",
    "    hidden5 = tf.layers.dense(hidden4, n_hidden, activation=tf.nn.elu, name=\"hidden5\")\n",
    "    outputs = tf.layers.dense(hidden5, n_outputs, name=\"outputs\")\n",
    "\n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "        labels=y,\n",
    "        logits=outputs\n",
    "    )\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "    \n",
    "\n",
    "alpha = 0.001\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(alpha)\n",
    "    train_step = optimizer.minimize(loss)\n",
    "\n",
    "with tf.name_scope(\"metrics\"):\n",
    "    correct = tf.nn.in_top_k(outputs, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name=\"accuracy\")\n",
    "    tb_train_writer = tf.summary.FileWriter(logs + \"/train\", tf.get_default_graph())\n",
    "    tb_cv_writer = tf.summary.FileWriter(logs + \"/cv\", tf.get_default_graph())\n",
    "    tb_accuracy = tf.summary.scalar(\"Accuracy\", accuracy)\n",
    "    \n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@Epoch  00: Train Accuracy 006.13% | Test Accuracy 006.11%\n",
      "@Epoch  40: Train Accuracy 009.70% | Test Accuracy 009.56%\n",
      "@Epoch  80: Train Accuracy 014.78% | Test Accuracy 014.49%\n",
      "@Epoch  120: Train Accuracy 021.02% | Test Accuracy 021.13%\n",
      "@Epoch  160: Train Accuracy 027.68% | Test Accuracy 028.32%\n",
      "@Epoch  200: Train Accuracy 033.88% | Test Accuracy 034.71%\n",
      "@Epoch  240: Train Accuracy 039.29% | Test Accuracy 040.41%\n",
      "@Epoch  280: Train Accuracy 043.42% | Test Accuracy 044.78%\n",
      "@Epoch  320: Train Accuracy 047.05% | Test Accuracy 048.85%\n",
      "@Epoch  360: Train Accuracy 050.14% | Test Accuracy 051.77%\n",
      "CPU times: user 26min 10s, sys: 4min 11s, total: 30min 22s%\n",
      "Wall time: 6min 56s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "epochs = 400\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for epoch in range(epochs):\n",
    "        sess.run(train_step, feed_dict={X: X_train, y: y_train})\n",
    "        acc, tb_acc = sess.run([accuracy, tb_accuracy], feed_dict={X: X_train, y: y_train})\n",
    "        cvacc, tb_cvacc = sess.run([accuracy, tb_accuracy], feed_dict={X: X_test, y: y_test})\n",
    "        tb_train_writer.add_summary(tb_acc, epoch)\n",
    "        tb_cv_writer.add_summary(tb_cvacc, epoch)\n",
    "        \n",
    "        end = \"\\n\" if epoch % 40 == 0 else \"\\r\"\n",
    "        print(f\"@Epoch {epoch: 03}: Train Accuracy {acc:07.2%} | Test Accuracy {cvacc:07.2%}\",\n",
    "              end=end)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "### FFNN V.03\n",
    "Feed forward neural network with elu activation function and ADAM Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "logs = \"./tf_logs/ffnn_v03\"\n",
    "\n",
    "n_inputs = 28 * 28\n",
    "n_hidden = 100\n",
    "n_outputs = 10\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int32, shape=None, name=\"y\")\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = tf.layers.dense(X, n_hidden, activation=tf.nn.elu, name=\"hidden1\")\n",
    "    hidden2 = tf.layers.dense(hidden1, n_hidden, activation=tf.nn.elu, name=\"hidden2\")\n",
    "    hidden3 = tf.layers.dense(hidden2, n_hidden, activation=tf.nn.elu, name=\"hidden3\")\n",
    "    hidden4 = tf.layers.dense(hidden3, n_hidden, activation=tf.nn.elu, name=\"hidden4\")\n",
    "    hidden5 = tf.layers.dense(hidden4, n_hidden, activation=tf.nn.elu, name=\"hidden5\")\n",
    "    outputs = tf.layers.dense(hidden5, n_outputs, name=\"outputs\")\n",
    "\n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "        labels=y,\n",
    "        logits=outputs\n",
    "    )\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "    \n",
    "\n",
    "alpha = 0.001\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.AdamOptimizer(alpha)\n",
    "    train_step = optimizer.minimize(loss)\n",
    "\n",
    "with tf.name_scope(\"metrics\"):\n",
    "    correct = tf.nn.in_top_k(outputs, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name=\"accuracy\")\n",
    "    tb_train_writer = tf.summary.FileWriter(logs + \"/train\", tf.get_default_graph())\n",
    "    tb_cv_writer = tf.summary.FileWriter(logs + \"/cv\", tf.get_default_graph())\n",
    "    tb_accuracy = tf.summary.scalar(\"Accuracy\", accuracy)\n",
    "    \n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@Epoch  00: Train Accuracy 025.93% | Test Accuracy 026.65%\n",
      "@Epoch  40: Train Accuracy 091.63% | Test Accuracy 091.74%\n",
      "@Epoch  80: Train Accuracy 094.57% | Test Accuracy 094.37%\n",
      "@Epoch  120: Train Accuracy 096.32% | Test Accuracy 095.71%\n",
      "@Epoch  160: Train Accuracy 097.35% | Test Accuracy 096.45%\n",
      "@Epoch  200: Train Accuracy 098.10% | Test Accuracy 096.92%\n",
      "@Epoch  240: Train Accuracy 098.75% | Test Accuracy 097.19%\n",
      "@Epoch  280: Train Accuracy 098.96% | Test Accuracy 097.31%\n",
      "@Epoch  320: Train Accuracy 099.46% | Test Accuracy 097.39%\n",
      "@Epoch  360: Train Accuracy 099.69% | Test Accuracy 097.42%\n",
      "CPU times: user 26min 11s, sys: 4min 17s, total: 30min 28s%\n",
      "Wall time: 7min 14s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "epochs = 400\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for epoch in range(epochs):\n",
    "        sess.run(train_step, feed_dict={X: X_train, y: y_train})\n",
    "        acc, tb_acc = sess.run([accuracy, tb_accuracy],\n",
    "                               feed_dict={X: X_train, y: y_train})\n",
    "        cvacc, tb_cvacc = sess.run([accuracy, tb_accuracy],\n",
    "                                   feed_dict={X: X_test, y: y_test})\n",
    "        tb_train_writer.add_summary(tb_acc, epoch)\n",
    "        tb_cv_writer.add_summary(tb_cvacc, epoch)\n",
    "        \n",
    "        end = \"\\n\" if epoch % 40 == 0 else \"\\r\"\n",
    "        print(f\"@Epoch {epoch: 03}: Train Accuracy {acc:07.2%} | Test Accuracy {cvacc:07.2%}\",\n",
    "              end=end)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "### FFNN V.04\n",
    "Feed forward neural network with elu activation function, ADAM Optimizer and Batch Norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "logs = \"./tf_logs/ffnn_v04/\"\n",
    "\n",
    "n_inputs = 28 * 28\n",
    "n_hidden = 100\n",
    "n_outputs = 10\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int32, shape=(None), name=\"y\")\n",
    "training = tf.placeholder_with_default(False, shape=(), name=\"training\")\n",
    "\n",
    "eta = 0.9\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    z1 = tf.layers.dense(X, n_hidden, name=\"z1\")\n",
    "    z1_bn = tf.layers.batch_normalization(z1, training=training, momentum=eta)\n",
    "    a1 = tf.nn.elu(z1_bn)\n",
    "    \n",
    "    z2 = tf.layers.dense(a1, n_hidden, name=\"z2\")\n",
    "    z2_bn = tf.layers.batch_normalization(z2, training=training, momentum=eta)\n",
    "    a2 = tf.nn.elu(z2_bn)\n",
    "    \n",
    "    z3 = tf.layers.dense(a2, n_hidden, name=\"z3\")\n",
    "    z3_bn = tf.layers.batch_normalization(z3, training=training, momentum=eta)\n",
    "    a3 = tf.nn.elu(z3_bn)\n",
    "    \n",
    "    z4 = tf.layers.dense(a3, n_hidden, name=\"z4\")\n",
    "    z4_bn = tf.layers.batch_normalization(z4, training=training, momentum=eta)\n",
    "    a4 = tf.nn.elu(z4)\n",
    "    \n",
    "    z5 = tf.layers.dense(a4, n_hidden, name=\"z5\")\n",
    "    z5_bn = tf.layers.batch_normalization(z5, training=training, momentum=eta)\n",
    "    a5 = tf.nn.elu(z5_bn)\n",
    "    \n",
    "    output = tf.layers.dense(a5, n_hidden, name=\"output\")\n",
    "\n",
    "\n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "        labels=y,\n",
    "        logits=output\n",
    "    )\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "\n",
    "    \n",
    "alpha = 0.001\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.AdamOptimizer(alpha)\n",
    "    train_step = optimizer.minimize(loss)\n",
    "\n",
    "    \n",
    "with tf.name_scope(\"metrics\"):\n",
    "    correct = tf.nn.in_top_k(output, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name=\"accuracy\")\n",
    "    tb_train_writer = tf.summary.FileWriter(logs + \"/train\", tf.get_default_graph())\n",
    "    tb_cv_writer = tf.summary.FileWriter(logs + \"/cv\", tf.get_default_graph())\n",
    "    tb_accuracy = tf.summary.scalar(\"Accuracy\", accuracy)\n",
    "\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@Epoch  00: Train Accuracy 005.91% | Test Accuracy 005.43%\n",
      "@Epoch  40: Train Accuracy 091.78% | Test Accuracy 092.06%\n",
      "@Epoch  80: Train Accuracy 095.16% | Test Accuracy 094.87%\n",
      "@Epoch  120: Train Accuracy 096.90% | Test Accuracy 096.05%\n",
      "@Epoch  160: Train Accuracy 097.95% | Test Accuracy 096.75%\n",
      "@Epoch  200: Train Accuracy 098.79% | Test Accuracy 097.07%\n",
      "@Epoch  240: Train Accuracy 099.32% | Test Accuracy 097.29%\n",
      "@Epoch  280: Train Accuracy 099.68% | Test Accuracy 097.37%\n",
      "@Epoch  320: Train Accuracy 099.87% | Test Accuracy 097.45%\n",
      "@Epoch  360: Train Accuracy 099.95% | Test Accuracy 097.43%\n",
      "CPU times: user 47min 35s, sys: 6min 25s, total: 54min7.43%\n",
      "Wall time: 10min 52s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Operations to compute the mean and standard deviation of the minibatch\n",
    "extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "epochs = 400\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for epoch in range(epochs):\n",
    "        sess.run([train_step, extra_update_ops], feed_dict={training: True, X: X_train, y: y_train})\n",
    "        acc, tb_acc = sess.run([accuracy, tb_accuracy],\n",
    "                               feed_dict={X: X_train, y: y_train})\n",
    "        cvacc, tb_cvacc = sess.run([accuracy, tb_accuracy],\n",
    "                                    feed_dict={X: X_test, y: y_test})\n",
    "        \n",
    "        tb_train_writer.add_summary(tb_acc, epoch)\n",
    "        tb_cv_writer.add_summary(tb_cvacc, epoch)\n",
    "        \n",
    "        end = \"\\n\" if epoch % 40 == 0 else \"\\r\"\n",
    "        print(f\"@Epoch {epoch:03}: Train Accuracy {acc:07.2%} | Test Accuracy {cvacc:07.2%}\",\n",
    "              end=end)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feed Forward Neural Network Version Comparison\n",
    "![FFNNs Comparisson](./images/ffnn_vs.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "----\n",
    "# Storing and Reusing TF Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from functools import partial\n",
    "\n",
    "def a_plus_b(a, b): return a + b\n",
    "a_plus_3 = partial(a_plus_b, b=3)\n",
    "a_plus_3(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Building and Storing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "logs = \"./tf_logs/ffnn_v05/\"\n",
    "\n",
    "n_inputs = 28 * 28\n",
    "n_hidden = 100\n",
    "n_output = 10\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int32, shape=None, name=\"y\")\n",
    "\n",
    "training = tf.placeholder_with_default(False, shape=(), name=\"training_bool\")\n",
    "\n",
    "eta = 0.9\n",
    "batch_norm = partial(tf.layers.batch_normalization,\n",
    "                     momentum=eta, training=training)\n",
    "\n",
    "def hidden_layer(inputs, units, hi, activation=tf.nn.elu):\n",
    "    \"\"\"\n",
    "    Create the hidden layer of a feed forward neural\n",
    "    network with batch norm.\n",
    "    \"\"\"\n",
    "    zi = tf.layers.dense(inputs, units, name=f\"z{hi}\")\n",
    "    zi_bn = batch_norm(zi, name=f\"z_bn{hi}\")\n",
    "    ai = activation(zi_bn, name=f\"a{hi}\")\n",
    "    \n",
    "    return ai\n",
    "\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden = hidden_layer(X, n_hidden, 1)\n",
    "    # Creating 5 ELU layers\n",
    "    for h_ix in range(2, 6):\n",
    "        hidden = hidden_layer(hidden, n_hidden, h_ix)\n",
    "    output = tf.layers.dense(hidden, n_output, name=\"output\")\n",
    "    \n",
    "\n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "        labels=y,\n",
    "        logits=output\n",
    "    )\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "\n",
    "    \n",
    "alpha = 0.005\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.AdamOptimizer(alpha)\n",
    "    train_step = optimizer.minimize(loss, name=\"train_step\")\n",
    "\n",
    "\n",
    "with tf.name_scope(\"metrics\"):\n",
    "    correct = tf.nn.in_top_k(output, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name=\"accuracy\")\n",
    "    tb_train_writer = tf.summary.FileWriter(logs + \"/train\", tf.get_default_graph())\n",
    "    tb_cv_writer = tf.summary.FileWriter(logs + \"/cv\", tf.get_default_graph())\n",
    "    tb_acc = tf.summary.scalar(\"Accuracy\", accuracy)\n",
    "    \n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@Epoch 000: Train Accuracy 043.06% | Test Accuracy 043.12%\n",
      "@Epoch 040: Train Accuracy 096.41% | Test Accuracy 095.82%\n",
      "@Epoch 080: Train Accuracy 098.79% | Test Accuracy 097.35%\n",
      "@Epoch 120: Train Accuracy 098.74% | Test Accuracy 097.15%\n",
      "@Epoch 160: Train Accuracy 099.95% | Test Accuracy 097.65%\n",
      "@Epoch 200: Train Accuracy 100.00% | Test Accuracy 097.68%\n",
      "@Epoch 240: Train Accuracy 100.00% | Test Accuracy 097.77%\n",
      "@Epoch 280: Train Accuracy 100.00% | Test Accuracy 097.81%\n",
      "@Epoch 320: Train Accuracy 100.00% | Test Accuracy 097.84%\n",
      "@Epoch 360: Train Accuracy 100.00% | Test Accuracy 097.85%\n",
      "@Epoch 399: Train Accuracy 100.00% | Test Accuracy 097.84%\r"
     ]
    }
   ],
   "source": [
    "# Operations to compute the running mean and variance.\n",
    "# ----------------------------------------------------\n",
    "# In general, *tf.GraphKeys*, is a collection of names\n",
    "# to collect and retrieve values associated with a graph\n",
    "extra_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "model_path = \"tfmodels/ffnn_v05.ckpt\"\n",
    "epochs = 400\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for epoch in range(epochs):\n",
    "        sess.run([train_step, extra_ops], feed_dict={X: X_train, y: y_train,\n",
    "                                                     training: True})\n",
    "        acc, bn_train_acc = sess.run([accuracy, tb_acc], feed_dict={X: X_train, y:y_train})\n",
    "        cvacc, bn_cv_acc = sess.run([accuracy, tb_acc], feed_dict={X: X_test, y:y_test})\n",
    "        tb_train_writer.add_summary(bn_train_acc, epoch)\n",
    "        tb_cv_writer.add_summary(bn_cv_acc, epoch)\n",
    "        end = \"\\n\" if epoch % 40 == 0 else \"\\r\"\n",
    "        print(f\"@Epoch {epoch:03}: Train Accuracy {acc:07.2%} | Test Accuracy {cvacc:07.2%}\",\n",
    "              end=end)\n",
    "    saver.save(sess, model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Restoring the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./tfmodels/ffnn_v05.ckpt\n",
      "@Epoch 000: Train Accuracy 100.00% | Test Accuracy 097.84%\n",
      "@Epoch 010: Train Accuracy 100.00% | Test Accuracy 097.84%\n",
      "@Epoch 020: Train Accuracy 100.00% | Test Accuracy 097.83%\n",
      "@Epoch 030: Train Accuracy 100.00% | Test Accuracy 097.83%\n",
      "@Epoch 039: Train Accuracy 100.00% | Test Accuracy 097.85%\r"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "model = tf.train.import_meta_graph(\"./tfmodels/ffnn_v05.ckpt.meta\")\n",
    "new_model = tf.train.Saver()\n",
    "new_model_path = \"./tfmodels/ffnn_v05_1.ckpt\"\n",
    "\n",
    "X = tf.get_default_graph().get_tensor_by_name(\"X:0\")\n",
    "y = tf.get_default_graph().get_tensor_by_name(\"y:0\")\n",
    "training = tf.get_default_graph().get_tensor_by_name(\"training_bool:0\")\n",
    "accuracy = tf.get_default_graph().get_tensor_by_name(\"metrics/accuracy:0\")\n",
    "train_step = tf.get_default_graph().get_operation_by_name(\"train/train_step\")\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "bn_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "epochs = 40\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    model.restore(sess, \"./tfmodels/ffnn_v05.ckpt\")\n",
    "    for epoch in range(epochs):\n",
    "        sess.run([train_step, bn_ops], feed_dict={X: X_train, y: y_train,\n",
    "                                                  training: True})\n",
    "        acc = sess.run(accuracy, feed_dict={X: X_train, y: y_train})\n",
    "        cvacc = sess.run(accuracy, feed_dict={X: X_test, y: y_test})\n",
    "        end = \"\\n\" if epoch % 10 == 0 else \"\\r\"\n",
    "        print(f\"@Epoch {epoch:03}: Train Accuracy {acc:07.2%} | Test Accuracy {cvacc:07.2%}\",\n",
    "              end=end)\n",
    "    new_model.save(sess, new_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "----\n",
    "# Transfer Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.random import choice, seed\n",
    "from functools import partial\n",
    "from math import ceil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_train_0_4 = y_train < 5\n",
    "y_0_4_train = y_train[map_train_0_4] \n",
    "X_0_4_train = X_train[map_train_0_4,:]\n",
    "\n",
    "map_test_0_4 = y_test < 5\n",
    "y_0_4_test = y_test[map_test_0_4]\n",
    "X_0_4_test = X_test[map_test_0_4, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Learning V.01\n",
    "We being by training a feed-forward neural network with 5 hidden layers, elu activation function and he initializiation. In order to train this NNet, we consider Adam optimization and early stopping.\n",
    "\n",
    "For educational purposes, we will consider the test-set as the validation set. Early stopping will kick in once the accuracy on the validation (test) set at epoch $t$ drops below the accuracy at epoch $t-1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "logs = \"./tf_logs/transfer/ffnn_0_4\"\n",
    "\n",
    "n_inputs = 28 * 28\n",
    "n_hidden = 100\n",
    "n_output = 5\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int32, shape=(None), name=\"y\")\n",
    "he_init = variance_scaling_initializer()\n",
    "\n",
    "hidden_layer = partial(tf.layers.dense, units=n_hidden,\n",
    "                       activation=tf.nn.elu,\n",
    "                       kernel_initializer=he_init)\n",
    "\n",
    "with tf.name_scope(\"DNN\"):\n",
    "    for hi in range(1, 6):\n",
    "        if hi == 1:\n",
    "            hidden = hidden_layer(inputs=X, name=f\"hidden_{hi}\")\n",
    "        else:\n",
    "            hidden = hidden_layer(inputs=hidden, name=f\"hidden_{hi}\")\n",
    "\n",
    "    output = hidden_layer(inputs=hidden, units=n_output,\n",
    "                          activation=None, name=\"output\")\n",
    "\n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "        labels=y,\n",
    "        logits=output)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "\n",
    "\n",
    "alpha = 0.001\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.AdamOptimizer(alpha)\n",
    "    train_step = optimizer.minimize(loss, name=\"train_step\")\n",
    "\n",
    "\n",
    "with tf.name_scope(\"metrics\"):\n",
    "    correct = tf.nn.in_top_k(output, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name=\"accuracy\")\n",
    "    ### Writer Configuration ###\n",
    "    writer_train = tf.summary.FileWriter(logs + \"/train\", tf.get_default_graph())\n",
    "    writer_test = tf.summary.FileWriter(logs + \"/test\", tf.get_default_graph())\n",
    "    writer_accuracy = tf.summary.scalar(\"Accuracy\", accuracy)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "model_saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch @000 Train Acc: 063.78% | Test Acc: 062.87% | ∆%: 100.0000% | Test loss: 147.022% |\n",
      "Epoch @020 Train Acc: 095.47% | Test Acc: 096.26% | ∆%: 003.2442% | Test loss: 012.223% |\n",
      "Epoch @040 Train Acc: 097.14% | Test Acc: 097.65% | ∆%: 001.2901% | Test loss: 007.678% |\n",
      "Epoch @060 Train Acc: 097.72% | Test Acc: 098.03% | ∆%: 001.3796% | Test loss: 006.510% | Stag: *\n",
      "Epoch @080 Train Acc: 098.10% | Test Acc: 098.15% | ∆%: 001.0023% | Test loss: 005.961% | Stag: *\n",
      "Epoch @188 Train Acc: 099.42% | Test Acc: 098.74% | ∆%: 017.4311% | Test loss: 004.557% | Stag: ******\n",
      "Epoch @200 Train Acc: 099.66% | Test Acc: 098.87% | ∆%: 003.8576% | Test loss: 003.959% | Stag: ******\n",
      "Epoch @201 Train Acc: 099.59% | Test Acc: 098.85% | ∆%: 009.2648% | Test loss: 004.165% | Stag: ******\n",
      "Epoch @202 Train Acc: 099.68% | Test Acc: 098.87% | ∆%: 000.8255% | Test loss: 003.844% | Stag: *******\n",
      "Epoch @203 Train Acc: 099.65% | Test Acc: 098.95% | ∆%: 000.1273% | Test loss: 003.817% | Stag: *******\n",
      "Epoch @204 Train Acc: 099.69% | Test Acc: 098.93% | ∆%: 000.5236% | Test loss: 003.792% | Stag: ********\n",
      "Epoch @205 Train Acc: 099.67% | Test Acc: 098.83% | ∆%: 004.9144% | Test loss: 004.000% | Stag: ********\n",
      "Epoch @206 Train Acc: 099.68% | Test Acc: 098.85% | ∆%: 004.9584% | Test loss: 004.001% | Stag: *********\n",
      "Epoch @207 Train Acc: 099.72% | Test Acc: 098.93% | ∆%: 000.0686% | Test loss: 003.815% | Stag: *********\n",
      "Epoch @208 Train Acc: 099.69% | Test Acc: 098.91% | ∆%: 000.1199% | Test loss: 003.817% | Stag: **********\n",
      "Epoch @209 Train Acc: 099.73% | Test Acc: 098.93% | ∆%: 001.4499% | Test loss: 003.867% | Stag: **********\n",
      "Early Stopping...\n",
      "The loss change rate was of 3.93860%\n"
     ]
    }
   ],
   "source": [
    "epochs = 400\n",
    "current_test_loss = 10 ** 10\n",
    "max_stag_threshold = 20\n",
    "current_stag = 0\n",
    "mean_stag_val = 0\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for epoch in range(epochs):\n",
    "        sess.run(train_step, feed_dict={X: X_0_4_train, y: y_0_4_train})\n",
    "        train_acc, writer_train_acc = sess.run([accuracy, writer_accuracy],\n",
    "                                               feed_dict={X: X_0_4_train, y: y_0_4_train})\n",
    "        test_acc, writer_test_acc = sess.run([accuracy, writer_accuracy],\n",
    "                                              feed_dict={X: X_0_4_test, y: y_0_4_test})\n",
    "        \n",
    "        test_loss = sess.run(loss, feed_dict={X: X_0_4_test, y: y_0_4_test})\n",
    "        # Add elements to summary\n",
    "        writer_test.add_summary(writer_test_acc, global_step=epoch)\n",
    "        writer_train.add_summary(writer_train_acc, global_step=epoch)\n",
    "        # We consider an stagnation if, for 20 steps, the test set does not\n",
    "        # move either way more than 2% from the current value\n",
    "        delta_test_loss = abs(test_loss / current_test_loss - 1)\n",
    "        if delta_test_loss < 0.01 or test_loss > current_test_loss:\n",
    "            if current_stag >= max_stag_threshold:\n",
    "                print(f\"Early Stopping...\\nThe loss change rate was of {mean_stag_val / max_stag_threshold:0.5%}\")\n",
    "                break\n",
    "            else:\n",
    "                current_stag += 1\n",
    "                mean_stag_val += delta_test_loss\n",
    "                end = \"\\n\" if current_stag > 10 else \"\\r\"\n",
    "                print((f\"Epoch @{epoch:03} Train Acc: {train_acc:07.02%} \"\n",
    "                       f\"| Test Acc: {test_acc:07.02%} | ∆%: {delta_test_loss:09.4%} \"\n",
    "                       f\"| Test loss: {test_loss:08.3%} | Stag: {'*' * ceil(current_stag / 2)}\"), end=end)\n",
    "        else:\n",
    "            current_stag = 0\n",
    "            mean_stag_val = 0\n",
    "            end = \"\\n\" if epoch % 20 == 0 else \"\\r\"\n",
    "            print((f\"Epoch @{epoch:03} Train Acc: {train_acc:07.02%} \"\n",
    "                   f\"| Test Acc: {test_acc:07.02%} | ∆%: {delta_test_loss:09.4%} \"\n",
    "                   f\"| Test loss: {test_loss:08.3%} |\"), end=end)\n",
    "            current_test_loss = test_loss\n",
    "\n",
    "    model_saver.save(sess, \"./tfmodels/nnet_0_4_v01.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Learning V.02\n",
    "In order to find a better preforming 5-layer Deep Neural Network, we proceed by creating a `DNN` class with homogeneous number of neurons in each of the hidden layers. We will make use of use of `BaseEstimator` and `ClassifierMixin` in order to cross-validate the performance and pick a better model.\n",
    "\n",
    "**NOTE:** This implementation does not take into account a batch size in order to fit the model. This simple class is to be used with training data that fits into memory. (See [tf.data](https://www.tensorflow.org/programmers_guide/datasets) for an optimized way to fit models in TensorFlow via batch-GD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.exceptions import NotFittedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_train_0_4 = y_train < 5\n",
    "y_0_4_train = y_train[map_train_0_4] \n",
    "X_0_4_train = X_train[map_train_0_4,:]\n",
    "\n",
    "y_0_4_cv, y_0_4_train = y_0_4_train[:5000], y_0_4_train[5000:]\n",
    "X_0_4_cv, X_0_4_train = X_0_4_train[:5000], X_0_4_train[5000:]\n",
    "\n",
    "map_test_0_4 = y_test < 5\n",
    "y_0_4_test = y_test[map_test_0_4]\n",
    "X_0_4_test = X_test[map_test_0_4, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DnnClassifier(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, n_hidden=5, n_neurons=100, optimizer=tf.train.AdamOptimizer,\n",
    "                 learning_rate=0.01, activation_fn=tf.nn.elu,\n",
    "                 initializer=tf.variance_scaling_initializer(),\n",
    "                 bn_momentum=None, dropout_rate=None, seed_state=None):\n",
    "        self.n_hidden = n_hidden\n",
    "        self.n_neurons = n_neurons\n",
    "        self.optimizer = optimizer\n",
    "        self.learning_rate = learning_rate\n",
    "        self.activation_fn = activation_fn\n",
    "        self.initializer = initializer # default He-initializiation\n",
    "        self.bn_momentum = bn_momentum\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.seed_state = seed_state\n",
    "        self._session = None\n",
    "    \n",
    "    def _dnn(self, inputs):\n",
    "        \"\"\"\n",
    "        Build hidden layers with optional support for batch normalization and dropout\n",
    "        \"\"\"\n",
    "        for hi in range(self.n_hidden):\n",
    "            # Dropout is applied *after* the activation function,\n",
    "            # if it is not None, this first step is applying dropout\n",
    "            # to the last seen layer (X being the first 'input' layer)\n",
    "            if self.dropout_rate:\n",
    "                inputs = tf.layers.dropout(inputs, rate=self.dropout_rate, training=self._training)\n",
    "                \n",
    "            inputs = tf.layers.dense(inputs, self.n_neurons, activation=self.activation_fn,\n",
    "                                     kernel_initializer=self.initializer,\n",
    "                                     name=f\"z_{hi + 1}\")\n",
    "            \n",
    "            if self.bn_momentum:\n",
    "                inputs = tf.layers.batch_normalization(inputs, momentum=self.bn_momentum,\n",
    "                                                       training=self._training)\n",
    "            inputs = self.activation_fn(inputs, name=f\"a_{hi + 1}\")\n",
    "        \n",
    "        return inputs\n",
    "    \n",
    "    def _build_graph(self, n_inputs, n_outputs):\n",
    "        y = tf.placeholder(tf.int32, shape=(None), name=\"y\")\n",
    "        \n",
    "        if self.bn_momentum or self.dropout_rate:\n",
    "            self._training = tf.placeholder_with_default(False, shape=(), name=\"training\") \n",
    "        else:\n",
    "            self._training = None\n",
    "            \n",
    "        with tf.name_scope(\"DNN\"):\n",
    "            X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "            final_hidden_layer = self._dnn(X)\n",
    "            logits = tf.layers.dense(final_hidden_layer, n_outputs,\n",
    "                                     kernel_initializer=self.initializer,\n",
    "                                     name=\"logits\")\n",
    "            y_proba = tf.nn.softmax(logits, name=\"y_proba\")\n",
    "            \n",
    "        with tf.name_scope(\"loss\"):\n",
    "            xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "                logits=logits,\n",
    "                labels=y)\n",
    "            loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "            \n",
    "        with tf.name_scope(\"train\"):\n",
    "            optimizer = self.optimizer(self.learning_rate)\n",
    "            train_step = optimizer.minimize(loss, name=\"train_step\")\n",
    "            \n",
    "        with tf.name_scope(\"metrics\"):\n",
    "            correct = tf.nn.in_top_k(logits, y, 1)\n",
    "            accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name=\"accuracy\")\n",
    "        \n",
    "        init = tf.global_variables_initializer()\n",
    "        saver = tf.train.Saver()\n",
    "        \n",
    "        self._X = X\n",
    "        self._y = y\n",
    "        self._y_proba = y_proba\n",
    "        self._loss = loss\n",
    "        self._train_step = train_step\n",
    "        self._accuracy = accuracy\n",
    "        self._init = init\n",
    "        self._saver = saver\n",
    "        \n",
    "    def close_session(self):\n",
    "        if self._session:\n",
    "            self._session.close()\n",
    "\n",
    "    def _get_model_parameters(self):\n",
    "        \"\"\"\n",
    "        Get all variable parameters\n",
    "        \"\"\"\n",
    "        with self._graph.as_default():\n",
    "            glob_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES)\n",
    "        \n",
    "        return {glob_var.op.name: value for glob_var, value\n",
    "                in zip(glob_vars, self._session.run(glob_vars))}\n",
    "    \n",
    "    def _restore_model_parameters(self, model_params):\n",
    "        glob_vars_names = list(model_params.keys())\n",
    "        assign_ops = {\n",
    "            glob_var_name: self._graph.get_operation_by_name(glob_var_name + \"/Assign\")\n",
    "            for glob_var_name in glob_vars_names\n",
    "        }\n",
    "        init_values = {gvar_name: assign_op.inputs[1]\n",
    "                       for gvar_name, assign_op in assign_ops.items()}\n",
    "        feed_dict = {init_values[glob_var_name]: model_params[glob_var_name]\n",
    "                     for glob_var_name in glob_vars_names}\n",
    "        \n",
    "        self._session.run(assign_ops, feed_dict=feed_dict)\n",
    "        \n",
    "    def fit(self, X, y, epochs, X_cv=None, y_cv=None):\n",
    "        self.close_session()\n",
    "        \n",
    "        self.classes_ = np.unique(y) # Retrieve unique indices and sort\n",
    "        n_inputs = X.shape[1]\n",
    "        n_outputs = len(self.classes_)\n",
    "        \n",
    "        # Convert labels to a sorted index from 0 to n_classes - 1\n",
    "        self.class_to_index_ = {label: index for label, index\n",
    "                                in enumerate(self.classes_)}\n",
    "        y = np.array([self.class_to_index_[label] for label in y],\n",
    "                     dtype=np.int32)\n",
    "        \n",
    "        # ******** Early Stopping utility parameters ********\n",
    "        max_epochs_without_progress = 20\n",
    "        epochs_without_progress = 0\n",
    "        best_loss = np.infty\n",
    "        best_params = None\n",
    "                \n",
    "        # Defining the graph inside the class.\n",
    "        # By calling the _build_graph method\n",
    "        self._graph = tf.Graph()\n",
    "        with self._graph.as_default():\n",
    "            self._build_graph(n_inputs, n_outputs)\n",
    "            # Extra operations for batch normalization (if needed)\n",
    "            extra_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "        \n",
    "        self._session = tf.Session(graph=self._graph)\n",
    "        with self._session.as_default() as sess:\n",
    "            sess.run(self._init)\n",
    "            for epoch in range(epochs):\n",
    "                # Creating the Feed-Dict dictionary\n",
    "                feed_dict = {self._X: X, self._y: y}                \n",
    "                if self._training is not None:\n",
    "                    feed_dict[self._training] = True\n",
    "                \n",
    "                if self.bn_momentum is not None:\n",
    "                    sess.run([self._train_step, extra_ops], feed_dict=feed_dict)\n",
    "                else:\n",
    "                    sess.run(self._train_step, feed_dict=feed_dict)\n",
    "                    \n",
    "                end = \"\\n\" if epoch % (epochs // 10) == 0 else \"\\r\"\n",
    "                if X_cv is not None and y_cv is not None:\n",
    "                    loss_cv, acc_cv = sess.run([self._loss, self._accuracy],\n",
    "                                               feed_dict={self._X: X_cv, self._y: y_cv})\n",
    "                    if loss_cv < best_loss:\n",
    "                        best_params = self._get_model_parameters()\n",
    "                        best_loss = loss_cv\n",
    "                        epochs_without_progress = 0\n",
    "                    else:\n",
    "                        epochs_without_progress += 1\n",
    "                    \n",
    "                    print((f\"@E{epoch}\\tCV Loss: {loss_cv:0.3f}\\tCV Acc: {acc_cv:0.3%}\"\n",
    "                           f\"\\tEWP:{epochs_without_progress:02}\"), end=end)\n",
    "                    \n",
    "                    if epochs_without_progress > max_epochs_without_progress:\n",
    "                        print(\"\\n\\n...Early Stopping\")\n",
    "                        break\n",
    "                    \n",
    "                else:\n",
    "                    acc_train = sess.run(self._accuracy, feed_dict)\n",
    "                    print(f\"@E{epoch}, training accuracy: {acc_train:07.3%}\",\n",
    "                          end=end)\n",
    "            \n",
    "            # If early stopping was used, rollback to the best model found\n",
    "            if best_params is not None:\n",
    "                self._restore_model_parameters(best_params)\n",
    "            \n",
    "            return self\n",
    "        \n",
    "    def predict_proba(self, X):\n",
    "        if self._session is None:\n",
    "            raise NotFittedError(f\"This {self.__class__.__name__} instance is not fitted yet\")\n",
    "\n",
    "        with self._session.as_default() as sess:\n",
    "            y_proba_result = sess.run(self._y_proba, feed_dict={self._X: X})\n",
    "\n",
    "        return y_proba_result\n",
    "\n",
    "    def predict(self, X):\n",
    "        class_indices = np.argmax(self.predict_proba(X), axis=1)\n",
    "        return np.array([self.classes_[label] for label in class_indices],\n",
    "                        dtype=np.int32)\n",
    "\n",
    "    def save(self, path):\n",
    "        self._saver.save(self._session, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@E0\tCV Loss: 1.145\tCV Acc: 56.860%\tEWP:00\n",
      "@E40\tCV Loss: 0.078\tCV Acc: 97.400%\tEWP:00\n",
      "@E80\tCV Loss: 0.053\tCV Acc: 98.460%\tEWP:00\n",
      "@E108\tCV Loss: 0.057\tCV Acc: 98.640%\tEWP:21\n",
      "\n",
      "...Early Stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DnnClassifier(activation_fn=<function elu at 0x112deef28>, bn_momentum=None,\n",
       "       dropout_rate=None,\n",
       "       initializer=<tensorflow.python.ops.init_ops.VarianceScaling object at 0x1855c4af98>,\n",
       "       learning_rate=0.01, n_hidden=5, n_neurons=100,\n",
       "       optimizer=<class 'tensorflow.python.training.adam.AdamOptimizer'>,\n",
       "       seed_state=None)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dnn01 = DnnClassifier()\n",
    "dnn01.fit(X_0_4_train, y_0_4_train, 400, X_0_4_cv, y_0_4_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.98851916715314259"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(y_0_4_test, dnn01.predict(X_0_4_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 50 candidates, totalling 150 fits\n",
      "[CV] n_neurons=50, n_hidden=11, learning_rate=0.001, dropout_rate=0.5, bn_momentum=0.999, activation_fn=<function relu at 0x112e07d08> \n",
      "@E0\tCV Loss: 1.609\tCV Acc: 19.220%\tEWP:00\n",
      "@E21\tCV Loss: 1.611\tCV Acc: 19.220%\tEWP:21\n",
      "\n",
      "...Early Stopping\n",
      "[CV]  n_neurons=50, n_hidden=11, learning_rate=0.001, dropout_rate=0.5, bn_momentum=0.999, activation_fn=<function relu at 0x112e07d08>, total=  14.4s\n",
      "[CV] n_neurons=50, n_hidden=11, learning_rate=0.001, dropout_rate=0.5, bn_momentum=0.999, activation_fn=<function relu at 0x112e07d08> \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   14.5s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@E0\tCV Loss: 1.609\tCV Acc: 20.660%\tEWP:00\n",
      "@E21\tCV Loss: 1.610\tCV Acc: 19.380%\tEWP:21\n",
      "\n",
      "...Early Stopping\n",
      "[CV]  n_neurons=50, n_hidden=11, learning_rate=0.001, dropout_rate=0.5, bn_momentum=0.999, activation_fn=<function relu at 0x112e07d08>, total=  13.8s\n",
      "[CV] n_neurons=50, n_hidden=11, learning_rate=0.001, dropout_rate=0.5, bn_momentum=0.999, activation_fn=<function relu at 0x112e07d08> \n",
      "@E0\tCV Loss: 1.609\tCV Acc: 19.220%\tEWP:00\n",
      "@E21\tCV Loss: 1.614\tCV Acc: 20.140%\tEWP:21\n",
      "\n",
      "...Early Stopping\n",
      "[CV]  n_neurons=50, n_hidden=11, learning_rate=0.001, dropout_rate=0.5, bn_momentum=0.999, activation_fn=<function relu at 0x112e07d08>, total=  12.9s\n",
      "[CV] n_neurons=250, n_hidden=7, learning_rate=0.001, dropout_rate=0.3, bn_momentum=0.999, activation_fn=<function elu at 0x112deef28> \n",
      "@E0\tCV Loss: 1.449\tCV Acc: 84.980%\tEWP:00\n",
      "@E126\tCV Loss: 0.127\tCV Acc: 96.700%\tEWP:21\n",
      "\n",
      "...Early Stopping\n",
      "[CV]  n_neurons=250, n_hidden=7, learning_rate=0.001, dropout_rate=0.3, bn_momentum=0.999, activation_fn=<function elu at 0x112deef28>, total= 3.1min\n",
      "[CV] n_neurons=250, n_hidden=7, learning_rate=0.001, dropout_rate=0.3, bn_momentum=0.999, activation_fn=<function elu at 0x112deef28> \n",
      "@E0\tCV Loss: 1.412\tCV Acc: 88.660%\tEWP:00\n",
      "@E44\tCV Loss: 0.167\tCV Acc: 96.300%\tEWP:21\n",
      "\n",
      "...Early Stopping\n",
      "[CV]  n_neurons=250, n_hidden=7, learning_rate=0.001, dropout_rate=0.3, bn_momentum=0.999, activation_fn=<function elu at 0x112deef28>, total= 1.1min\n",
      "[CV] n_neurons=250, n_hidden=7, learning_rate=0.001, dropout_rate=0.3, bn_momentum=0.999, activation_fn=<function elu at 0x112deef28> \n",
      "@E0\tCV Loss: 1.426\tCV Acc: 76.640%\tEWP:00\n",
      "@E48\tCV Loss: 0.191\tCV Acc: 95.040%\tEWP:21\n",
      "\n",
      "...Early Stopping\n",
      "[CV]  n_neurons=250, n_hidden=7, learning_rate=0.001, dropout_rate=0.3, bn_momentum=0.999, activation_fn=<function elu at 0x112deef28>, total= 1.3min\n",
      "[CV] n_neurons=250, n_hidden=11, learning_rate=0.005, dropout_rate=0.5, bn_momentum=0.95, activation_fn=<function relu at 0x112e07d08> \n",
      "@E0\tCV Loss: 1.609\tCV Acc: 19.240%\tEWP:00\n",
      "@E23\tCV Loss: 1.648\tCV Acc: 20.140%\tEWP:21\n",
      "\n",
      "...Early Stopping\n",
      "[CV]  n_neurons=250, n_hidden=11, learning_rate=0.005, dropout_rate=0.5, bn_momentum=0.95, activation_fn=<function relu at 0x112e07d08>, total=  53.9s\n",
      "[CV] n_neurons=250, n_hidden=11, learning_rate=0.005, dropout_rate=0.5, bn_momentum=0.95, activation_fn=<function relu at 0x112e07d08> \n",
      "@E0\tCV Loss: 1.610\tCV Acc: 19.380%\tEWP:00\n",
      "@E32\tCV Loss: 1.626\tCV Acc: 20.160%\tEWP:21\n",
      "\n",
      "...Early Stopping\n",
      "[CV]  n_neurons=250, n_hidden=11, learning_rate=0.005, dropout_rate=0.5, bn_momentum=0.95, activation_fn=<function relu at 0x112e07d08>, total= 1.2min\n",
      "[CV] n_neurons=250, n_hidden=11, learning_rate=0.005, dropout_rate=0.5, bn_momentum=0.95, activation_fn=<function relu at 0x112e07d08> \n",
      "@E0\tCV Loss: 1.610\tCV Acc: 20.140%\tEWP:00\n",
      "@E31\tCV Loss: 1.655\tCV Acc: 20.140%\tEWP:21\n",
      "\n",
      "...Early Stopping\n",
      "[CV]  n_neurons=250, n_hidden=11, learning_rate=0.005, dropout_rate=0.5, bn_momentum=0.95, activation_fn=<function relu at 0x112e07d08>, total= 1.2min\n",
      "[CV] n_neurons=250, n_hidden=7, learning_rate=0.001, dropout_rate=0.2, bn_momentum=0.95, activation_fn=<function relu at 0x112e07d08> \n",
      "@E0\tCV Loss: 1.609\tCV Acc: 20.440%\tEWP:00\n",
      "@E150\tCV Loss: 0.066\tCV Acc: 98.480%\tEWP:09\n",
      "@E162\tCV Loss: 0.066\tCV Acc: 98.540%\tEWP:21\n",
      "\n",
      "...Early Stopping\n",
      "[CV]  n_neurons=250, n_hidden=7, learning_rate=0.001, dropout_rate=0.2, bn_momentum=0.95, activation_fn=<function relu at 0x112e07d08>, total= 3.9min\n",
      "[CV] n_neurons=250, n_hidden=7, learning_rate=0.001, dropout_rate=0.2, bn_momentum=0.95, activation_fn=<function relu at 0x112e07d08> \n",
      "@E0\tCV Loss: 1.609\tCV Acc: 28.060%\tEWP:00\n",
      "@E150\tCV Loss: 0.070\tCV Acc: 98.260%\tEWP:00\n",
      "@E194\tCV Loss: 0.067\tCV Acc: 98.560%\tEWP:21\n",
      "\n",
      "...Early Stopping\n",
      "[CV]  n_neurons=250, n_hidden=7, learning_rate=0.001, dropout_rate=0.2, bn_momentum=0.95, activation_fn=<function relu at 0x112e07d08>, total= 4.8min\n",
      "[CV] n_neurons=250, n_hidden=7, learning_rate=0.001, dropout_rate=0.2, bn_momentum=0.95, activation_fn=<function relu at 0x112e07d08> \n",
      "@E0\tCV Loss: 1.609\tCV Acc: 18.680%\tEWP:00\n",
      "@E150\tCV Loss: 0.072\tCV Acc: 98.280%\tEWP:12\n",
      "@E159\tCV Loss: 0.071\tCV Acc: 98.400%\tEWP:21\n",
      "\n",
      "...Early Stopping\n",
      "[CV]  n_neurons=250, n_hidden=7, learning_rate=0.001, dropout_rate=0.2, bn_momentum=0.95, activation_fn=<function relu at 0x112e07d08>, total= 3.9min\n",
      "[CV] n_neurons=100, n_hidden=7, learning_rate=0.0001, dropout_rate=0.3, bn_momentum=0.9, activation_fn=<function elu at 0x112deef28> \n",
      "@E0\tCV Loss: 1.614\tCV Acc: 16.680%\tEWP:00\n",
      "@E150\tCV Loss: 0.172\tCV Acc: 94.720%\tEWP:00\n",
      "@E300\tCV Loss: 0.125\tCV Acc: 96.300%\tEWP:00\n",
      "@E450\tCV Loss: 0.108\tCV Acc: 96.980%\tEWP:00\n",
      "@E600\tCV Loss: 0.097\tCV Acc: 97.300%\tEWP:00\n",
      "@E750\tCV Loss: 0.087\tCV Acc: 97.600%\tEWP:00\n",
      "@E900\tCV Loss: 0.078\tCV Acc: 97.880%\tEWP:00\n",
      "@E945\tCV Loss: 0.077\tCV Acc: 97.920%\tEWP:21\n",
      "\n",
      "...Early Stopping\n",
      "[CV]  n_neurons=100, n_hidden=7, learning_rate=0.0001, dropout_rate=0.3, bn_momentum=0.9, activation_fn=<function elu at 0x112deef28>, total= 8.9min\n",
      "[CV] n_neurons=100, n_hidden=7, learning_rate=0.0001, dropout_rate=0.3, bn_momentum=0.9, activation_fn=<function elu at 0x112deef28> \n",
      "@E0\tCV Loss: 1.587\tCV Acc: 34.800%\tEWP:00\n",
      "@E150\tCV Loss: 0.155\tCV Acc: 95.360%\tEWP:00\n",
      "@E300\tCV Loss: 0.118\tCV Acc: 96.460%\tEWP:00\n",
      "@E450\tCV Loss: 0.103\tCV Acc: 97.020%\tEWP:00\n",
      "@E600\tCV Loss: 0.090\tCV Acc: 97.340%\tEWP:00\n",
      "@E750\tCV Loss: 0.080\tCV Acc: 97.700%\tEWP:00\n",
      "@E900\tCV Loss: 0.071\tCV Acc: 97.840%\tEWP:00\n",
      "@E1050\tCV Loss: 0.066\tCV Acc: 98.100%\tEWP:00\n",
      "@E1200\tCV Loss: 0.060\tCV Acc: 98.320%\tEWP:00\n",
      "@E1350\tCV Loss: 0.056\tCV Acc: 98.400%\tEWP:00\n",
      "[CV]  n_neurons=100, n_hidden=7, learning_rate=0.0001, dropout_rate=0.3, bn_momentum=0.9, activation_fn=<function elu at 0x112deef28>, total=14.7min\n",
      "[CV] n_neurons=100, n_hidden=7, learning_rate=0.0001, dropout_rate=0.3, bn_momentum=0.9, activation_fn=<function elu at 0x112deef28> \n",
      "@E0\tCV Loss: 1.591\tCV Acc: 30.180%\tEWP:00\n",
      "@E150\tCV Loss: 0.144\tCV Acc: 95.460%\tEWP:00\n",
      "@E300\tCV Loss: 0.108\tCV Acc: 96.860%\tEWP:00\n",
      "@E450\tCV Loss: 0.094\tCV Acc: 97.300%\tEWP:00\n",
      "@E600\tCV Loss: 0.084\tCV Acc: 97.680%\tEWP:00\n",
      "@E750\tCV Loss: 0.076\tCV Acc: 97.960%\tEWP:01\n",
      "@E900\tCV Loss: 0.070\tCV Acc: 98.140%\tEWP:10\n",
      "@E1050\tCV Loss: 0.065\tCV Acc: 98.280%\tEWP:06\n",
      "@E1147\tCV Loss: 0.064\tCV Acc: 98.340%\tEWP:21\n",
      "\n",
      "...Early Stopping\n",
      "[CV]  n_neurons=100, n_hidden=7, learning_rate=0.0001, dropout_rate=0.3, bn_momentum=0.9, activation_fn=<function elu at 0x112deef28>, total=11.1min\n",
      "[CV] n_neurons=250, n_hidden=13, learning_rate=0.005, dropout_rate=0.5, bn_momentum=0.99, activation_fn=<function relu at 0x112e07d08> \n",
      "@E0\tCV Loss: 1.611\tCV Acc: 19.220%\tEWP:00\n",
      "@E31\tCV Loss: 1.615\tCV Acc: 19.380%\tEWP:21\n",
      "\n",
      "...Early Stopping\n",
      "[CV]  n_neurons=250, n_hidden=13, learning_rate=0.005, dropout_rate=0.5, bn_momentum=0.99, activation_fn=<function relu at 0x112e07d08>, total= 1.4min\n",
      "[CV] n_neurons=250, n_hidden=13, learning_rate=0.005, dropout_rate=0.5, bn_momentum=0.99, activation_fn=<function relu at 0x112e07d08> \n",
      "@E0\tCV Loss: 1.612\tCV Acc: 19.380%\tEWP:00\n",
      "@E114\tCV Loss: 1.401\tCV Acc: 37.800%\tEWP:21\n",
      "\n",
      "...Early Stopping\n",
      "[CV]  n_neurons=250, n_hidden=13, learning_rate=0.005, dropout_rate=0.5, bn_momentum=0.99, activation_fn=<function relu at 0x112e07d08>, total= 5.1min\n",
      "[CV] n_neurons=250, n_hidden=13, learning_rate=0.005, dropout_rate=0.5, bn_momentum=0.99, activation_fn=<function relu at 0x112e07d08> \n",
      "@E0\tCV Loss: 1.612\tCV Acc: 19.220%\tEWP:00\n",
      "@E116\tCV Loss: 1.600\tCV Acc: 37.900%\tEWP:21\n",
      "\n",
      "...Early Stopping\n",
      "[CV]  n_neurons=250, n_hidden=13, learning_rate=0.005, dropout_rate=0.5, bn_momentum=0.99, activation_fn=<function relu at 0x112e07d08>, total= 5.0min\n",
      "[CV] n_neurons=100, n_hidden=5, learning_rate=0.001, dropout_rate=0.4, bn_momentum=0.99, activation_fn=<function elu at 0x112deef28> \n",
      "@E0\tCV Loss: 1.522\tCV Acc: 43.200%\tEWP:00\n",
      "@E150\tCV Loss: 0.070\tCV Acc: 98.120%\tEWP:00\n",
      "@E256\tCV Loss: 0.060\tCV Acc: 98.320%\tEWP:21\n",
      "\n",
      "...Early Stopping\n",
      "[CV]  n_neurons=100, n_hidden=5, learning_rate=0.001, dropout_rate=0.4, bn_momentum=0.99, activation_fn=<function elu at 0x112deef28>, total= 1.9min\n",
      "[CV] n_neurons=100, n_hidden=5, learning_rate=0.001, dropout_rate=0.4, bn_momentum=0.99, activation_fn=<function elu at 0x112deef28> \n",
      "@E0\tCV Loss: 1.507\tCV Acc: 61.460%\tEWP:00\n",
      "@E150\tCV Loss: 0.072\tCV Acc: 97.900%\tEWP:00\n",
      "@E300\tCV Loss: 0.051\tCV Acc: 98.620%\tEWP:00\n",
      "@E341\tCV Loss: 0.051\tCV Acc: 98.640%\tEWP:21\n",
      "\n",
      "...Early Stopping\n",
      "[CV]  n_neurons=100, n_hidden=5, learning_rate=0.001, dropout_rate=0.4, bn_momentum=0.99, activation_fn=<function elu at 0x112deef28>, total= 2.5min\n",
      "[CV] n_neurons=100, n_hidden=5, learning_rate=0.001, dropout_rate=0.4, bn_momentum=0.99, activation_fn=<function elu at 0x112deef28> \n",
      "@E0\tCV Loss: 1.547\tCV Acc: 58.640%\tEWP:00\n",
      "@E150\tCV Loss: 0.069\tCV Acc: 97.940%\tEWP:00\n",
      "@E267\tCV Loss: 0.058\tCV Acc: 98.380%\tEWP:21\n",
      "\n",
      "...Early Stopping\n",
      "[CV]  n_neurons=100, n_hidden=5, learning_rate=0.001, dropout_rate=0.4, bn_momentum=0.99, activation_fn=<function elu at 0x112deef28>, total= 2.0min\n",
      "[CV] n_neurons=200, n_hidden=7, learning_rate=0.001, dropout_rate=0.4, bn_momentum=0.98, activation_fn=<function elu at 0x112deef28> \n",
      "@E0\tCV Loss: 1.488\tCV Acc: 79.960%\tEWP:00\n",
      "@E150\tCV Loss: 0.063\tCV Acc: 98.300%\tEWP:04\n",
      "@E209\tCV Loss: 0.055\tCV Acc: 98.440%\tEWP:21\n",
      "\n",
      "...Early Stopping\n",
      "[CV]  n_neurons=200, n_hidden=7, learning_rate=0.001, dropout_rate=0.4, bn_momentum=0.98, activation_fn=<function elu at 0x112deef28>, total= 3.9min\n",
      "[CV] n_neurons=200, n_hidden=7, learning_rate=0.001, dropout_rate=0.4, bn_momentum=0.98, activation_fn=<function elu at 0x112deef28> \n",
      "@E0\tCV Loss: 1.473\tCV Acc: 70.140%\tEWP:00\n",
      "@E150\tCV Loss: 0.056\tCV Acc: 98.520%\tEWP:00\n",
      "@E243\tCV Loss: 0.049\tCV Acc: 98.680%\tEWP:21\n",
      "\n",
      "...Early Stopping\n",
      "[CV]  n_neurons=200, n_hidden=7, learning_rate=0.001, dropout_rate=0.4, bn_momentum=0.98, activation_fn=<function elu at 0x112deef28>, total= 4.6min\n",
      "[CV] n_neurons=200, n_hidden=7, learning_rate=0.001, dropout_rate=0.4, bn_momentum=0.98, activation_fn=<function elu at 0x112deef28> \n",
      "@E0\tCV Loss: 1.479\tCV Acc: 76.000%\tEWP:00\n",
      "@E150\tCV Loss: 0.058\tCV Acc: 98.520%\tEWP:00\n",
      "@E231\tCV Loss: 0.052\tCV Acc: 98.660%\tEWP:21\n",
      "\n",
      "...Early Stopping\n",
      "[CV]  n_neurons=200, n_hidden=7, learning_rate=0.001, dropout_rate=0.4, bn_momentum=0.98, activation_fn=<function elu at 0x112deef28>, total= 4.4min\n",
      "[CV] n_neurons=300, n_hidden=13, learning_rate=0.005, dropout_rate=0.3, bn_momentum=0.98, activation_fn=<function relu at 0x112e07d08> \n",
      "@E0\tCV Loss: 1.611\tCV Acc: 19.220%\tEWP:00\n",
      "@E42\tCV Loss: 1.376\tCV Acc: 37.500%\tEWP:21\n",
      "\n",
      "...Early Stopping\n",
      "[CV]  n_neurons=300, n_hidden=13, learning_rate=0.005, dropout_rate=0.3, bn_momentum=0.98, activation_fn=<function relu at 0x112e07d08>, total= 2.4min\n",
      "[CV] n_neurons=300, n_hidden=13, learning_rate=0.005, dropout_rate=0.3, bn_momentum=0.98, activation_fn=<function relu at 0x112e07d08> \n",
      "@E0\tCV Loss: 1.611\tCV Acc: 19.380%\tEWP:00\n",
      "@E43\tCV Loss: 2.623\tCV Acc: 32.040%\tEWP:21\n",
      "\n",
      "...Early Stopping\n",
      "[CV]  n_neurons=300, n_hidden=13, learning_rate=0.005, dropout_rate=0.3, bn_momentum=0.98, activation_fn=<function relu at 0x112e07d08>, total= 2.5min\n",
      "[CV] n_neurons=300, n_hidden=13, learning_rate=0.005, dropout_rate=0.3, bn_momentum=0.98, activation_fn=<function relu at 0x112e07d08> \n",
      "@E0\tCV Loss: 1.611\tCV Acc: 19.220%\tEWP:00\n",
      "@E44\tCV Loss: 1.515\tCV Acc: 38.140%\tEWP:21\n",
      "\n",
      "...Early Stopping\n",
      "[CV]  n_neurons=300, n_hidden=13, learning_rate=0.005, dropout_rate=0.3, bn_momentum=0.98, activation_fn=<function relu at 0x112e07d08>, total= 2.5min\n",
      "[CV] n_neurons=100, n_hidden=7, learning_rate=0.0001, dropout_rate=0.6, bn_momentum=0.9, activation_fn=<function elu at 0x112deef28> \n",
      "@E0\tCV Loss: 1.595\tCV Acc: 34.580%\tEWP:00\n",
      "@E150\tCV Loss: 0.902\tCV Acc: 62.140%\tEWP:00\n",
      "@E300\tCV Loss: 0.452\tCV Acc: 77.200%\tEWP:00\n",
      "@E450\tCV Loss: 0.387\tCV Acc: 78.900%\tEWP:00\n",
      "@E600\tCV Loss: 0.360\tCV Acc: 91.300%\tEWP:00\n",
      "@E750\tCV Loss: 0.214\tCV Acc: 94.220%\tEWP:00\n",
      "@E900\tCV Loss: 0.149\tCV Acc: 95.720%\tEWP:00\n",
      "@E1050\tCV Loss: 0.137\tCV Acc: 96.080%\tEWP:00\n",
      "@E1200\tCV Loss: 0.131\tCV Acc: 96.220%\tEWP:00\n",
      "@E1350\tCV Loss: 0.125\tCV Acc: 96.520%\tEWP:00\n",
      "[CV]  n_neurons=100, n_hidden=7, learning_rate=0.0001, dropout_rate=0.6, bn_momentum=0.9, activation_fn=<function elu at 0x112deef28>, total=14.6min\n",
      "[CV] n_neurons=100, n_hidden=7, learning_rate=0.0001, dropout_rate=0.6, bn_momentum=0.9, activation_fn=<function elu at 0x112deef28> \n",
      "@E0\tCV Loss: 1.622\tCV Acc: 12.740%\tEWP:00\n",
      "@E150\tCV Loss: 1.293\tCV Acc: 43.680%\tEWP:00\n",
      "@E300\tCV Loss: 0.531\tCV Acc: 78.240%\tEWP:00\n",
      "@E450\tCV Loss: 0.228\tCV Acc: 92.940%\tEWP:00\n",
      "@E600\tCV Loss: 0.176\tCV Acc: 94.920%\tEWP:00\n",
      "@E750\tCV Loss: 0.161\tCV Acc: 95.500%\tEWP:00\n",
      "@E900\tCV Loss: 0.152\tCV Acc: 95.740%\tEWP:00\n",
      "@E1050\tCV Loss: 0.144\tCV Acc: 96.000%\tEWP:00\n",
      "@E1200\tCV Loss: 0.138\tCV Acc: 96.240%\tEWP:10\n",
      "@E1350\tCV Loss: 0.132\tCV Acc: 96.440%\tEWP:02\n",
      "[CV]  n_neurons=100, n_hidden=7, learning_rate=0.0001, dropout_rate=0.6, bn_momentum=0.9, activation_fn=<function elu at 0x112deef28>, total=14.5min\n",
      "[CV] n_neurons=100, n_hidden=7, learning_rate=0.0001, dropout_rate=0.6, bn_momentum=0.9, activation_fn=<function elu at 0x112deef28> \n",
      "@E0\tCV Loss: 1.592\tCV Acc: 21.640%\tEWP:00\n",
      "@E65\tCV Loss: 1.533\tCV Acc: 22.020%\tEWP:21\n",
      "\n",
      "...Early Stopping\n",
      "[CV]  n_neurons=100, n_hidden=7, learning_rate=0.0001, dropout_rate=0.6, bn_momentum=0.9, activation_fn=<function elu at 0x112deef28>, total=  39.2s\n",
      "[CV] n_neurons=300, n_hidden=11, learning_rate=0.0001, dropout_rate=0.2, bn_momentum=0.98, activation_fn=<function elu at 0x112deef28> \n",
      "@E0\tCV Loss: 1.587\tCV Acc: 44.460%\tEWP:00\n",
      "@E150\tCV Loss: 0.086\tCV Acc: 97.860%\tEWP:00\n",
      "@E300\tCV Loss: 0.065\tCV Acc: 98.360%\tEWP:00\n",
      "@E425\tCV Loss: 0.057\tCV Acc: 98.600%\tEWP:21\n",
      "\n",
      "...Early Stopping\n",
      "[CV]  n_neurons=300, n_hidden=11, learning_rate=0.0001, dropout_rate=0.2, bn_momentum=0.98, activation_fn=<function elu at 0x112deef28>, total=19.1min\n",
      "[CV] n_neurons=300, n_hidden=11, learning_rate=0.0001, dropout_rate=0.2, bn_momentum=0.98, activation_fn=<function elu at 0x112deef28> \n",
      "@E0\tCV Loss: 1.590\tCV Acc: 43.820%\tEWP:00\n",
      "@E150\tCV Loss: 0.096\tCV Acc: 97.520%\tEWP:00\n",
      "@E300\tCV Loss: 0.073\tCV Acc: 98.260%\tEWP:01\n",
      "@E419\tCV Loss: 0.066\tCV Acc: 98.440%\tEWP:21\n",
      "\n",
      "...Early Stopping\n",
      "[CV]  n_neurons=300, n_hidden=11, learning_rate=0.0001, dropout_rate=0.2, bn_momentum=0.98, activation_fn=<function elu at 0x112deef28>, total=19.2min\n",
      "[CV] n_neurons=300, n_hidden=11, learning_rate=0.0001, dropout_rate=0.2, bn_momentum=0.98, activation_fn=<function elu at 0x112deef28> \n",
      "@E0\tCV Loss: 1.592\tCV Acc: 47.820%\tEWP:00\n",
      "@E150\tCV Loss: 0.084\tCV Acc: 97.820%\tEWP:00\n",
      "@E300\tCV Loss: 0.068\tCV Acc: 98.420%\tEWP:02\n",
      "@E409\tCV Loss: 0.064\tCV Acc: 98.660%\tEWP:21\n",
      "\n",
      "...Early Stopping\n",
      "[CV]  n_neurons=300, n_hidden=11, learning_rate=0.0001, dropout_rate=0.2, bn_momentum=0.98, activation_fn=<function elu at 0x112deef28>, total=18.5min\n",
      "[CV] n_neurons=200, n_hidden=11, learning_rate=0.001, dropout_rate=0.3, bn_momentum=0.95, activation_fn=<function elu at 0x112deef28> \n",
      "@E0\tCV Loss: 1.523\tCV Acc: 74.320%\tEWP:00\n",
      "@E32\tCV Loss: 0.202\tCV Acc: 97.060%\tEWP:21\n",
      "\n",
      "...Early Stopping\n",
      "[CV]  n_neurons=200, n_hidden=11, learning_rate=0.001, dropout_rate=0.3, bn_momentum=0.95, activation_fn=<function elu at 0x112deef28>, total=  53.9s\n",
      "[CV] n_neurons=200, n_hidden=11, learning_rate=0.001, dropout_rate=0.3, bn_momentum=0.95, activation_fn=<function elu at 0x112deef28> \n",
      "@E0\tCV Loss: 1.532\tCV Acc: 63.680%\tEWP:00\n",
      "@E32\tCV Loss: 0.207\tCV Acc: 96.740%\tEWP:21\n",
      "\n",
      "...Early Stopping\n",
      "[CV]  n_neurons=200, n_hidden=11, learning_rate=0.001, dropout_rate=0.3, bn_momentum=0.95, activation_fn=<function elu at 0x112deef28>, total=  54.2s\n",
      "[CV] n_neurons=200, n_hidden=11, learning_rate=0.001, dropout_rate=0.3, bn_momentum=0.95, activation_fn=<function elu at 0x112deef28> \n",
      "@E0\tCV Loss: 1.551\tCV Acc: 71.040%\tEWP:00\n",
      "@E32\tCV Loss: 0.209\tCV Acc: 96.800%\tEWP:21\n",
      "\n",
      "...Early Stopping\n",
      "[CV]  n_neurons=200, n_hidden=11, learning_rate=0.001, dropout_rate=0.3, bn_momentum=0.95, activation_fn=<function elu at 0x112deef28>, total=  53.9s\n",
      "[CV] n_neurons=50, n_hidden=11, learning_rate=0.001, dropout_rate=0.3, bn_momentum=0.9, activation_fn=<function elu at 0x112deef28> \n",
      "@E0\tCV Loss: 1.601\tCV Acc: 27.380%\tEWP:00\n",
      "@E150\tCV Loss: 0.111\tCV Acc: 97.520%\tEWP:00\n",
      "@E300\tCV Loss: 0.079\tCV Acc: 98.280%\tEWP:05\n",
      "@E316\tCV Loss: 0.078\tCV Acc: 98.300%\tEWP:21\n",
      "\n",
      "...Early Stopping\n",
      "[CV]  n_neurons=50, n_hidden=11, learning_rate=0.001, dropout_rate=0.3, bn_momentum=0.9, activation_fn=<function elu at 0x112deef28>, total= 2.4min\n",
      "[CV] n_neurons=50, n_hidden=11, learning_rate=0.001, dropout_rate=0.3, bn_momentum=0.9, activation_fn=<function elu at 0x112deef28> \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@E0\tCV Loss: 1.586\tCV Acc: 42.920%\tEWP:00\n",
      "@E150\tCV Loss: 0.110\tCV Acc: 97.600%\tEWP:00\n",
      "@E269\tCV Loss: 0.083\tCV Acc: 98.180%\tEWP:21\n",
      "\n",
      "...Early Stopping\n",
      "[CV]  n_neurons=50, n_hidden=11, learning_rate=0.001, dropout_rate=0.3, bn_momentum=0.9, activation_fn=<function elu at 0x112deef28>, total= 2.0min\n",
      "[CV] n_neurons=50, n_hidden=11, learning_rate=0.001, dropout_rate=0.3, bn_momentum=0.9, activation_fn=<function elu at 0x112deef28> \n",
      "@E0\tCV Loss: 1.597\tCV Acc: 35.280%\tEWP:00\n",
      "@E150\tCV Loss: 0.107\tCV Acc: 97.580%\tEWP:00\n",
      "@E210\tCV Loss: 0.097\tCV Acc: 97.960%\tEWP:21\n",
      "\n",
      "...Early Stopping\n",
      "[CV]  n_neurons=50, n_hidden=11, learning_rate=0.001, dropout_rate=0.3, bn_momentum=0.9, activation_fn=<function elu at 0x112deef28>, total= 1.6min\n",
      "[CV] n_neurons=200, n_hidden=5, learning_rate=0.0001, dropout_rate=0.6, bn_momentum=0.99, activation_fn=<function relu at 0x112e07d08> \n",
      "@E0\tCV Loss: 1.609\tCV Acc: 13.680%\tEWP:00\n",
      "@E24\tCV Loss: 1.609\tCV Acc: 22.020%\tEWP:21\n",
      "\n",
      "...Early Stopping\n",
      "[CV]  n_neurons=200, n_hidden=5, learning_rate=0.0001, dropout_rate=0.6, bn_momentum=0.99, activation_fn=<function relu at 0x112e07d08>, total=  20.2s\n",
      "[CV] n_neurons=200, n_hidden=5, learning_rate=0.0001, dropout_rate=0.6, bn_momentum=0.99, activation_fn=<function relu at 0x112e07d08> \n",
      "@E0\tCV Loss: 1.608\tCV Acc: 29.760%\tEWP:00\n",
      "@E21\tCV Loss: 1.609\tCV Acc: 19.220%\tEWP:21\n",
      "\n",
      "...Early Stopping\n",
      "[CV]  n_neurons=200, n_hidden=5, learning_rate=0.0001, dropout_rate=0.6, bn_momentum=0.99, activation_fn=<function relu at 0x112e07d08>, total=  17.9s\n",
      "[CV] n_neurons=200, n_hidden=5, learning_rate=0.0001, dropout_rate=0.6, bn_momentum=0.99, activation_fn=<function relu at 0x112e07d08> \n",
      "@E0\tCV Loss: 1.606\tCV Acc: 21.660%\tEWP:00\n",
      "@E21\tCV Loss: 1.609\tCV Acc: 22.020%\tEWP:21\n",
      "\n",
      "...Early Stopping\n",
      "[CV]  n_neurons=200, n_hidden=5, learning_rate=0.0001, dropout_rate=0.6, bn_momentum=0.99, activation_fn=<function relu at 0x112e07d08>, total=  17.7s\n",
      "[CV] n_neurons=100, n_hidden=7, learning_rate=0.005, dropout_rate=0.6, bn_momentum=0.9, activation_fn=<function elu at 0x112deef28> \n",
      "@E0\tCV Loss: 1.531\tCV Acc: 38.960%\tEWP:00\n",
      "@E150\tCV Loss: 0.082\tCV Acc: 97.780%\tEWP:00\n",
      "@E248\tCV Loss: 0.069\tCV Acc: 98.180%\tEWP:21\n",
      "\n",
      "...Early Stopping\n",
      "[CV]  n_neurons=100, n_hidden=7, learning_rate=0.005, dropout_rate=0.6, bn_momentum=0.9, activation_fn=<function elu at 0x112deef28>, total= 2.1min\n",
      "[CV] n_neurons=100, n_hidden=7, learning_rate=0.005, dropout_rate=0.6, bn_momentum=0.9, activation_fn=<function elu at 0x112deef28> \n",
      "@E0\tCV Loss: 1.548\tCV Acc: 50.680%\tEWP:00\n",
      "@E150\tCV Loss: 0.078\tCV Acc: 97.920%\tEWP:02\n",
      "@E233\tCV Loss: 0.065\tCV Acc: 98.280%\tEWP:21\n",
      "\n",
      "...Early Stopping\n",
      "[CV]  n_neurons=100, n_hidden=7, learning_rate=0.005, dropout_rate=0.6, bn_momentum=0.9, activation_fn=<function elu at 0x112deef28>, total= 2.0min\n",
      "[CV] n_neurons=100, n_hidden=7, learning_rate=0.005, dropout_rate=0.6, bn_momentum=0.9, activation_fn=<function elu at 0x112deef28> \n",
      "@E0\tCV Loss: 1.534\tCV Acc: 45.100%\tEWP:00\n",
      "@E150\tCV Loss: 0.077\tCV Acc: 98.020%\tEWP:04\n",
      "@E236\tCV Loss: 0.065\tCV Acc: 98.400%\tEWP:21\n",
      "\n",
      "...Early Stopping\n",
      "[CV]  n_neurons=100, n_hidden=7, learning_rate=0.005, dropout_rate=0.6, bn_momentum=0.9, activation_fn=<function elu at 0x112deef28>, total= 2.0min\n",
      "[CV] n_neurons=100, n_hidden=11, learning_rate=0.005, dropout_rate=0.6, bn_momentum=0.999, activation_fn=<function relu at 0x112e07d08> \n",
      "@E0\tCV Loss: 1.610\tCV Acc: 19.220%\tEWP:00\n",
      "@E21\tCV Loss: 2.615\tCV Acc: 19.380%\tEWP:21\n",
      "\n",
      "...Early Stopping\n",
      "[CV]  n_neurons=100, n_hidden=11, learning_rate=0.005, dropout_rate=0.6, bn_momentum=0.999, activation_fn=<function relu at 0x112e07d08>, total=  18.3s\n",
      "[CV] n_neurons=100, n_hidden=11, learning_rate=0.005, dropout_rate=0.6, bn_momentum=0.999, activation_fn=<function relu at 0x112e07d08> \n",
      "@E0\tCV Loss: 1.610\tCV Acc: 19.220%\tEWP:00\n",
      "@E21\tCV Loss: 2.220\tCV Acc: 19.380%\tEWP:21\n",
      "\n",
      "...Early Stopping\n",
      "[CV]  n_neurons=100, n_hidden=11, learning_rate=0.005, dropout_rate=0.6, bn_momentum=0.999, activation_fn=<function relu at 0x112e07d08>, total=  18.3s\n",
      "[CV] n_neurons=100, n_hidden=11, learning_rate=0.005, dropout_rate=0.6, bn_momentum=0.999, activation_fn=<function relu at 0x112e07d08> \n",
      "@E0\tCV Loss: 1.610\tCV Acc: 19.220%\tEWP:00\n",
      "@E21\tCV Loss: 2.376\tCV Acc: 19.380%\tEWP:21\n",
      "\n",
      "...Early Stopping\n",
      "[CV]  n_neurons=100, n_hidden=11, learning_rate=0.005, dropout_rate=0.6, bn_momentum=0.999, activation_fn=<function relu at 0x112e07d08>, total=  18.5s\n",
      "[CV] n_neurons=100, n_hidden=5, learning_rate=0.0001, dropout_rate=0.4, bn_momentum=0.9, activation_fn=<function relu at 0x112e07d08> \n",
      "@E0\tCV Loss: 1.609\tCV Acc: 18.700%\tEWP:00\n",
      "@E150\tCV Loss: 1.453\tCV Acc: 43.760%\tEWP:00\n",
      "@E300\tCV Loss: 1.023\tCV Acc: 64.060%\tEWP:00\n",
      "@E450\tCV Loss: 0.760\tCV Acc: 88.660%\tEWP:00\n",
      "@E600\tCV Loss: 0.574\tCV Acc: 92.280%\tEWP:00\n",
      "@E750\tCV Loss: 0.364\tCV Acc: 94.560%\tEWP:00\n",
      "@E900\tCV Loss: 0.238\tCV Acc: 96.120%\tEWP:00\n",
      "@E1050\tCV Loss: 0.164\tCV Acc: 96.920%\tEWP:00\n",
      "@E1200\tCV Loss: 0.123\tCV Acc: 97.380%\tEWP:00\n",
      "@E1350\tCV Loss: 0.103\tCV Acc: 97.680%\tEWP:00\n",
      "[CV]  n_neurons=100, n_hidden=5, learning_rate=0.0001, dropout_rate=0.4, bn_momentum=0.9, activation_fn=<function relu at 0x112e07d08>, total= 9.7min\n",
      "[CV] n_neurons=100, n_hidden=5, learning_rate=0.0001, dropout_rate=0.4, bn_momentum=0.9, activation_fn=<function relu at 0x112e07d08> \n",
      "@E0\tCV Loss: 1.609\tCV Acc: 12.680%\tEWP:00\n",
      "@E150\tCV Loss: 1.413\tCV Acc: 50.840%\tEWP:00\n",
      "@E300\tCV Loss: 1.066\tCV Acc: 75.100%\tEWP:00\n",
      "@E450\tCV Loss: 0.880\tCV Acc: 83.520%\tEWP:00\n",
      "@E600\tCV Loss: 0.738\tCV Acc: 89.580%\tEWP:00\n",
      "@E750\tCV Loss: 0.608\tCV Acc: 93.660%\tEWP:00\n",
      "@E900\tCV Loss: 0.456\tCV Acc: 95.500%\tEWP:00\n",
      "@E1050\tCV Loss: 0.234\tCV Acc: 96.580%\tEWP:00\n",
      "@E1200\tCV Loss: 0.169\tCV Acc: 97.200%\tEWP:00\n",
      "@E1350\tCV Loss: 0.134\tCV Acc: 97.720%\tEWP:00\n",
      "[CV]  n_neurons=100, n_hidden=5, learning_rate=0.0001, dropout_rate=0.4, bn_momentum=0.9, activation_fn=<function relu at 0x112e07d08>, total= 9.7min\n",
      "[CV] n_neurons=100, n_hidden=5, learning_rate=0.0001, dropout_rate=0.4, bn_momentum=0.9, activation_fn=<function relu at 0x112e07d08> \n",
      "@E0\tCV Loss: 1.609\tCV Acc: 22.060%\tEWP:00\n",
      "@E150\tCV Loss: 1.356\tCV Acc: 52.440%\tEWP:00\n",
      "@E300\tCV Loss: 1.035\tCV Acc: 80.820%\tEWP:00\n",
      "@E450\tCV Loss: 0.820\tCV Acc: 68.140%\tEWP:00\n",
      "@E600\tCV Loss: 0.594\tCV Acc: 91.120%\tEWP:00\n",
      "@E750\tCV Loss: 0.403\tCV Acc: 94.080%\tEWP:00\n",
      "@E900\tCV Loss: 0.277\tCV Acc: 95.660%\tEWP:00\n",
      "@E1050\tCV Loss: 0.207\tCV Acc: 96.520%\tEWP:00\n",
      "@E1200\tCV Loss: 0.164\tCV Acc: 97.060%\tEWP:00\n",
      "@E1350\tCV Loss: 0.136\tCV Acc: 97.300%\tEWP:00\n",
      "[CV]  n_neurons=100, n_hidden=5, learning_rate=0.0001, dropout_rate=0.4, bn_momentum=0.9, activation_fn=<function relu at 0x112e07d08>, total= 9.7min\n",
      "[CV] n_neurons=100, n_hidden=13, learning_rate=0.001, dropout_rate=0.5, bn_momentum=0.98, activation_fn=<function relu at 0x112e07d08> \n",
      "@E0\tCV Loss: 1.609\tCV Acc: 19.220%\tEWP:00\n",
      "@E49\tCV Loss: 1.609\tCV Acc: 20.140%\tEWP:21\n",
      "\n",
      "...Early Stopping\n",
      "[CV]  n_neurons=100, n_hidden=13, learning_rate=0.001, dropout_rate=0.5, bn_momentum=0.98, activation_fn=<function relu at 0x112e07d08>, total=  42.9s\n",
      "[CV] n_neurons=100, n_hidden=13, learning_rate=0.001, dropout_rate=0.5, bn_momentum=0.98, activation_fn=<function relu at 0x112e07d08> \n",
      "@E0\tCV Loss: 1.609\tCV Acc: 20.140%\tEWP:00\n",
      "@E57\tCV Loss: 1.609\tCV Acc: 20.140%\tEWP:21\n",
      "\n",
      "...Early Stopping\n",
      "[CV]  n_neurons=100, n_hidden=13, learning_rate=0.001, dropout_rate=0.5, bn_momentum=0.98, activation_fn=<function relu at 0x112e07d08>, total=  49.1s\n",
      "[CV] n_neurons=100, n_hidden=13, learning_rate=0.001, dropout_rate=0.5, bn_momentum=0.98, activation_fn=<function relu at 0x112e07d08> \n",
      "@E0\tCV Loss: 1.609\tCV Acc: 19.380%\tEWP:00\n",
      "@E54\tCV Loss: 1.609\tCV Acc: 22.020%\tEWP:21\n",
      "\n",
      "...Early Stopping\n",
      "[CV]  n_neurons=100, n_hidden=13, learning_rate=0.001, dropout_rate=0.5, bn_momentum=0.98, activation_fn=<function relu at 0x112e07d08>, total=  46.8s\n",
      "[CV] n_neurons=50, n_hidden=11, learning_rate=0.1, dropout_rate=0.5, bn_momentum=0.98, activation_fn=<function elu at 0x112deef28> \n",
      "@E0\tCV Loss: 2.615\tCV Acc: 17.100%\tEWP:00\n",
      "@E21\tCV Loss: 984.025\tCV Acc: 40.940%\tEWP:21061\n",
      "\n",
      "...Early Stopping\n",
      "[CV]  n_neurons=50, n_hidden=11, learning_rate=0.1, dropout_rate=0.5, bn_momentum=0.98, activation_fn=<function elu at 0x112deef28>, total=  12.9s\n",
      "[CV] n_neurons=50, n_hidden=11, learning_rate=0.1, dropout_rate=0.5, bn_momentum=0.98, activation_fn=<function elu at 0x112deef28> \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@E0\tCV Loss: 2.731\tCV Acc: 20.100%\tEWP:00\n",
      "@E21\tCV Loss: 2874.552\tCV Acc: 37.940%\tEWP:214\n",
      "\n",
      "...Early Stopping\n",
      "[CV]  n_neurons=50, n_hidden=11, learning_rate=0.1, dropout_rate=0.5, bn_momentum=0.98, activation_fn=<function elu at 0x112deef28>, total=  13.0s\n",
      "[CV] n_neurons=50, n_hidden=11, learning_rate=0.1, dropout_rate=0.5, bn_momentum=0.98, activation_fn=<function elu at 0x112deef28> \n",
      "@E0\tCV Loss: 1.828\tCV Acc: 19.080%\tEWP:00\n",
      "@E21\tCV Loss: 486.167\tCV Acc: 40.600%\tEWP:213\n",
      "\n",
      "...Early Stopping\n",
      "[CV]  n_neurons=50, n_hidden=11, learning_rate=0.1, dropout_rate=0.5, bn_momentum=0.98, activation_fn=<function elu at 0x112deef28>, total=  13.1s\n",
      "[CV] n_neurons=250, n_hidden=7, learning_rate=0.005, dropout_rate=0.3, bn_momentum=0.98, activation_fn=<function elu at 0x112deef28> \n",
      "@E0\tCV Loss: 1.269\tCV Acc: 53.700%\tEWP:00\n",
      "@E22\tCV Loss: 0.609\tCV Acc: 96.340%\tEWP:21\n",
      "\n",
      "...Early Stopping\n",
      "[CV]  n_neurons=250, n_hidden=7, learning_rate=0.005, dropout_rate=0.3, bn_momentum=0.98, activation_fn=<function elu at 0x112deef28>, total=  33.7s\n",
      "[CV] n_neurons=250, n_hidden=7, learning_rate=0.005, dropout_rate=0.3, bn_momentum=0.98, activation_fn=<function elu at 0x112deef28> \n",
      "@E0\tCV Loss: 1.266\tCV Acc: 46.940%\tEWP:00\n",
      "@E23\tCV Loss: 0.430\tCV Acc: 97.280%\tEWP:21\n",
      "\n",
      "...Early Stopping\n",
      "[CV]  n_neurons=250, n_hidden=7, learning_rate=0.005, dropout_rate=0.3, bn_momentum=0.98, activation_fn=<function elu at 0x112deef28>, total=  33.5s\n",
      "[CV] n_neurons=250, n_hidden=7, learning_rate=0.005, dropout_rate=0.3, bn_momentum=0.98, activation_fn=<function elu at 0x112deef28> \n",
      "@E0\tCV Loss: 1.386\tCV Acc: 30.380%\tEWP:00\n",
      "@E150\tCV Loss: 0.052\tCV Acc: 98.760%\tEWP:00\n",
      "@E218\tCV Loss: 0.051\tCV Acc: 98.940%\tEWP:21\n",
      "\n",
      "...Early Stopping\n",
      "[CV]  n_neurons=250, n_hidden=7, learning_rate=0.005, dropout_rate=0.3, bn_momentum=0.98, activation_fn=<function elu at 0x112deef28>, total= 5.0min\n",
      "[CV] n_neurons=200, n_hidden=5, learning_rate=0.0001, dropout_rate=0.6, bn_momentum=0.999, activation_fn=<function elu at 0x112deef28> \n",
      "@E0\tCV Loss: 1.616\tCV Acc: 18.440%\tEWP:00\n",
      "@E150\tCV Loss: 0.221\tCV Acc: 93.160%\tEWP:00\n",
      "@E300\tCV Loss: 0.172\tCV Acc: 95.560%\tEWP:00\n",
      "@E450\tCV Loss: 0.155\tCV Acc: 96.180%\tEWP:00\n",
      "@E600\tCV Loss: 0.139\tCV Acc: 96.660%\tEWP:00\n",
      "@E750\tCV Loss: 0.125\tCV Acc: 97.100%\tEWP:00\n",
      "@E900\tCV Loss: 0.113\tCV Acc: 97.280%\tEWP:00\n",
      "@E1050\tCV Loss: 0.101\tCV Acc: 97.500%\tEWP:00\n",
      "@E1200\tCV Loss: 0.092\tCV Acc: 97.640%\tEWP:00\n",
      "@E1350\tCV Loss: 0.084\tCV Acc: 97.660%\tEWP:00\n",
      "[CV]  n_neurons=200, n_hidden=5, learning_rate=0.0001, dropout_rate=0.6, bn_momentum=0.999, activation_fn=<function elu at 0x112deef28>, total=19.8min\n",
      "[CV] n_neurons=200, n_hidden=5, learning_rate=0.0001, dropout_rate=0.6, bn_momentum=0.999, activation_fn=<function elu at 0x112deef28> \n",
      "@E0\tCV Loss: 1.594\tCV Acc: 25.540%\tEWP:00\n",
      "@E150\tCV Loss: 0.235\tCV Acc: 92.580%\tEWP:00\n",
      "@E300\tCV Loss: 0.175\tCV Acc: 95.320%\tEWP:00\n",
      "@E450\tCV Loss: 0.153\tCV Acc: 96.200%\tEWP:00\n",
      "@E600\tCV Loss: 0.139\tCV Acc: 96.640%\tEWP:00\n",
      "@E750\tCV Loss: 0.126\tCV Acc: 96.860%\tEWP:00\n",
      "@E900\tCV Loss: 0.115\tCV Acc: 97.100%\tEWP:00\n",
      "@E1050\tCV Loss: 0.105\tCV Acc: 97.340%\tEWP:00\n",
      "@E1200\tCV Loss: 0.096\tCV Acc: 97.500%\tEWP:00\n",
      "@E1350\tCV Loss: 0.088\tCV Acc: 97.580%\tEWP:00\n",
      "[CV]  n_neurons=200, n_hidden=5, learning_rate=0.0001, dropout_rate=0.6, bn_momentum=0.999, activation_fn=<function elu at 0x112deef28>, total=20.7min\n",
      "[CV] n_neurons=200, n_hidden=5, learning_rate=0.0001, dropout_rate=0.6, bn_momentum=0.999, activation_fn=<function elu at 0x112deef28> \n",
      "@E0\tCV Loss: 1.614\tCV Acc: 18.080%\tEWP:00\n",
      "@E150\tCV Loss: 0.236\tCV Acc: 92.660%\tEWP:00\n",
      "@E300\tCV Loss: 0.165\tCV Acc: 95.740%\tEWP:00\n",
      "@E450\tCV Loss: 0.145\tCV Acc: 96.500%\tEWP:00\n",
      "@E600\tCV Loss: 0.130\tCV Acc: 96.900%\tEWP:00\n",
      "@E750\tCV Loss: 0.116\tCV Acc: 97.140%\tEWP:00\n",
      "@E900\tCV Loss: 0.104\tCV Acc: 97.220%\tEWP:00\n",
      "@E1050\tCV Loss: 0.095\tCV Acc: 97.440%\tEWP:00\n",
      "@E1200\tCV Loss: 0.086\tCV Acc: 97.700%\tEWP:00\n",
      "@E1350\tCV Loss: 0.080\tCV Acc: 97.840%\tEWP:00\n",
      "[CV]  n_neurons=200, n_hidden=5, learning_rate=0.0001, dropout_rate=0.6, bn_momentum=0.999, activation_fn=<function elu at 0x112deef28>, total=21.7min\n",
      "[CV] n_neurons=100, n_hidden=13, learning_rate=0.001, dropout_rate=0.2, bn_momentum=0.98, activation_fn=<function relu at 0x112e07d08> \n",
      "@E0\tCV Loss: 1.609\tCV Acc: 19.380%\tEWP:00\n",
      "@E54\tCV Loss: 1.609\tCV Acc: 20.140%\tEWP:21\n",
      "\n",
      "...Early Stopping\n",
      "[CV]  n_neurons=100, n_hidden=13, learning_rate=0.001, dropout_rate=0.2, bn_momentum=0.98, activation_fn=<function relu at 0x112e07d08>, total=  51.9s\n",
      "[CV] n_neurons=100, n_hidden=13, learning_rate=0.001, dropout_rate=0.2, bn_momentum=0.98, activation_fn=<function relu at 0x112e07d08> \n",
      "@E0\tCV Loss: 1.609\tCV Acc: 20.140%\tEWP:00\n",
      "@E65\tCV Loss: 1.609\tCV Acc: 22.020%\tEWP:21\n",
      "\n",
      "...Early Stopping\n",
      "[CV]  n_neurons=100, n_hidden=13, learning_rate=0.001, dropout_rate=0.2, bn_momentum=0.98, activation_fn=<function relu at 0x112e07d08>, total= 1.0min\n",
      "[CV] n_neurons=100, n_hidden=13, learning_rate=0.001, dropout_rate=0.2, bn_momentum=0.98, activation_fn=<function relu at 0x112e07d08> \n",
      "@E0\tCV Loss: 1.609\tCV Acc: 19.220%\tEWP:00\n",
      "@E53\tCV Loss: 1.609\tCV Acc: 20.140%\tEWP:21\n",
      "\n",
      "...Early Stopping\n",
      "[CV]  n_neurons=100, n_hidden=13, learning_rate=0.001, dropout_rate=0.2, bn_momentum=0.98, activation_fn=<function relu at 0x112e07d08>, total=  51.4s\n",
      "[CV] n_neurons=200, n_hidden=7, learning_rate=0.005, dropout_rate=0.4, bn_momentum=0.99, activation_fn=<function relu at 0x112e07d08> \n",
      "@E0\tCV Loss: 1.608\tCV Acc: 30.960%\tEWP:00\n",
      "@E26\tCV Loss: 2.772\tCV Acc: 30.460%\tEWP:21\n",
      "\n",
      "...Early Stopping\n",
      "[CV]  n_neurons=200, n_hidden=7, learning_rate=0.005, dropout_rate=0.4, bn_momentum=0.99, activation_fn=<function relu at 0x112e07d08>, total=  32.1s\n",
      "[CV] n_neurons=200, n_hidden=7, learning_rate=0.005, dropout_rate=0.4, bn_momentum=0.99, activation_fn=<function relu at 0x112e07d08> \n",
      "@E0\tCV Loss: 1.608\tCV Acc: 26.980%\tEWP:00\n",
      "@E27\tCV Loss: 2.509\tCV Acc: 33.520%\tEWP:21\n",
      "\n",
      "...Early Stopping\n",
      "[CV]  n_neurons=200, n_hidden=7, learning_rate=0.005, dropout_rate=0.4, bn_momentum=0.99, activation_fn=<function relu at 0x112e07d08>, total=  33.0s\n",
      "[CV] n_neurons=200, n_hidden=7, learning_rate=0.005, dropout_rate=0.4, bn_momentum=0.99, activation_fn=<function relu at 0x112e07d08> \n",
      "@E0\tCV Loss: 1.608\tCV Acc: 31.080%\tEWP:00\n",
      "@E28\tCV Loss: 4.351\tCV Acc: 36.340%\tEWP:21\n",
      "\n",
      "...Early Stopping\n",
      "[CV]  n_neurons=200, n_hidden=7, learning_rate=0.005, dropout_rate=0.4, bn_momentum=0.99, activation_fn=<function relu at 0x112e07d08>, total=  34.3s\n",
      "[CV] n_neurons=250, n_hidden=5, learning_rate=0.005, dropout_rate=0.5, bn_momentum=0.999, activation_fn=<function relu at 0x112e07d08> \n",
      "@E0\tCV Loss: 1.596\tCV Acc: 26.540%\tEWP:00\n",
      "@E24\tCV Loss: 3.735\tCV Acc: 56.080%\tEWP:21\n",
      "\n",
      "...Early Stopping\n",
      "[CV]  n_neurons=250, n_hidden=5, learning_rate=0.005, dropout_rate=0.5, bn_momentum=0.999, activation_fn=<function relu at 0x112e07d08>, total=  29.2s\n",
      "[CV] n_neurons=250, n_hidden=5, learning_rate=0.005, dropout_rate=0.5, bn_momentum=0.999, activation_fn=<function relu at 0x112e07d08> \n",
      "@E0\tCV Loss: 1.602\tCV Acc: 19.380%\tEWP:00\n",
      "@E26\tCV Loss: 1.629\tCV Acc: 55.220%\tEWP:21\n",
      "\n",
      "...Early Stopping\n",
      "[CV]  n_neurons=250, n_hidden=5, learning_rate=0.005, dropout_rate=0.5, bn_momentum=0.999, activation_fn=<function relu at 0x112e07d08>, total=  31.2s\n",
      "[CV] n_neurons=250, n_hidden=5, learning_rate=0.005, dropout_rate=0.5, bn_momentum=0.999, activation_fn=<function relu at 0x112e07d08> \n",
      "@E0\tCV Loss: 1.617\tCV Acc: 19.380%\tEWP:00\n",
      "@E26\tCV Loss: 1.274\tCV Acc: 56.760%\tEWP:21\n",
      "\n",
      "...Early Stopping\n",
      "[CV]  n_neurons=250, n_hidden=5, learning_rate=0.005, dropout_rate=0.5, bn_momentum=0.999, activation_fn=<function relu at 0x112e07d08>, total=  31.2s\n",
      "[CV] n_neurons=200, n_hidden=5, learning_rate=0.1, dropout_rate=0.6, bn_momentum=0.999, activation_fn=<function elu at 0x112deef28> \n",
      "@E0\tCV Loss: 65.904\tCV Acc: 19.380%\tEWP:00\n",
      "@E21\tCV Loss: 35831.852\tCV Acc: 56.540%\tEWP:216\n",
      "\n",
      "...Early Stopping\n",
      "[CV]  n_neurons=200, n_hidden=5, learning_rate=0.1, dropout_rate=0.6, bn_momentum=0.999, activation_fn=<function elu at 0x112deef28>, total=  20.4s\n",
      "[CV] n_neurons=200, n_hidden=5, learning_rate=0.1, dropout_rate=0.6, bn_momentum=0.999, activation_fn=<function elu at 0x112deef28> \n",
      "@E0\tCV Loss: 128.238\tCV Acc: 19.380%\tEWP:00\n",
      "@E21\tCV Loss: 13968.927\tCV Acc: 71.200%\tEWP:215\n",
      "\n",
      "...Early Stopping\n",
      "[CV]  n_neurons=200, n_hidden=5, learning_rate=0.1, dropout_rate=0.6, bn_momentum=0.999, activation_fn=<function elu at 0x112deef28>, total=  20.3s\n",
      "[CV] n_neurons=200, n_hidden=5, learning_rate=0.1, dropout_rate=0.6, bn_momentum=0.999, activation_fn=<function elu at 0x112deef28> \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@E0\tCV Loss: 122.455\tCV Acc: 20.280%\tEWP:00\n",
      "@E21\tCV Loss: 6729.663\tCV Acc: 85.220%\tEWP:2191\n",
      "\n",
      "...Early Stopping\n",
      "[CV]  n_neurons=200, n_hidden=5, learning_rate=0.1, dropout_rate=0.6, bn_momentum=0.999, activation_fn=<function elu at 0x112deef28>, total=  20.8s\n",
      "[CV] n_neurons=300, n_hidden=13, learning_rate=0.1, dropout_rate=0.5, bn_momentum=0.98, activation_fn=<function relu at 0x112e07d08> \n",
      "@E0\tCV Loss: 4401377.500\tCV Acc: 19.380%\tEWP:00\n",
      "@E85\tCV Loss: 1.608\tCV Acc: 22.020%\tEWP:21000006532\n",
      "\n",
      "...Early Stopping\n",
      "[CV]  n_neurons=300, n_hidden=13, learning_rate=0.1, dropout_rate=0.5, bn_momentum=0.98, activation_fn=<function relu at 0x112e07d08>, total=14.4min\n",
      "[CV] n_neurons=300, n_hidden=13, learning_rate=0.1, dropout_rate=0.5, bn_momentum=0.98, activation_fn=<function relu at 0x112e07d08> \n",
      "@E0\tCV Loss: 9224851.000\tCV Acc: 19.380%\tEWP:00\n",
      "@E91\tCV Loss: 1.608\tCV Acc: 22.020%\tEWP:2100000016\n",
      "\n",
      "...Early Stopping\n",
      "[CV]  n_neurons=300, n_hidden=13, learning_rate=0.1, dropout_rate=0.5, bn_momentum=0.98, activation_fn=<function relu at 0x112e07d08>, total= 5.2min\n",
      "[CV] n_neurons=300, n_hidden=13, learning_rate=0.1, dropout_rate=0.5, bn_momentum=0.98, activation_fn=<function relu at 0x112e07d08> \n",
      "@E0\tCV Loss: 7640166.500\tCV Acc: 19.380%\tEWP:00\n",
      "@E74\tCV Loss: 1.608\tCV Acc: 22.020%\tEWP:21000000174\n",
      "\n",
      "...Early Stopping\n",
      "[CV]  n_neurons=300, n_hidden=13, learning_rate=0.1, dropout_rate=0.5, bn_momentum=0.98, activation_fn=<function relu at 0x112e07d08>, total= 4.1min\n",
      "[CV] n_neurons=250, n_hidden=11, learning_rate=0.0001, dropout_rate=0.3, bn_momentum=0.999, activation_fn=<function elu at 0x112deef28> \n",
      "@E0\tCV Loss: 1.598\tCV Acc: 33.920%\tEWP:00\n",
      "@E150\tCV Loss: 0.134\tCV Acc: 96.300%\tEWP:00\n",
      "@E300\tCV Loss: 0.104\tCV Acc: 97.160%\tEWP:00\n",
      "@E450\tCV Loss: 0.087\tCV Acc: 97.900%\tEWP:08\n",
      "@E575\tCV Loss: 0.078\tCV Acc: 98.240%\tEWP:21\n",
      "\n",
      "...Early Stopping\n",
      "[CV]  n_neurons=250, n_hidden=11, learning_rate=0.0001, dropout_rate=0.3, bn_momentum=0.999, activation_fn=<function elu at 0x112deef28>, total=22.6min\n",
      "[CV] n_neurons=250, n_hidden=11, learning_rate=0.0001, dropout_rate=0.3, bn_momentum=0.999, activation_fn=<function elu at 0x112deef28> \n",
      "@E0\tCV Loss: 1.601\tCV Acc: 27.880%\tEWP:00\n",
      "@E150\tCV Loss: 0.134\tCV Acc: 96.360%\tEWP:00\n",
      "@E300\tCV Loss: 0.103\tCV Acc: 96.960%\tEWP:00\n",
      "@E448\tCV Loss: 0.091\tCV Acc: 97.600%\tEWP:21\n",
      "\n",
      "...Early Stopping\n",
      "[CV]  n_neurons=250, n_hidden=11, learning_rate=0.0001, dropout_rate=0.3, bn_momentum=0.999, activation_fn=<function elu at 0x112deef28>, total=17.2min\n",
      "[CV] n_neurons=250, n_hidden=11, learning_rate=0.0001, dropout_rate=0.3, bn_momentum=0.999, activation_fn=<function elu at 0x112deef28> \n",
      "@E0\tCV Loss: 1.594\tCV Acc: 29.820%\tEWP:00\n",
      "@E150\tCV Loss: 0.143\tCV Acc: 96.360%\tEWP:00\n",
      "@E300\tCV Loss: 0.111\tCV Acc: 96.740%\tEWP:15\n",
      "@E418\tCV Loss: 0.101\tCV Acc: 97.120%\tEWP:21\n",
      "\n",
      "...Early Stopping\n",
      "[CV]  n_neurons=250, n_hidden=11, learning_rate=0.0001, dropout_rate=0.3, bn_momentum=0.999, activation_fn=<function elu at 0x112deef28>, total=16.0min\n",
      "[CV] n_neurons=100, n_hidden=7, learning_rate=0.005, dropout_rate=0.3, bn_momentum=0.98, activation_fn=<function relu at 0x112e07d08> \n",
      "@E0\tCV Loss: 1.604\tCV Acc: 24.940%\tEWP:00\n",
      "@E31\tCV Loss: 1.176\tCV Acc: 56.780%\tEWP:21\n",
      "\n",
      "...Early Stopping\n",
      "[CV]  n_neurons=100, n_hidden=7, learning_rate=0.005, dropout_rate=0.3, bn_momentum=0.98, activation_fn=<function relu at 0x112e07d08>, total=  19.1s\n",
      "[CV] n_neurons=100, n_hidden=7, learning_rate=0.005, dropout_rate=0.3, bn_momentum=0.98, activation_fn=<function relu at 0x112e07d08> \n",
      "@E0\tCV Loss: 1.606\tCV Acc: 30.480%\tEWP:00\n",
      "@E31\tCV Loss: 1.390\tCV Acc: 56.220%\tEWP:21\n",
      "\n",
      "...Early Stopping\n",
      "[CV]  n_neurons=100, n_hidden=7, learning_rate=0.005, dropout_rate=0.3, bn_momentum=0.98, activation_fn=<function relu at 0x112e07d08>, total=  18.8s\n",
      "[CV] n_neurons=100, n_hidden=7, learning_rate=0.005, dropout_rate=0.3, bn_momentum=0.98, activation_fn=<function relu at 0x112e07d08> \n",
      "@E0\tCV Loss: 1.610\tCV Acc: 19.160%\tEWP:00\n",
      "@E150\tCV Loss: 0.240\tCV Acc: 96.160%\tEWP:00\n",
      "@E235\tCV Loss: 0.051\tCV Acc: 98.820%\tEWP:21\n",
      "\n",
      "...Early Stopping\n",
      "[CV]  n_neurons=100, n_hidden=7, learning_rate=0.005, dropout_rate=0.3, bn_momentum=0.98, activation_fn=<function relu at 0x112e07d08>, total= 2.1min\n",
      "[CV] n_neurons=200, n_hidden=7, learning_rate=0.005, dropout_rate=0.5, bn_momentum=0.98, activation_fn=<function elu at 0x112deef28> \n",
      "@E0\tCV Loss: 1.393\tCV Acc: 69.340%\tEWP:00\n",
      "@E25\tCV Loss: 0.677\tCV Acc: 96.880%\tEWP:21\n",
      "\n",
      "...Early Stopping\n",
      "[CV]  n_neurons=200, n_hidden=7, learning_rate=0.005, dropout_rate=0.5, bn_momentum=0.98, activation_fn=<function elu at 0x112deef28>, total=  31.2s\n",
      "[CV] n_neurons=200, n_hidden=7, learning_rate=0.005, dropout_rate=0.5, bn_momentum=0.98, activation_fn=<function elu at 0x112deef28> \n",
      "@E0\tCV Loss: 1.433\tCV Acc: 50.640%\tEWP:00\n",
      "@E25\tCV Loss: 0.711\tCV Acc: 96.980%\tEWP:21\n",
      "\n",
      "...Early Stopping\n",
      "[CV]  n_neurons=200, n_hidden=7, learning_rate=0.005, dropout_rate=0.5, bn_momentum=0.98, activation_fn=<function elu at 0x112deef28>, total=  32.8s\n",
      "[CV] n_neurons=200, n_hidden=7, learning_rate=0.005, dropout_rate=0.5, bn_momentum=0.98, activation_fn=<function elu at 0x112deef28> \n",
      "@E0\tCV Loss: 1.465\tCV Acc: 41.460%\tEWP:00\n",
      "@E27\tCV Loss: 0.672\tCV Acc: 96.560%\tEWP:21\n",
      "\n",
      "...Early Stopping\n",
      "[CV]  n_neurons=200, n_hidden=7, learning_rate=0.005, dropout_rate=0.5, bn_momentum=0.98, activation_fn=<function elu at 0x112deef28>, total=  36.8s\n",
      "[CV] n_neurons=200, n_hidden=11, learning_rate=0.001, dropout_rate=0.4, bn_momentum=0.98, activation_fn=<function relu at 0x112e07d08> \n",
      "@E0\tCV Loss: 1.609\tCV Acc: 19.380%\tEWP:00\n",
      "@E51\tCV Loss: 1.609\tCV Acc: 20.140%\tEWP:21\n",
      "\n",
      "...Early Stopping\n",
      "[CV]  n_neurons=200, n_hidden=11, learning_rate=0.001, dropout_rate=0.4, bn_momentum=0.98, activation_fn=<function relu at 0x112e07d08>, total= 1.6min\n",
      "[CV] n_neurons=200, n_hidden=11, learning_rate=0.001, dropout_rate=0.4, bn_momentum=0.98, activation_fn=<function relu at 0x112e07d08> \n",
      "@E0\tCV Loss: 1.609\tCV Acc: 20.140%\tEWP:00\n",
      "@E46\tCV Loss: 1.610\tCV Acc: 20.140%\tEWP:21\n",
      "\n",
      "...Early Stopping\n",
      "[CV]  n_neurons=200, n_hidden=11, learning_rate=0.001, dropout_rate=0.4, bn_momentum=0.98, activation_fn=<function relu at 0x112e07d08>, total= 1.4min\n",
      "[CV] n_neurons=200, n_hidden=11, learning_rate=0.001, dropout_rate=0.4, bn_momentum=0.98, activation_fn=<function relu at 0x112e07d08> \n",
      "@E0\tCV Loss: 1.609\tCV Acc: 20.140%\tEWP:00\n",
      "@E58\tCV Loss: 1.609\tCV Acc: 20.140%\tEWP:21\n",
      "\n",
      "...Early Stopping\n",
      "[CV]  n_neurons=200, n_hidden=11, learning_rate=0.001, dropout_rate=0.4, bn_momentum=0.98, activation_fn=<function relu at 0x112e07d08>, total= 1.7min\n",
      "[CV] n_neurons=250, n_hidden=13, learning_rate=0.1, dropout_rate=0.5, bn_momentum=0.999, activation_fn=<function relu at 0x112e07d08> \n",
      "@E0\tCV Loss: 681954.812\tCV Acc: 19.380%\tEWP:00\n",
      "@E21\tCV Loss: 37286656.000\tCV Acc: 22.020%\tEWP:21841\n",
      "\n",
      "...Early Stopping\n",
      "[CV]  n_neurons=250, n_hidden=13, learning_rate=0.1, dropout_rate=0.5, bn_momentum=0.999, activation_fn=<function relu at 0x112e07d08>, total= 1.1min\n",
      "[CV] n_neurons=250, n_hidden=13, learning_rate=0.1, dropout_rate=0.5, bn_momentum=0.999, activation_fn=<function relu at 0x112e07d08> \n",
      "@E0\tCV Loss: 720216.500\tCV Acc: 19.380%\tEWP:00\n",
      "@E21\tCV Loss: 590985536.000\tCV Acc: 19.220%\tEWP:2104\n",
      "\n",
      "...Early Stopping\n",
      "[CV]  n_neurons=250, n_hidden=13, learning_rate=0.1, dropout_rate=0.5, bn_momentum=0.999, activation_fn=<function relu at 0x112e07d08>, total= 1.0min\n",
      "[CV] n_neurons=250, n_hidden=13, learning_rate=0.1, dropout_rate=0.5, bn_momentum=0.999, activation_fn=<function relu at 0x112e07d08> \n",
      "@E0\tCV Loss: 865055.438\tCV Acc: 19.380%\tEWP:00\n",
      "@E21\tCV Loss: 1105370240.000\tCV Acc: 22.020%\tEWP:2173\n",
      "\n",
      "...Early Stopping\n",
      "[CV]  n_neurons=250, n_hidden=13, learning_rate=0.1, dropout_rate=0.5, bn_momentum=0.999, activation_fn=<function relu at 0x112e07d08>, total= 1.0min\n",
      "[CV] n_neurons=50, n_hidden=11, learning_rate=0.1, dropout_rate=0.4, bn_momentum=0.999, activation_fn=<function relu at 0x112e07d08> \n",
      "@E0\tCV Loss: 8.100\tCV Acc: 19.380%\tEWP:00\n",
      "@E21\tCV Loss: 327234976.000\tCV Acc: 19.380%\tEWP:21\n",
      "\n",
      "...Early Stopping\n",
      "[CV]  n_neurons=50, n_hidden=11, learning_rate=0.1, dropout_rate=0.4, bn_momentum=0.999, activation_fn=<function relu at 0x112e07d08>, total=  13.6s\n",
      "[CV] n_neurons=50, n_hidden=11, learning_rate=0.1, dropout_rate=0.4, bn_momentum=0.999, activation_fn=<function relu at 0x112e07d08> \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@E0\tCV Loss: 7.626\tCV Acc: 19.380%\tEWP:00\n",
      "@E21\tCV Loss: 231254.344\tCV Acc: 20.140%\tEWP:21\n",
      "\n",
      "...Early Stopping\n",
      "[CV]  n_neurons=50, n_hidden=11, learning_rate=0.1, dropout_rate=0.4, bn_momentum=0.999, activation_fn=<function relu at 0x112e07d08>, total=  14.0s\n",
      "[CV] n_neurons=50, n_hidden=11, learning_rate=0.1, dropout_rate=0.4, bn_momentum=0.999, activation_fn=<function relu at 0x112e07d08> \n",
      "@E0\tCV Loss: 4.658\tCV Acc: 19.380%\tEWP:00\n",
      "@E22\tCV Loss: 666555.875\tCV Acc: 19.380%\tEWP:21\n",
      "\n",
      "...Early Stopping\n",
      "[CV]  n_neurons=50, n_hidden=11, learning_rate=0.1, dropout_rate=0.4, bn_momentum=0.999, activation_fn=<function relu at 0x112e07d08>, total=  15.6s\n",
      "[CV] n_neurons=100, n_hidden=5, learning_rate=0.005, dropout_rate=0.6, bn_momentum=0.999, activation_fn=<function relu at 0x112e07d08> \n",
      "@E0\tCV Loss: 1.608\tCV Acc: 21.920%\tEWP:00\n",
      "@E29\tCV Loss: 4.582\tCV Acc: 37.840%\tEWP:21\n",
      "\n",
      "...Early Stopping\n",
      "[CV]  n_neurons=100, n_hidden=5, learning_rate=0.005, dropout_rate=0.6, bn_momentum=0.999, activation_fn=<function relu at 0x112e07d08>, total=  16.4s\n",
      "[CV] n_neurons=100, n_hidden=5, learning_rate=0.005, dropout_rate=0.6, bn_momentum=0.999, activation_fn=<function relu at 0x112e07d08> \n",
      "@E0\tCV Loss: 1.607\tCV Acc: 23.600%\tEWP:00\n",
      "@E33\tCV Loss: 5.604\tCV Acc: 41.660%\tEWP:21\n",
      "\n",
      "...Early Stopping\n",
      "[CV]  n_neurons=100, n_hidden=5, learning_rate=0.005, dropout_rate=0.6, bn_momentum=0.999, activation_fn=<function relu at 0x112e07d08>, total=  18.3s\n",
      "[CV] n_neurons=100, n_hidden=5, learning_rate=0.005, dropout_rate=0.6, bn_momentum=0.999, activation_fn=<function relu at 0x112e07d08> \n",
      "@E0\tCV Loss: 1.603\tCV Acc: 20.700%\tEWP:00\n",
      "@E28\tCV Loss: 8.604\tCV Acc: 19.380%\tEWP:21\n",
      "\n",
      "...Early Stopping\n",
      "[CV]  n_neurons=100, n_hidden=5, learning_rate=0.005, dropout_rate=0.6, bn_momentum=0.999, activation_fn=<function relu at 0x112e07d08>, total=  15.2s\n",
      "[CV] n_neurons=50, n_hidden=7, learning_rate=0.005, dropout_rate=0.4, bn_momentum=0.95, activation_fn=<function relu at 0x112e07d08> \n",
      "@E0\tCV Loss: 1.609\tCV Acc: 19.200%\tEWP:00\n",
      "@E150\tCV Loss: 0.456\tCV Acc: 95.540%\tEWP:00\n",
      "@E264\tCV Loss: 0.109\tCV Acc: 97.720%\tEWP:21\n",
      "\n",
      "...Early Stopping\n",
      "[CV]  n_neurons=50, n_hidden=7, learning_rate=0.005, dropout_rate=0.4, bn_momentum=0.95, activation_fn=<function relu at 0x112e07d08>, total= 1.6min\n",
      "[CV] n_neurons=50, n_hidden=7, learning_rate=0.005, dropout_rate=0.4, bn_momentum=0.95, activation_fn=<function relu at 0x112e07d08> \n",
      "@E0\tCV Loss: 1.610\tCV Acc: 19.240%\tEWP:00\n",
      "@E48\tCV Loss: 2.022\tCV Acc: 38.400%\tEWP:21\n",
      "\n",
      "...Early Stopping\n",
      "[CV]  n_neurons=50, n_hidden=7, learning_rate=0.005, dropout_rate=0.4, bn_momentum=0.95, activation_fn=<function relu at 0x112e07d08>, total=  21.1s\n",
      "[CV] n_neurons=50, n_hidden=7, learning_rate=0.005, dropout_rate=0.4, bn_momentum=0.95, activation_fn=<function relu at 0x112e07d08> \n",
      "@E0\tCV Loss: 1.610\tCV Acc: 19.940%\tEWP:00\n",
      "@E51\tCV Loss: 0.993\tCV Acc: 60.480%\tEWP:21\n",
      "\n",
      "...Early Stopping\n",
      "[CV]  n_neurons=50, n_hidden=7, learning_rate=0.005, dropout_rate=0.4, bn_momentum=0.95, activation_fn=<function relu at 0x112e07d08>, total=  21.3s\n",
      "[CV] n_neurons=250, n_hidden=11, learning_rate=0.1, dropout_rate=0.3, bn_momentum=0.99, activation_fn=<function elu at 0x112deef28> \n",
      "@E0\tCV Loss: 3451.981\tCV Acc: 19.220%\tEWP:00\n",
      "@E21\tCV Loss: 114200.578\tCV Acc: 53.440%\tEWP:216263\n",
      "\n",
      "...Early Stopping\n",
      "[CV]  n_neurons=250, n_hidden=11, learning_rate=0.1, dropout_rate=0.3, bn_momentum=0.99, activation_fn=<function elu at 0x112deef28>, total=  55.5s\n",
      "[CV] n_neurons=250, n_hidden=11, learning_rate=0.1, dropout_rate=0.3, bn_momentum=0.99, activation_fn=<function elu at 0x112deef28> \n",
      "@E0\tCV Loss: 4038.290\tCV Acc: 20.140%\tEWP:00\n",
      "@E21\tCV Loss: 104106.273\tCV Acc: 58.940%\tEWP:216203\n",
      "\n",
      "...Early Stopping\n",
      "[CV]  n_neurons=250, n_hidden=11, learning_rate=0.1, dropout_rate=0.3, bn_momentum=0.99, activation_fn=<function elu at 0x112deef28>, total=  55.5s\n",
      "[CV] n_neurons=250, n_hidden=11, learning_rate=0.1, dropout_rate=0.3, bn_momentum=0.99, activation_fn=<function elu at 0x112deef28> \n",
      "@E0\tCV Loss: 3492.750\tCV Acc: 20.140%\tEWP:00\n",
      "@E21\tCV Loss: 135027.781\tCV Acc: 59.400%\tEWP:214275\n",
      "\n",
      "...Early Stopping\n",
      "[CV]  n_neurons=250, n_hidden=11, learning_rate=0.1, dropout_rate=0.3, bn_momentum=0.99, activation_fn=<function elu at 0x112deef28>, total= 1.0min\n",
      "[CV] n_neurons=200, n_hidden=7, learning_rate=0.005, dropout_rate=0.5, bn_momentum=0.99, activation_fn=<function elu at 0x112deef28> \n",
      "@E0\tCV Loss: 1.456\tCV Acc: 46.580%\tEWP:00\n",
      "@E25\tCV Loss: 1.317\tCV Acc: 96.140%\tEWP:21\n",
      "\n",
      "...Early Stopping\n",
      "[CV]  n_neurons=200, n_hidden=7, learning_rate=0.005, dropout_rate=0.5, bn_momentum=0.99, activation_fn=<function elu at 0x112deef28>, total=  33.6s\n",
      "[CV] n_neurons=200, n_hidden=7, learning_rate=0.005, dropout_rate=0.5, bn_momentum=0.99, activation_fn=<function elu at 0x112deef28> \n",
      "@E0\tCV Loss: 1.426\tCV Acc: 44.580%\tEWP:00\n",
      "@E25\tCV Loss: 1.588\tCV Acc: 96.120%\tEWP:21\n",
      "\n",
      "...Early Stopping\n",
      "[CV]  n_neurons=200, n_hidden=7, learning_rate=0.005, dropout_rate=0.5, bn_momentum=0.99, activation_fn=<function elu at 0x112deef28>, total=  33.8s\n",
      "[CV] n_neurons=200, n_hidden=7, learning_rate=0.005, dropout_rate=0.5, bn_momentum=0.99, activation_fn=<function elu at 0x112deef28> \n",
      "@E0\tCV Loss: 1.451\tCV Acc: 61.320%\tEWP:00\n",
      "@E27\tCV Loss: 1.506\tCV Acc: 95.580%\tEWP:21\n",
      "\n",
      "...Early Stopping\n",
      "[CV]  n_neurons=200, n_hidden=7, learning_rate=0.005, dropout_rate=0.5, bn_momentum=0.99, activation_fn=<function elu at 0x112deef28>, total=  33.5s\n",
      "[CV] n_neurons=250, n_hidden=7, learning_rate=0.001, dropout_rate=0.4, bn_momentum=0.999, activation_fn=<function elu at 0x112deef28> \n",
      "@E0\tCV Loss: 1.466\tCV Acc: 67.380%\tEWP:00\n",
      "@E49\tCV Loss: 0.343\tCV Acc: 93.620%\tEWP:21\n",
      "\n",
      "...Early Stopping\n",
      "[CV]  n_neurons=250, n_hidden=7, learning_rate=0.001, dropout_rate=0.4, bn_momentum=0.999, activation_fn=<function elu at 0x112deef28>, total= 1.3min\n",
      "[CV] n_neurons=250, n_hidden=7, learning_rate=0.001, dropout_rate=0.4, bn_momentum=0.999, activation_fn=<function elu at 0x112deef28> \n",
      "@E0\tCV Loss: 1.470\tCV Acc: 79.520%\tEWP:00\n",
      "@E42\tCV Loss: 0.286\tCV Acc: 95.140%\tEWP:21\n",
      "\n",
      "...Early Stopping\n",
      "[CV]  n_neurons=250, n_hidden=7, learning_rate=0.001, dropout_rate=0.4, bn_momentum=0.999, activation_fn=<function elu at 0x112deef28>, total= 1.2min\n",
      "[CV] n_neurons=250, n_hidden=7, learning_rate=0.001, dropout_rate=0.4, bn_momentum=0.999, activation_fn=<function elu at 0x112deef28> \n",
      "@E0\tCV Loss: 1.470\tCV Acc: 80.100%\tEWP:00\n",
      "@E41\tCV Loss: 0.261\tCV Acc: 95.720%\tEWP:21\n",
      "\n",
      "...Early Stopping\n",
      "[CV]  n_neurons=250, n_hidden=7, learning_rate=0.001, dropout_rate=0.4, bn_momentum=0.999, activation_fn=<function elu at 0x112deef28>, total= 1.1min\n",
      "[CV] n_neurons=200, n_hidden=11, learning_rate=0.1, dropout_rate=0.3, bn_momentum=0.999, activation_fn=<function relu at 0x112e07d08> \n",
      "@E0\tCV Loss: 248068.359\tCV Acc: 19.380%\tEWP:00\n",
      "@E21\tCV Loss: 20961860.000\tCV Acc: 22.020%\tEWP:2150\n",
      "\n",
      "...Early Stopping\n",
      "[CV]  n_neurons=200, n_hidden=11, learning_rate=0.1, dropout_rate=0.3, bn_momentum=0.999, activation_fn=<function relu at 0x112e07d08>, total=  40.3s\n",
      "[CV] n_neurons=200, n_hidden=11, learning_rate=0.1, dropout_rate=0.3, bn_momentum=0.999, activation_fn=<function relu at 0x112e07d08> \n",
      "@E0\tCV Loss: 104271.648\tCV Acc: 19.220%\tEWP:00\n",
      "@E21\tCV Loss: 713174.500\tCV Acc: 22.020%\tEWP:219410\n",
      "\n",
      "...Early Stopping\n",
      "[CV]  n_neurons=200, n_hidden=11, learning_rate=0.1, dropout_rate=0.3, bn_momentum=0.999, activation_fn=<function relu at 0x112e07d08>, total=  40.5s\n",
      "[CV] n_neurons=200, n_hidden=11, learning_rate=0.1, dropout_rate=0.3, bn_momentum=0.999, activation_fn=<function relu at 0x112e07d08> \n",
      "@E0\tCV Loss: 72833.398\tCV Acc: 20.140%\tEWP:00\n",
      "@E21\tCV Loss: 2343130.000\tCV Acc: 20.140%\tEWP:2172\n",
      "\n",
      "...Early Stopping\n",
      "[CV]  n_neurons=200, n_hidden=11, learning_rate=0.1, dropout_rate=0.3, bn_momentum=0.999, activation_fn=<function relu at 0x112e07d08>, total=  40.2s\n",
      "[CV] n_neurons=300, n_hidden=13, learning_rate=0.1, dropout_rate=0.6, bn_momentum=0.999, activation_fn=<function elu at 0x112deef28> \n",
      "@E0\tCV Loss: 24010.049\tCV Acc: 19.220%\tEWP:00\n",
      "@E21\tCV Loss: 27252838367232.000\tCV Acc: 19.380%\tEWP:217\n",
      "\n",
      "...Early Stopping\n",
      "[CV]  n_neurons=300, n_hidden=13, learning_rate=0.1, dropout_rate=0.6, bn_momentum=0.999, activation_fn=<function elu at 0x112deef28>, total= 1.3min\n",
      "[CV] n_neurons=300, n_hidden=13, learning_rate=0.1, dropout_rate=0.6, bn_momentum=0.999, activation_fn=<function elu at 0x112deef28> \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@E0\tCV Loss: 19827.926\tCV Acc: 19.380%\tEWP:00\n",
      "@E21\tCV Loss: 4004179869696.000\tCV Acc: 19.380%\tEWP:2181\n",
      "\n",
      "...Early Stopping\n",
      "[CV]  n_neurons=300, n_hidden=13, learning_rate=0.1, dropout_rate=0.6, bn_momentum=0.999, activation_fn=<function elu at 0x112deef28>, total= 1.3min\n",
      "[CV] n_neurons=300, n_hidden=13, learning_rate=0.1, dropout_rate=0.6, bn_momentum=0.999, activation_fn=<function elu at 0x112deef28> \n",
      "@E0\tCV Loss: 15752.604\tCV Acc: 19.380%\tEWP:00\n",
      "@E21\tCV Loss: 2925396492288.000\tCV Acc: 19.380%\tEWP:2170\n",
      "\n",
      "...Early Stopping\n",
      "[CV]  n_neurons=300, n_hidden=13, learning_rate=0.1, dropout_rate=0.6, bn_momentum=0.999, activation_fn=<function elu at 0x112deef28>, total= 1.3min\n",
      "[CV] n_neurons=100, n_hidden=11, learning_rate=0.0001, dropout_rate=0.3, bn_momentum=0.999, activation_fn=<function elu at 0x112deef28> \n",
      "@E0\tCV Loss: 1.606\tCV Acc: 17.300%\tEWP:00\n",
      "@E150\tCV Loss: 0.322\tCV Acc: 93.740%\tEWP:00\n",
      "@E300\tCV Loss: 0.140\tCV Acc: 95.880%\tEWP:00\n",
      "@E450\tCV Loss: 0.128\tCV Acc: 96.560%\tEWP:00\n",
      "@E532\tCV Loss: 0.125\tCV Acc: 96.780%\tEWP:21\n",
      "\n",
      "...Early Stopping\n",
      "[CV]  n_neurons=100, n_hidden=11, learning_rate=0.0001, dropout_rate=0.3, bn_momentum=0.999, activation_fn=<function elu at 0x112deef28>, total= 7.3min\n",
      "[CV] n_neurons=100, n_hidden=11, learning_rate=0.0001, dropout_rate=0.3, bn_momentum=0.999, activation_fn=<function elu at 0x112deef28> \n",
      "@E0\tCV Loss: 1.611\tCV Acc: 24.960%\tEWP:00\n",
      "@E150\tCV Loss: 0.308\tCV Acc: 92.440%\tEWP:00\n",
      "@E300\tCV Loss: 0.141\tCV Acc: 95.920%\tEWP:00\n",
      "@E388\tCV Loss: 0.135\tCV Acc: 96.460%\tEWP:21\n",
      "\n",
      "...Early Stopping\n",
      "[CV]  n_neurons=100, n_hidden=11, learning_rate=0.0001, dropout_rate=0.3, bn_momentum=0.999, activation_fn=<function elu at 0x112deef28>, total= 5.2min\n",
      "[CV] n_neurons=100, n_hidden=11, learning_rate=0.0001, dropout_rate=0.3, bn_momentum=0.999, activation_fn=<function elu at 0x112deef28> \n",
      "@E0\tCV Loss: 1.604\tCV Acc: 27.000%\tEWP:00\n",
      "@E150\tCV Loss: 0.279\tCV Acc: 93.720%\tEWP:00\n",
      "@E300\tCV Loss: 0.148\tCV Acc: 95.880%\tEWP:00\n",
      "@E401\tCV Loss: 0.135\tCV Acc: 96.660%\tEWP:21\n",
      "\n",
      "...Early Stopping\n",
      "[CV]  n_neurons=100, n_hidden=11, learning_rate=0.0001, dropout_rate=0.3, bn_momentum=0.999, activation_fn=<function elu at 0x112deef28>, total= 5.2min\n",
      "[CV] n_neurons=300, n_hidden=11, learning_rate=0.005, dropout_rate=0.5, bn_momentum=0.99, activation_fn=<function relu at 0x112e07d08> \n",
      "@E0\tCV Loss: 1.611\tCV Acc: 19.380%\tEWP:00\n",
      "@E93\tCV Loss: 1.285\tCV Acc: 38.660%\tEWP:21\n",
      "\n",
      "...Early Stopping\n",
      "[CV]  n_neurons=300, n_hidden=11, learning_rate=0.005, dropout_rate=0.5, bn_momentum=0.99, activation_fn=<function relu at 0x112e07d08>, total= 4.3min\n",
      "[CV] n_neurons=300, n_hidden=11, learning_rate=0.005, dropout_rate=0.5, bn_momentum=0.99, activation_fn=<function relu at 0x112e07d08> \n",
      "@E0\tCV Loss: 1.611\tCV Acc: 19.380%\tEWP:00\n",
      "@E46\tCV Loss: 1.477\tCV Acc: 36.960%\tEWP:21\n",
      "\n",
      "...Early Stopping\n",
      "[CV]  n_neurons=300, n_hidden=11, learning_rate=0.005, dropout_rate=0.5, bn_momentum=0.99, activation_fn=<function relu at 0x112e07d08>, total= 2.2min\n",
      "[CV] n_neurons=300, n_hidden=11, learning_rate=0.005, dropout_rate=0.5, bn_momentum=0.99, activation_fn=<function relu at 0x112e07d08> \n",
      "@E0\tCV Loss: 1.612\tCV Acc: 19.380%\tEWP:00\n",
      "@E105\tCV Loss: 2.490\tCV Acc: 38.420%\tEWP:21\n",
      "\n",
      "...Early Stopping\n",
      "[CV]  n_neurons=300, n_hidden=11, learning_rate=0.005, dropout_rate=0.5, bn_momentum=0.99, activation_fn=<function relu at 0x112e07d08>, total= 4.9min\n",
      "[CV] n_neurons=250, n_hidden=5, learning_rate=0.001, dropout_rate=0.3, bn_momentum=0.99, activation_fn=<function relu at 0x112e07d08> \n",
      "@E0\tCV Loss: 1.597\tCV Acc: 29.180%\tEWP:00\n",
      "@E39\tCV Loss: 1.272\tCV Acc: 87.420%\tEWP:21\n",
      "\n",
      "...Early Stopping\n",
      "[CV]  n_neurons=250, n_hidden=5, learning_rate=0.001, dropout_rate=0.3, bn_momentum=0.99, activation_fn=<function relu at 0x112e07d08>, total=  42.4s\n",
      "[CV] n_neurons=250, n_hidden=5, learning_rate=0.001, dropout_rate=0.3, bn_momentum=0.99, activation_fn=<function relu at 0x112e07d08> \n",
      "@E0\tCV Loss: 1.598\tCV Acc: 24.480%\tEWP:00\n",
      "@E40\tCV Loss: 1.204\tCV Acc: 90.840%\tEWP:21\n",
      "\n",
      "...Early Stopping\n",
      "[CV]  n_neurons=250, n_hidden=5, learning_rate=0.001, dropout_rate=0.3, bn_momentum=0.99, activation_fn=<function relu at 0x112e07d08>, total=  45.4s\n",
      "[CV] n_neurons=250, n_hidden=5, learning_rate=0.001, dropout_rate=0.3, bn_momentum=0.99, activation_fn=<function relu at 0x112e07d08> \n",
      "@E0\tCV Loss: 1.598\tCV Acc: 27.000%\tEWP:00\n",
      "@E40\tCV Loss: 1.248\tCV Acc: 89.660%\tEWP:21\n",
      "\n",
      "...Early Stopping\n",
      "[CV]  n_neurons=250, n_hidden=5, learning_rate=0.001, dropout_rate=0.3, bn_momentum=0.99, activation_fn=<function relu at 0x112e07d08>, total=  45.9s\n",
      "[CV] n_neurons=100, n_hidden=13, learning_rate=0.005, dropout_rate=0.4, bn_momentum=0.99, activation_fn=<function elu at 0x112deef28> \n",
      "@E0\tCV Loss: 1.593\tCV Acc: 22.200%\tEWP:00\n",
      "@E24\tCV Loss: 26.820\tCV Acc: 40.880%\tEWP:21\n",
      "\n",
      "...Early Stopping\n",
      "[CV]  n_neurons=100, n_hidden=13, learning_rate=0.005, dropout_rate=0.4, bn_momentum=0.99, activation_fn=<function elu at 0x112deef28>, total=  26.9s\n",
      "[CV] n_neurons=100, n_hidden=13, learning_rate=0.005, dropout_rate=0.4, bn_momentum=0.99, activation_fn=<function elu at 0x112deef28> \n",
      "@E0\tCV Loss: 1.597\tCV Acc: 22.020%\tEWP:00\n",
      "@E26\tCV Loss: 3.112\tCV Acc: 95.040%\tEWP:21\n",
      "\n",
      "...Early Stopping\n",
      "[CV]  n_neurons=100, n_hidden=13, learning_rate=0.005, dropout_rate=0.4, bn_momentum=0.99, activation_fn=<function elu at 0x112deef28>, total=  27.6s\n",
      "[CV] n_neurons=100, n_hidden=13, learning_rate=0.005, dropout_rate=0.4, bn_momentum=0.99, activation_fn=<function elu at 0x112deef28> \n",
      "@E0\tCV Loss: 1.595\tCV Acc: 24.200%\tEWP:00\n",
      "@E25\tCV Loss: 4.966\tCV Acc: 78.840%\tEWP:215\n",
      "\n",
      "...Early Stopping\n",
      "[CV]  n_neurons=100, n_hidden=13, learning_rate=0.005, dropout_rate=0.4, bn_momentum=0.99, activation_fn=<function elu at 0x112deef28>, total=  28.5s\n",
      "[CV] n_neurons=50, n_hidden=13, learning_rate=0.1, dropout_rate=0.4, bn_momentum=0.999, activation_fn=<function relu at 0x112e07d08> \n",
      "@E0\tCV Loss: 3.733\tCV Acc: 20.140%\tEWP:00\n",
      "@E21\tCV Loss: 5444378.500\tCV Acc: 19.380%\tEWP:21\n",
      "\n",
      "...Early Stopping\n",
      "[CV]  n_neurons=50, n_hidden=13, learning_rate=0.1, dropout_rate=0.4, bn_momentum=0.999, activation_fn=<function relu at 0x112e07d08>, total=  15.5s\n",
      "[CV] n_neurons=50, n_hidden=13, learning_rate=0.1, dropout_rate=0.4, bn_momentum=0.999, activation_fn=<function relu at 0x112e07d08> \n",
      "@E0\tCV Loss: 4.453\tCV Acc: 19.380%\tEWP:00\n",
      "@E21\tCV Loss: 425953.969\tCV Acc: 19.380%\tEWP:21\n",
      "\n",
      "...Early Stopping\n",
      "[CV]  n_neurons=50, n_hidden=13, learning_rate=0.1, dropout_rate=0.4, bn_momentum=0.999, activation_fn=<function relu at 0x112e07d08>, total=  14.9s\n",
      "[CV] n_neurons=50, n_hidden=13, learning_rate=0.1, dropout_rate=0.4, bn_momentum=0.999, activation_fn=<function relu at 0x112e07d08> \n",
      "@E0\tCV Loss: 2.473\tCV Acc: 19.380%\tEWP:00\n",
      "@E21\tCV Loss: 13252138.000\tCV Acc: 19.380%\tEWP:21\n",
      "\n",
      "...Early Stopping\n",
      "[CV]  n_neurons=50, n_hidden=13, learning_rate=0.1, dropout_rate=0.4, bn_momentum=0.999, activation_fn=<function relu at 0x112e07d08>, total=  15.4s\n",
      "[CV] n_neurons=200, n_hidden=11, learning_rate=0.005, dropout_rate=0.3, bn_momentum=0.99, activation_fn=<function elu at 0x112deef28> \n",
      "@E0\tCV Loss: 1.590\tCV Acc: 21.260%\tEWP:00\n",
      "@E24\tCV Loss: 2.929\tCV Acc: 94.880%\tEWP:21\n",
      "\n",
      "...Early Stopping\n",
      "[CV]  n_neurons=200, n_hidden=11, learning_rate=0.005, dropout_rate=0.3, bn_momentum=0.99, activation_fn=<function elu at 0x112deef28>, total=  45.5s\n",
      "[CV] n_neurons=200, n_hidden=11, learning_rate=0.005, dropout_rate=0.3, bn_momentum=0.99, activation_fn=<function elu at 0x112deef28> \n",
      "@E0\tCV Loss: 1.595\tCV Acc: 29.180%\tEWP:00\n",
      "@E24\tCV Loss: 1.795\tCV Acc: 96.820%\tEWP:21\n",
      "\n",
      "...Early Stopping\n",
      "[CV]  n_neurons=200, n_hidden=11, learning_rate=0.005, dropout_rate=0.3, bn_momentum=0.99, activation_fn=<function elu at 0x112deef28>, total=  45.8s\n",
      "[CV] n_neurons=200, n_hidden=11, learning_rate=0.005, dropout_rate=0.3, bn_momentum=0.99, activation_fn=<function elu at 0x112deef28> \n",
      "@E0\tCV Loss: 1.628\tCV Acc: 7.820%\tEWP:00\n",
      "@E25\tCV Loss: 3.987\tCV Acc: 94.640%\tEWP:21\n",
      "\n",
      "...Early Stopping\n",
      "[CV]  n_neurons=200, n_hidden=11, learning_rate=0.005, dropout_rate=0.3, bn_momentum=0.99, activation_fn=<function elu at 0x112deef28>, total=  46.5s\n",
      "[CV] n_neurons=50, n_hidden=7, learning_rate=0.001, dropout_rate=0.5, bn_momentum=0.99, activation_fn=<function elu at 0x112deef28> \n",
      "@E0\tCV Loss: 1.606\tCV Acc: 22.200%\tEWP:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@E150\tCV Loss: 0.172\tCV Acc: 96.500%\tEWP:00\n",
      "@E300\tCV Loss: 0.117\tCV Acc: 97.500%\tEWP:00\n",
      "@E450\tCV Loss: 0.099\tCV Acc: 97.760%\tEWP:10\n",
      "@E483\tCV Loss: 0.098\tCV Acc: 97.880%\tEWP:21\n",
      "\n",
      "...Early Stopping\n",
      "[CV]  n_neurons=50, n_hidden=7, learning_rate=0.001, dropout_rate=0.5, bn_momentum=0.99, activation_fn=<function elu at 0x112deef28>, total= 2.9min\n",
      "[CV] n_neurons=50, n_hidden=7, learning_rate=0.001, dropout_rate=0.5, bn_momentum=0.99, activation_fn=<function elu at 0x112deef28> \n",
      "@E0\tCV Loss: 1.568\tCV Acc: 36.040%\tEWP:00\n",
      "@E90\tCV Loss: 0.193\tCV Acc: 95.880%\tEWP:21\n",
      "\n",
      "...Early Stopping\n",
      "[CV]  n_neurons=50, n_hidden=7, learning_rate=0.001, dropout_rate=0.5, bn_momentum=0.99, activation_fn=<function elu at 0x112deef28>, total=  36.9s\n",
      "[CV] n_neurons=50, n_hidden=7, learning_rate=0.001, dropout_rate=0.5, bn_momentum=0.99, activation_fn=<function elu at 0x112deef28> \n",
      "@E0\tCV Loss: 1.595\tCV Acc: 29.200%\tEWP:00\n",
      "@E150\tCV Loss: 0.148\tCV Acc: 97.000%\tEWP:00\n",
      "@E270\tCV Loss: 0.118\tCV Acc: 97.440%\tEWP:21\n",
      "\n",
      "...Early Stopping\n",
      "[CV]  n_neurons=50, n_hidden=7, learning_rate=0.001, dropout_rate=0.5, bn_momentum=0.99, activation_fn=<function elu at 0x112deef28>, total= 1.7min\n",
      "[CV] n_neurons=100, n_hidden=11, learning_rate=0.0001, dropout_rate=0.6, bn_momentum=0.999, activation_fn=<function elu at 0x112deef28> \n",
      "@E0\tCV Loss: 1.613\tCV Acc: 12.560%\tEWP:00\n",
      "@E133\tCV Loss: 1.603\tCV Acc: 22.020%\tEWP:21\n",
      "\n",
      "...Early Stopping\n",
      "[CV]  n_neurons=100, n_hidden=11, learning_rate=0.0001, dropout_rate=0.6, bn_momentum=0.999, activation_fn=<function elu at 0x112deef28>, total= 1.9min\n",
      "[CV] n_neurons=100, n_hidden=11, learning_rate=0.0001, dropout_rate=0.6, bn_momentum=0.999, activation_fn=<function elu at 0x112deef28> \n",
      "@E0\tCV Loss: 1.605\tCV Acc: 26.360%\tEWP:00\n",
      "@E150\tCV Loss: 1.575\tCV Acc: 22.020%\tEWP:00\n",
      "@E300\tCV Loss: 1.240\tCV Acc: 40.260%\tEWP:00\n",
      "@E331\tCV Loss: 1.242\tCV Acc: 40.340%\tEWP:21\n",
      "\n",
      "...Early Stopping\n",
      "[CV]  n_neurons=100, n_hidden=11, learning_rate=0.0001, dropout_rate=0.6, bn_momentum=0.999, activation_fn=<function elu at 0x112deef28>, total= 4.6min\n",
      "[CV] n_neurons=100, n_hidden=11, learning_rate=0.0001, dropout_rate=0.6, bn_momentum=0.999, activation_fn=<function elu at 0x112deef28> \n",
      "@E0\tCV Loss: 1.608\tCV Acc: 19.400%\tEWP:00\n",
      "@E85\tCV Loss: 1.603\tCV Acc: 22.020%\tEWP:21\n",
      "\n",
      "...Early Stopping\n",
      "[CV]  n_neurons=100, n_hidden=11, learning_rate=0.0001, dropout_rate=0.6, bn_momentum=0.999, activation_fn=<function elu at 0x112deef28>, total= 1.2min\n",
      "[CV] n_neurons=300, n_hidden=7, learning_rate=0.1, dropout_rate=0.2, bn_momentum=0.98, activation_fn=<function elu at 0x112deef28> \n",
      "@E0\tCV Loss: 2245.568\tCV Acc: 19.380%\tEWP:00\n",
      "@E150\tCV Loss: 0.076\tCV Acc: 97.900%\tEWP:000014\n",
      "@E212\tCV Loss: 0.055\tCV Acc: 98.460%\tEWP:21\n",
      "\n",
      "...Early Stopping\n",
      "[CV]  n_neurons=300, n_hidden=7, learning_rate=0.1, dropout_rate=0.2, bn_momentum=0.98, activation_fn=<function elu at 0x112deef28>, total= 7.1min\n",
      "[CV] n_neurons=300, n_hidden=7, learning_rate=0.1, dropout_rate=0.2, bn_momentum=0.98, activation_fn=<function elu at 0x112deef28> \n",
      "@E0\tCV Loss: 277.255\tCV Acc: 20.080%\tEWP:00\n",
      "@E150\tCV Loss: 0.079\tCV Acc: 98.260%\tEWP:000452\n",
      "@E246\tCV Loss: 0.059\tCV Acc: 98.440%\tEWP:21\n",
      "\n",
      "...Early Stopping\n",
      "[CV]  n_neurons=300, n_hidden=7, learning_rate=0.1, dropout_rate=0.2, bn_momentum=0.98, activation_fn=<function elu at 0x112deef28>, total= 8.0min\n",
      "[CV] n_neurons=300, n_hidden=7, learning_rate=0.1, dropout_rate=0.2, bn_momentum=0.98, activation_fn=<function elu at 0x112deef28> \n",
      "@E0\tCV Loss: 504.922\tCV Acc: 19.920%\tEWP:00\n",
      "@E150\tCV Loss: 0.091\tCV Acc: 97.940%\tEWP:000552\n",
      "@E298\tCV Loss: 0.061\tCV Acc: 98.540%\tEWP:21\n",
      "\n",
      "...Early Stopping\n",
      "[CV]  n_neurons=300, n_hidden=7, learning_rate=0.1, dropout_rate=0.2, bn_momentum=0.98, activation_fn=<function elu at 0x112deef28>, total= 9.5min\n",
      "[CV] n_neurons=50, n_hidden=11, learning_rate=0.005, dropout_rate=0.4, bn_momentum=0.9, activation_fn=<function relu at 0x112e07d08> \n",
      "@E0\tCV Loss: 1.609\tCV Acc: 20.140%\tEWP:00\n",
      "@E101\tCV Loss: 1.561\tCV Acc: 23.960%\tEWP:21\n",
      "\n",
      "...Early Stopping\n",
      "[CV]  n_neurons=50, n_hidden=11, learning_rate=0.005, dropout_rate=0.4, bn_momentum=0.9, activation_fn=<function relu at 0x112e07d08>, total=  54.0s\n",
      "[CV] n_neurons=50, n_hidden=11, learning_rate=0.005, dropout_rate=0.4, bn_momentum=0.9, activation_fn=<function relu at 0x112e07d08> \n",
      "@E0\tCV Loss: 1.609\tCV Acc: 19.380%\tEWP:00\n",
      "@E51\tCV Loss: 1.689\tCV Acc: 20.400%\tEWP:21\n",
      "\n",
      "...Early Stopping\n",
      "[CV]  n_neurons=50, n_hidden=11, learning_rate=0.005, dropout_rate=0.4, bn_momentum=0.9, activation_fn=<function relu at 0x112e07d08>, total=  29.0s\n",
      "[CV] n_neurons=50, n_hidden=11, learning_rate=0.005, dropout_rate=0.4, bn_momentum=0.9, activation_fn=<function relu at 0x112e07d08> \n",
      "@E0\tCV Loss: 1.609\tCV Acc: 19.380%\tEWP:00\n",
      "@E59\tCV Loss: 1.941\tCV Acc: 22.540%\tEWP:21\n",
      "\n",
      "...Early Stopping\n",
      "[CV]  n_neurons=50, n_hidden=11, learning_rate=0.005, dropout_rate=0.4, bn_momentum=0.9, activation_fn=<function relu at 0x112e07d08>, total=  31.9s\n",
      "[CV] n_neurons=250, n_hidden=7, learning_rate=0.0001, dropout_rate=0.6, bn_momentum=0.999, activation_fn=<function relu at 0x112e07d08> \n",
      "@E0\tCV Loss: 1.611\tCV Acc: 12.960%\tEWP:00\n",
      "@E38\tCV Loss: 1.609\tCV Acc: 20.140%\tEWP:21\n",
      "\n",
      "...Early Stopping\n",
      "[CV]  n_neurons=250, n_hidden=7, learning_rate=0.0001, dropout_rate=0.6, bn_momentum=0.999, activation_fn=<function relu at 0x112e07d08>, total= 1.0min\n",
      "[CV] n_neurons=250, n_hidden=7, learning_rate=0.0001, dropout_rate=0.6, bn_momentum=0.999, activation_fn=<function relu at 0x112e07d08> \n",
      "@E0\tCV Loss: 1.611\tCV Acc: 18.640%\tEWP:00\n",
      "@E35\tCV Loss: 1.609\tCV Acc: 20.240%\tEWP:21\n",
      "\n",
      "...Early Stopping\n",
      "[CV]  n_neurons=250, n_hidden=7, learning_rate=0.0001, dropout_rate=0.6, bn_momentum=0.999, activation_fn=<function relu at 0x112e07d08>, total=  54.8s\n",
      "[CV] n_neurons=250, n_hidden=7, learning_rate=0.0001, dropout_rate=0.6, bn_momentum=0.999, activation_fn=<function relu at 0x112e07d08> \n",
      "@E0\tCV Loss: 1.608\tCV Acc: 23.740%\tEWP:00\n",
      "@E33\tCV Loss: 1.609\tCV Acc: 19.120%\tEWP:21\n",
      "\n",
      "...Early Stopping\n",
      "[CV]  n_neurons=250, n_hidden=7, learning_rate=0.0001, dropout_rate=0.6, bn_momentum=0.999, activation_fn=<function relu at 0x112e07d08>, total=  52.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 150 out of 150 | elapsed: 494.9min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@E0\tCV Loss: 1.588\tCV Acc: 42.460%\tEWP:00\n",
      "@E150\tCV Loss: 0.092\tCV Acc: 97.640%\tEWP:00\n",
      "@E300\tCV Loss: 0.064\tCV Acc: 98.460%\tEWP:00\n",
      "@E450\tCV Loss: 0.052\tCV Acc: 98.840%\tEWP:00\n",
      "@E522\tCV Loss: 0.050\tCV Acc: 98.920%\tEWP:21\n",
      "\n",
      "...Early Stopping\n",
      "CPU times: user 1d 16h 41min 52s, sys: 5h 45min 19s, total: 1d 22h 27min 11s\n",
      "Wall time: 8h 51min 13s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "choser_params = {\n",
    "    \"n_hidden\": [5, 7, 11, 13],\n",
    "    \"n_neurons\": [50, 100, 200, 250, 300],\n",
    "    \"learning_rate\": [0.1, 0.001, 0.005, 0.0001],\n",
    "    \"activation_fn\": [tf.nn.elu, tf.nn.relu],\n",
    "    \"dropout_rate\": [0.2, 0.3, 0.4, 0.5, 0.6],\n",
    "    \"bn_momentum\": [0.9, 0.95, 0.98, 0.99, 0.999]\n",
    "}\n",
    "\n",
    "dnn02 = RandomizedSearchCV(DnnClassifier(), choser_params, n_iter=50, verbose=2)\n",
    "dnn02.fit(X_0_4_train, y_0_4_train, **{\"X_cv\": X_0_4_cv, \"y_cv\": y_0_4_cv,  \"epochs\":1500})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.99280015567230984"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_0_4_test, dnn02.predict(X_0_4_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_neurons': 300,\n",
       " 'n_hidden': 11,\n",
       " 'learning_rate': 0.0001,\n",
       " 'dropout_rate': 0.2,\n",
       " 'bn_momentum': 0.98,\n",
       " 'activation_fn': <function tensorflow.python.ops.gen_nn_ops.elu(features, name=None)>}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dnn02.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./tfmodels/best_mnist_0_4_dnn'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dnn02.best_estimator_.save(\"./tfmodels/best_mnist_0_4_dnn\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfer Learning\n",
    "### Deep Learning V.02.01\n",
    "Create a new DNN that reuses all the pretrained hidden layers of the previous model, freezes them, and replaces the softmax output layer with a new one.\n",
    "\n",
    "Train this new DNN on digits 5 to 9, using only 100 images per digit, and time how long it takes. Despite this small number of examples, can you achieve high precision?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_features_labels(features, labels, nsamp=100, seed=None):\n",
    "    np.random.seed(seed)\n",
    "    db = np.c_[labels, features]\n",
    "    np.random.shuffle(db)\n",
    "    return db[:nsamp, 1:], db[:nsamp, 0]\n",
    "\n",
    "map_train_5_9 = y_train >= 5\n",
    "map_test_5_9 = y_test >= 5\n",
    "\n",
    "y_5_9_train = y_train[map_train_5_9] - 5\n",
    "X_5_9_train = X_train[map_train_5_9,:]\n",
    "\n",
    "y_5_9_cv, y_5_9_train = y_5_9_train[:5000], y_5_9_train[5000:]\n",
    "X_5_9_cv, X_5_9_train = X_5_9_train[:5000], X_5_9_train[5000:]\n",
    "\n",
    "y_5_9_test = y_test[map_test_5_9] - 5\n",
    "X_5_9_test = X_test[map_test_5_9, :]\n",
    "\n",
    "X_5_9_train, y_5_9_train = sample_features_labels(X_5_9_train, y_5_9_train, seed=1643)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "dnn_04 = tf.train.import_meta_graph(\"./tfmodels/best_mnist_0_4_dnn.meta\")\n",
    "\n",
    "X = tf.get_default_graph().get_tensor_by_name(\"DNN/X:0\")\n",
    "y = tf.get_default_graph().get_tensor_by_name(\"y:0\")\n",
    "training = tf.get_default_graph().get_tensor_by_name(\"training:0\")\n",
    "loss = tf.get_default_graph().get_tensor_by_name(\"loss/loss:0\")\n",
    "y_proba = tf.get_default_graph().get_tensor_by_name(\"DNN/y_proba:0\")\n",
    "logits = y_proba.op.inputs[0]\n",
    "accuracy = tf.get_default_graph().get_tensor_by_name(\"metrics/Mean:0\") # metrics/accuracy was not created\n",
    "\n",
    "# To freeze the lower layers, we keep only the 'logits' tensor from the list of trainable variables\n",
    "\n",
    "learning_rate = 0.001\n",
    "train_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=\"logits\")\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate, name=\"Adam_tl\")\n",
    "train_step = optimizer.minimize(loss, var_list=train_vars, name=\"train_step_tl\")\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./tfmodels/best_mnist_0_4_dnn\n",
      "@E0\tCV Acc:27.90%\tCV Loss 1.60\tEOP:00\n",
      "@E20\tCV Acc:48.38%\tCV Loss 1.29\tEOP:00\n",
      "@E40\tCV Acc:51.74%\tCV Loss 1.27\tEOP:00\n",
      "@E60\tCV Acc:53.72%\tCV Loss 1.25\tEOP:00\n",
      "@E80\tCV Acc:54.76%\tCV Loss 1.21\tEOP:00\n",
      "@E100\tCV Acc:56.08%\tCV Loss 1.17\tEOP:04\n",
      "@E116\tCV Acc:55.62%\tCV Loss 1.19\tEOP:20\n",
      "...Early Stopping\n",
      "CPU times: user 2min 25s, sys: 4.91 s, total: 2min 30s\n",
      "Wall time: 51.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "max_epochs_without_progress = 20\n",
    "epochs_without_progress = 0\n",
    "best_loss = np.infty\n",
    "epochs = 200\n",
    "with tf.Session() as sess:\n",
    "    # 1. Initialize all variables\n",
    "    sess.run(init)\n",
    "    # 2. Restore to older state: neurons for every layer return\n",
    "    # to their last value\n",
    "    dnn_04.restore(sess, \"./tfmodels/best_mnist_0_4_dnn\")\n",
    "    # 3. We re-initialize the output layer (logits), which\n",
    "    # we will learn as a brand new one (we will not consier\n",
    "    # past values)\n",
    "    for var in train_vars:\n",
    "        var.initializer.run()\n",
    "        \n",
    "    for epoch in range(epochs):\n",
    "        sess.run(train_step, feed_dict={X: X_5_9_train, y: y_5_9_train, training:True})\n",
    "        cv_acc, cv_loss = sess.run([accuracy, loss],\n",
    "                                   feed_dict={X: X_5_9_cv, y: y_5_9_cv})\n",
    "        \n",
    "        end = \"\\r\" if epoch % (epochs // 10) != 0 else \"\\n\"\n",
    "        if cv_loss > best_loss:\n",
    "            epochs_without_progress += 1\n",
    "        else:\n",
    "            best_loss = cv_loss\n",
    "            epochs_without_progress = 0\n",
    "            saver.save(sess, \"./tfmodels/best_mnist_5_9_dnn\")\n",
    "            \n",
    "        print(f\"@E{epoch}\\tCV Acc:{cv_acc:0.2%}\\tCV Loss {cv_loss:0.2f}\\tEOP:{epochs_without_progress:02}\", end=end)\n",
    "        if epochs_without_progress >= max_epochs_without_progress:\n",
    "            print(\"\\n...Early Stopping\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Caching the output layer**  \n",
    "Try caching the frozen layers and train the model again. How much faster is it now?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "learning_rate = 0.001\n",
    "# Load the graph\n",
    "dnn_04 = tf.train.import_meta_graph(\"./tfmodels/best_mnist_0_4_dnn.meta\")\n",
    "\n",
    "X = tf.get_default_graph().get_tensor_by_name(\"DNN/X:0\")\n",
    "y = tf.get_default_graph().get_tensor_by_name(\"y:0\")\n",
    "\n",
    "loss = tf.get_default_graph().get_tensor_by_name(\"loss/loss:0\")\n",
    "train_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=\"logits\")\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate, name=\"Adam_t2\")\n",
    "train_step = optimizer.minimize(loss, var_list=train_vars)\n",
    "\n",
    "accuracy = tf.get_default_graph().get_tensor_by_name(\"metrics/Mean:0\")\n",
    "\n",
    "# Last hidden layer\n",
    "hidden5_tensor = tf.get_default_graph().get_tensor_by_name(\"DNN/a_11:0\")\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./tfmodels/best_mnist_0_4_dnn\n",
      "@Epoch 000\tCV Acc: 0.32\tBest Loss 1.56\tEOP:00\n",
      "@Epoch 020\tCV Acc: 0.61\tBest Loss 1.07\tEOP:00\n",
      "@Epoch 040\tCV Acc: 0.65\tBest Loss 0.95\tEOP:00\n",
      "@Epoch 060\tCV Acc: 0.66\tBest Loss 0.89\tEOP:00\n",
      "@Epoch 080\tCV Acc: 0.67\tBest Loss 0.86\tEOP:00\n",
      "@Epoch 100\tCV Acc: 0.69\tBest Loss 0.83\tEOP:00\n",
      "@Epoch 120\tCV Acc: 0.70\tBest Loss 0.82\tEOP:00\n",
      "@Epoch 140\tCV Acc: 0.70\tBest Loss 0.80\tEOP:00\n",
      "@Epoch 160\tCV Acc: 0.70\tBest Loss 0.80\tEOP:00\n",
      "@Epoch 180\tCV Acc: 0.71\tBest Loss 0.79\tEOP:00\n",
      "CPU times: user 1min 11s, sys: 3.54 s, total: 1min 15s\n",
      "Wall time: 1min 12s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "max_epochs_without_progress = 20\n",
    "epochs_without_progress = 0\n",
    "best_loss = np.infty\n",
    "epochs = 200\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    dnn_04.restore(sess, \"./tfmodels/best_mnist_0_4_dnn\")\n",
    "    for var in train_vars: var.initializer.run()\n",
    "        \n",
    "    # Caching the frozen layers\n",
    "    hidden5_train = sess.run(hidden5_tensor, feed_dict={X: X_5_9_train})\n",
    "    hidden5_cv = sess.run(hidden5_tensor, feed_dict={X: X_5_9_cv})\n",
    "    \n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        sess.run(train_step, feed_dict={hidden5_tensor: hidden5_train, y: y_5_9_train})\n",
    "        cv_acc, cv_loss = sess.run([accuracy, loss],\n",
    "                                   feed_dict={hidden5_tensor: hidden5_cv, y: y_5_9_cv})\n",
    "        \n",
    "        if best_loss > cv_loss:\n",
    "            best_loss = cv_loss\n",
    "            epochs_without_progress = 0\n",
    "            saver.save(sess, \"./tfmodels/best_mnist_5_9_dnn\")\n",
    "        else:\n",
    "            epochs_without_progress += 1\n",
    "            \n",
    "        end = \"\\n\" if epoch % (epochs // 10) == 0 else \"\\r\"\n",
    "        print((f\"@Epoch {epoch:03}\\tCV Acc: {cv_acc:0.2f}\\tBest Loss \"\n",
    "               \"{best_loss:0.2f}\\tEOP:{epochs_without_progress:02}\"), end=end)\n",
    "        if epochs_without_progress >= max_epochs_without_progress:\n",
    "            print(\"\\n...Early Stopping\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Learning V.02.02\n",
    "**Unfreze the last two layers and continue training**  \n",
    "How much better is the accuracy now?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "dnn_04 = tf.train.import_meta_graph(\"./tfmodels/best_mnist_0_4_dnn.meta\")\n",
    "dnn_04_graph = tf.get_default_graph()\n",
    "\n",
    "X = dnn_04_graph.get_tensor_by_name(\"DNN/X:0\")\n",
    "y = dnn_04_graph.get_tensor_by_name(\"y:0\")\n",
    "loss = dnn_04_graph.get_tensor_by_name(\"loss/loss:0\")\n",
    "accuracy = dnn_04_graph.get_tensor_by_name(\"metrics/Mean:0\")\n",
    "\n",
    "learning_rate = 0.001\n",
    "# By stating the scope of the trainable variables, in this case, \n",
    "# the output layer and last hidden two hidden layers, backprop\n",
    "# will only compute and affect the layers inside the scope.\n",
    "trainable_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,\n",
    "                                   scope=\"logits|z_1[01]|batch_\\w+(9|10)|dropout(9|10)\")\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate, name=\"Adam_t3\")\n",
    "train_step = optimizer.minimize(loss, var_list=trainable_vars)\n",
    "\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./tfmodels/best_mnist_0_4_dnn\n",
      "@Epoch 000\tCV Acc: 0.37\tBest Loss 3.28\tEOP:00\n",
      "@Epoch 057\tCV Acc: 0.75\tBest Loss 0.74\tEOP:20\n",
      "...Early Stopping\n",
      "CPU times: user 56 s, sys: 1.66 s, total: 57.7 s\n",
      "Wall time: 10.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "epochs = 1000\n",
    "max_epochs_without_progress = 20\n",
    "epochs_without_progress = 0\n",
    "best_loss = np.infty\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    dnn_04.restore(sess, \"./tfmodels/best_mnist_0_4_dnn\")\n",
    "    for epoch in range(epochs):\n",
    "        sess.run(train_step, feed_dict={X: X_5_9_train, y: y_5_9_train})\n",
    "        cv_acc, cv_loss = sess.run([accuracy, loss],\n",
    "                                   feed_dict={X: X_5_9_cv, y: y_5_9_cv})\n",
    "        \n",
    "        if cv_loss < best_loss:\n",
    "            best_loss = cv_loss\n",
    "            epochs_without_progress = 0\n",
    "        else:\n",
    "            epochs_without_progress += 1\n",
    "        \n",
    "        end = \"\\n\" if epoch % (epochs // 10) == 0 else \"\\r\"\n",
    "        print((f\"@Epoch {epoch:03}\\tCV Acc: {cv_acc:0.2f}\\tBest Loss \"\n",
    "               f\"{best_loss:0.2f}\\tEOP:{epochs_without_progress:02}\"), end=end)\n",
    "        \n",
    "        if epochs_without_progress >= max_epochs_without_progress:\n",
    "            print(\"\\n...Early Stopping\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pretraining on an auxiliary task\n",
    "In this exercise you will build a DNN that compares two MNIST digit images and predicts whether they represent the same digit or not. Then you will reuse the lower layers of this network to train an MNIST classifier using very little training data\n",
    "\n",
    "1. Start by building two DNNs (let’s call them DNN A and B), both similar to the one you built earlier but without the output layer: each DNN should have five hidden layers of 100 neurons each, He initialization, and ELU activation. Next, add one more hidden layer with 10 units on top of both DNNs. To do this, you should use TensorFlow’s concat() function with axis=1 to concatenate the outputs of both DNNs for each instance, then feed the result to the hidden layer. Finally, add an output layer with a single neuron using the logistic activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "he_init = tf.variance_scaling_initializer()\n",
    "def dnn(inputs, n_hidden, n_neurons, name=None,\n",
    "        activation=tf.nn.elu, initializer=he_init):\n",
    "    \"\"\"\n",
    "    Generate a standard Feedforward Deep Neural Network\n",
    "    with standard number of neurons\n",
    "    \"\"\"\n",
    "    with tf.variable_scope(name, default_name=\"dnn\"):\n",
    "        for hidden in range(n_hidden):\n",
    "            inputs = tf.layers.dense(inputs, n_neurons,\n",
    "                                     activation=activation,\n",
    "                                     kernel_initializer=initializer,\n",
    "                                     name=f\"hidden_{hidden + 1}\")\n",
    "        return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "n_inputs = 28 * 28\n",
    "\n",
    "with tf.name_scope(\"DNN\"):\n",
    "    X = tf.placeholder(tf.float32, shape=(None, 2, n_inputs), name=\"X\")\n",
    "    y = tf.placeholder(tf.int32, shape=[None, 1], name=\"y\")\n",
    "    X1, X2 = tf.unstack(X, axis=1)\n",
    "\n",
    "    dnn1 = dnn(X1, 5, 100, name=\"DNN1\")\n",
    "    dnn2 = dnn(X2, 5, 100, name=\"DNN2\")\n",
    "\n",
    "    dnn_concat = tf.concat([dnn1, dnn2], axis=1)\n",
    "    hidden = tf.layers.dense(dnn_concat, 10,\n",
    "                             activation=tf.nn.elu,\n",
    "                             kernel_initializer=he_init)\n",
    "\n",
    "    logits = tf.layers.dense(hidden, 1,\n",
    "                             activation=None,\n",
    "                             kernel_initializer=he_init,\n",
    "                             name=\"logit\")\n",
    "\n",
    "    y_proba = tf.nn.sigmoid(logits, name=\"y_proba\")\n",
    "    y_pred = tf.cast(tf.greater_equal(logits, 0), tf.int32)\n",
    "\n",
    "with tf.name_scope(\"loss\"):\n",
    "    y_float = tf.cast(y, tf.float32)\n",
    "    xentropy = tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "        labels=y_float,\n",
    "        logits=logits\n",
    "    )\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "    \n",
    "alpha = 0.001\n",
    "with tf.name_scope(\"training\"):\n",
    "    optimizer = tf.train.AdamOptimizer(alpha)\n",
    "    train_step = optimizer.minimize(loss)\n",
    "    \n",
    "with tf.name_scope(\"metrics\"):\n",
    "    y_pred_correct = tf.equal(y, y_pred)\n",
    "    accuracy = tf.reduce_mean(tf.cast(y_pred_correct, tf.float32), name=\"accuracy\")\n",
    "\n",
    "with tf.name_scope(\"ops\"):\n",
    "    init = tf.global_variables_initializer()\n",
    "    saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Split the MNIST training set in two sets: split #1 should containing 55,000 images, and split #2 should contain contain 5,000 images. Create a function that generates a training batch where each instance is a pair of MNIST images picked from split #1. Half of the training instances should be pairs of images that belong to the same class, while the other half should be images from different classes. For each pair, the training label should be 0 if the images are from the same class, or 1 if they are from different classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_training_batch(images, labels, batch_size, seed=None):\n",
    "    if batch_size < 1:\n",
    "        raise ValueError(\"Batch size must be greater than 0\")\n",
    "    # Force every batch to be a pair number\n",
    "    np.random.seed(seed)\n",
    "    n_labels = len(labels)\n",
    "    if batch_size % 2 != 0: batch_size += 1\n",
    "    target_indices = np.arange(n_labels); np.random.shuffle(target_indices)\n",
    "    target_indices = target_indices[:batch_size]\n",
    "    same_vals = [np.random.choice(np.where(labels == labels[target])[0])\n",
    "                 for target in target_indices[:batch_size // 2]]\n",
    "    diff_vals = [np.random.choice(np.where(labels != labels[target])[0])\n",
    "                 for target in target_indices[batch_size // 2:]]\n",
    "    vals = same_vals + diff_vals\n",
    "    \n",
    "    rand_ix_choices = np.arange(batch_size); np.random.shuffle(rand_ix_choices)\n",
    "    Xv = [np.r_[images[t], images[v]] for t, v in zip(target_indices, vals)]\n",
    "    # First half is the same; second half are different\n",
    "    yv = np.array([0 if ix < batch_size / 2 else 1 for ix in range(batch_size)])\n",
    "    \n",
    "    Xv = np.array(Xv).reshape(batch_size, 2, -1)\n",
    "    yv = yv.reshape(-1, 1)\n",
    "    \n",
    "    return Xv[rand_ix_choices], yv[rand_ix_choices]\n",
    "\n",
    "def show_sample(Xbatch, ybatch, index):\n",
    "    if ybatch[index] == 0:\n",
    "        print(\"Numbers are the same\")\n",
    "    else:\n",
    "        print(\"Numbers are different\")\n",
    "    fig = plt.figure()\n",
    "    ax1 = fig.add_subplot(121)\n",
    "    ax2 = fig.add_subplot(122)\n",
    "    ax1.imshow(Xbatch[index][0].reshape(28, 28), cmap=\"gray_r\")\n",
    "    ax2.imshow(Xbatch[index][1].reshape(28, 28), cmap=\"gray_r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xb, yb = get_training_batch(X_train, y_train, 4, seed=3141592)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0]])"
      ]
     },
     "execution_count": 302,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0) Numbers are the same\n",
      "1) Numbers are different\n",
      "2) Numbers are different\n",
      "3) Numbers are the same\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i in range(4):\n",
    "    print(f\"{i})\", end=\" \")\n",
    "    show_sample(Xb, yb, i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Train the DNN on this training set. For each image pair, you can simultaneously feed the first image to DNN A and the second image to DNN B. The whole network will gradually learn to tell whether two images belong to the same class or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@Epoch 0\tloss:0.58\ttest accuracy:0.73\n",
      "@Epoch 10\tloss:0.21\ttest accuracy:0.91\n",
      "@Epoch 20\tloss:0.10\ttest accuracy:0.94\n",
      "@Epoch 30\tloss:0.06\ttest accuracy:0.95\n",
      "@Epoch 40\tloss:0.06\ttest accuracy:0.95\n",
      "@Epoch 50\tloss:0.04\ttest accuracy:0.96\n",
      "@Epoch 60\tloss:0.02\ttest accuracy:0.96\n",
      "@Epoch 70\tloss:0.02\ttest accuracy:0.96\n",
      "@Epoch 80\tloss:0.02\ttest accuracy:0.97\n",
      "@Epoch 90\tloss:0.01\ttest accuracy:0.97\n",
      "CPU times: user 19min 37s, sys: 2min 20s, total: 21min 57s\n",
      "Wall time: 14min 57s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "epochs = 100\n",
    "batch_size = 2000\n",
    "nelements = X_train.shape[0]\n",
    "Xb_test, yb_test = get_training_batch(X_test, y_test, 500)\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for epoch in range(epochs):\n",
    "        for batch in range(nelements // batch_size):\n",
    "            Xb, yb = get_training_batch(X_train, y_train, batch_size)\n",
    "            epoch_loss, _ = sess.run([loss, train_step], feed_dict={X: Xb, y: yb})\n",
    "        acc_test = sess.run(accuracy, feed_dict={X: Xb_test, y: yb_test})\n",
    "        end = \"\\n\" if epoch % 10 == 0 else \"\\r\"\n",
    "        print(f\"@Epoch {epoch}\\tloss:{epoch_loss:0.2f}\\ttest accuracy:{acc_test:0.2f}\", end=end)\n",
    "    saver.save(sess, \"./tfmodels/tranfer_nnet.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Now create a new DNN by reusing and freezing the hidden layers of DNN A and adding a softmax output layer on top with 10 neurons. Train this network on split #2 and see if you can achieve high performance despite having only 500 images per class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
