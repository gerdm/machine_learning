{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from ipywidgets import FloatSlider, interact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import exp, identity\n",
    "\n",
    "def softmax(x, z, i):\n",
    "    return exp(x[i] * z[i]) / np.sum(exp(x * z))\n",
    "\n",
    "def dsoftmaxz(x, z, ix_eval, ix_deriv):\n",
    "    \"\"\"\n",
    "    Compute the partial of the softmax function w.r.t.\n",
    "    the ix_deriv index of z; evaluated at ix_eval\n",
    "    \n",
    "    Paramters\n",
    "    ---------\n",
    "    x: nx1 numpy array\n",
    "        The input elements to the softmax function \n",
    "    z: nx1 numpy array\n",
    "        The coefficients to the softmax function\n",
    "    ix_eval: int\n",
    "        The index to evaluate the softmax partial\n",
    "    ix_deriv: int\n",
    "        The index at z from which the partial will\n",
    "        be valuated\n",
    "        \n",
    "    Return\n",
    "    ------\n",
    "    float: The partial derivative of the softmax function\n",
    "    \"\"\"\n",
    "    if ix_eval == ix_deriv:\n",
    "        softmax_eval = softmax(X, Z, ix_eval)\n",
    "        return X[ix_eval] * softmax_eval * (1 - softmax_eval)\n",
    "    else:\n",
    "        softmax_eval = softmax(X, Z, ix_eval)\n",
    "        softmax_deriv = softmax(X, Z, ix_deriv)\n",
    "        return -X[ix_deriv] * softmax_eval * softmax_deriv\n",
    "    \n",
    "\n",
    "def dsoftmaxx(x, z, ix_eval):\n",
    "    \"\"\"\n",
    "    Compute the partial derivative of the softmax\n",
    "    function w.r.t. the input vector x.\n",
    "    \"\"\"\n",
    "    pass\n",
    "    \n",
    "\n",
    "def dnum_softmax(x, z, i, perturbation_index, perturbation=1e-6):\n",
    "    \"\"\"\n",
    "    Compute numerical derivative of the softmax function\n",
    "    w.r.t. weight index \"perturbation_index\" evaluated\n",
    "    at index \"i\"\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    x: nx1 numpy array\n",
    "        The input elements to the softmax function \n",
    "    z: nx1 numpy array\n",
    "        The coefficients to the softmax function\n",
    "    perturbation_index: int\n",
    "        The index where the perturbation will be performed\n",
    "    perturbatio: float (positive)\n",
    "        The value to perturbate the softmax function\n",
    "        \n",
    "    Reutrns\n",
    "    -------\n",
    "    float: The numerical derivative of the softmax function\n",
    "    \"\"\"\n",
    "    zc = np.copy(z)\n",
    "    zc[perturbation_index] += perturbation\n",
    "    return (softmax(x, zc, i) - softmax(x, z, i)) / perturbation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1bccf1762bf4392a0eefeb7fbefa2c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.interact_softmax>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def interact_softmax(z1, z2, z3, z4):\n",
    "    X = np.array([1, 2, 3, 4])\n",
    "    Z = np.array([z1, z2, z3, z4])\n",
    "    Y = [softmax(X, Z, i) for i in range(4)]\n",
    "    sns.barplot(X, Y)\n",
    "    plt.ylim(0, 1)\n",
    "    plt.xticks([0, 1, 2, 3], [\"x1\", \"x2\", \"x3\", \"x4\"])\n",
    "    plt.title(\"Softmax\")\n",
    "    plt.show()\n",
    "    \n",
    "interact(interact_softmax,\n",
    "         z1 = FloatSlider(min=0, max=1.2, step=0.1, value=0.5),\n",
    "         z2 = FloatSlider(min=0, max=1.2, step=0.1, value=0.5),\n",
    "         z3 = FloatSlider(min=0, max=1.2, step=0.1, value=0.5),\n",
    "         z4 = FloatSlider(min=0, max=1.2, step=0.1, value=0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.066041037873964384,\n",
       " 0.072986634457837832,\n",
       " 0.72798200884534181,\n",
       " 0.13299031882285606]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.array([1, 4, 3, 2])\n",
    "Z = np.array([ 0.3,  0.1,  0.9,  0.5])\n",
    "\n",
    "[softmax(X, Z, i) for i in range(4)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\newcommand{\\partialwrt}[2]{\\frac{\\partial #1}{\\partial #2}}$\n",
    "Define the softmax function $g(z_i| z)$ as\n",
    "$$\n",
    "    g(z_i| z) = \\frac{e^{z_i}}{\\sum_{j=1}^n e^{z_j }}; \\ z_k = x_k\\theta_k\n",
    "$$\n",
    "\n",
    "Then,\n",
    "$$\n",
    "\\partialwrt{g}{\\theta_k}(z_i) = \n",
    "\\begin{cases}\n",
    "     x_ig(z_i)[1 - g(z_i)] & k = i \\\\\n",
    "     -x_k g(z_i)g(z_k) & k \\neq i\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Furthermore,\n",
    "$$\n",
    "    \\partialwrt{g}{x}(z_i) = -\\left[\\sum_j\\exp{x\\theta_j}\\right]^{-2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.6755783637255615e-08"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testing numerical softmax partial v.s. theoretical softmax partial w.r.t. theta zi\n",
    "softmax_deriv = X[0] * softmax(X, Z, 0) * (1 - softmax(X, Z, 0))\n",
    "softmax_numderiv = dnum_softmax(X, Z, 0, 0)\n",
    "\n",
    "abs(softmax_deriv - softmax_numderiv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.8641973117485904e-08"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testing numerical v.s. theoretical partial softmax evaluated at\n",
    "# index i, derivative at index j != i\n",
    "softmax_deriv = dnum_softmax(X, Z, 0, 2)\n",
    "softmax_numderiv = -X[2] * softmax(X, Z, 0) * softmax(X, Z,2 )\n",
    "abs(softmax_deriv - softmax_numderiv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the **Cross-Entropy Cost Function** $C$ as\n",
    "$$\n",
    "    C^{(i)} = \\sum_j t_j^{(i)} \\log y_j^{(i)}\n",
    "$$\n",
    "\n",
    "That is, the cost function considering training case $i$ over all output units $j$; $t_j^{(i)}$ is the target ouput at neuron $j$, training case $i$, and $y_j^{(i)}$ is the model output at neuron $j$, training case $i$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.06604104,  0.26416415,  0.19812311,  0.13208208])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = 0\n",
    "shock = 1e-6\n",
    "softmax(X, Z, 0) * X - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.029638553568977453"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax(X, Z, 0) * np.sum(Z[0] - np.sum(Z * exp(X * Z)) / sum(exp(X * Z)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.029638549198240938"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(softmax(X+shock, Z, 0) - softmax(X, Z, 0)) / shock"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
