{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Character level language model - Dinosaurus land\n",
    "### Keras Implementation of the Deep-Learning Specializiation project\n",
    "\n",
    "\n",
    "Naming new dinos using RNNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gerardoduran/anaconda/envs/deeplearning/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, SimpleRNN, Activation, Dense\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import LambdaCallback\n",
    "from numpy.random import seed\n",
    "import tensorflow as tf\n",
    "import random\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_ix(encoding):\n",
    "    \"\"\"\n",
    "    Decode an encoded dinosaur sequence presented as an\n",
    "    (n,1) numpy array\n",
    "    \"\"\"\n",
    "    string = \"\".join(ix_to_char[ix] for ix in encoding.ravel())\n",
    "    return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "with open(\"dinos.txt\", \"r\") as f:\n",
    "    dinos = f.read().lower()\n",
    "characters = sorted(list(set(dinos)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We look at a small sample of the names in the training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['opisthocoelicaudia', 'skeleton', 'marshosaurus', 'tototlmimus',\n",
       "       'ozraptor', 'hylosaurus', 'elvisaurus'], dtype='<U23')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed(1643)\n",
    "np.random.choice(dinos.split(), 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map from all characters in the training dataset to a uniquex index\n",
    "char_to_ix = {ix:char for char, ix in enumerate(characters)}\n",
    "# Reverse map of char_to_ix to retrieve the index given the character\n",
    "ix_to_char = {char:ix for char, ix in enumerate(characters,)}\n",
    "\n",
    "nchars = len(characters)\n",
    "nvocab = len(dinos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'\\n': 0,\n",
       " 'a': 1,\n",
       " 'b': 2,\n",
       " 'c': 3,\n",
       " 'd': 4,\n",
       " 'e': 5,\n",
       " 'f': 6,\n",
       " 'g': 7,\n",
       " 'h': 8,\n",
       " 'i': 9,\n",
       " 'j': 10,\n",
       " 'k': 11,\n",
       " 'l': 12,\n",
       " 'm': 13,\n",
       " 'n': 14,\n",
       " 'o': 15,\n",
       " 'p': 16,\n",
       " 'q': 17,\n",
       " 'r': 18,\n",
       " 's': 19,\n",
       " 't': 20,\n",
       " 'u': 21,\n",
       " 'v': 22,\n",
       " 'w': 23,\n",
       " 'x': 24,\n",
       " 'y': 25,\n",
       " 'z': 26}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char_to_ix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training instances: 19,903\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((19903, 10, 1), (19903, 27))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Size of the sequence to pass as training example\n",
    "seqlen = 10\n",
    "xtrain, ytrain = [], []\n",
    "for i in range(0, nvocab - seqlen):\n",
    "    xt = dinos[i: i + seqlen]\n",
    "    yt = dinos[i + seqlen]\n",
    "    xtrain.append([char_to_ix[char] for char in xt])\n",
    "    ytrain.append([char_to_ix[char] for char in yt])\n",
    "    \n",
    "# training dataset is now\n",
    "# of shape (xtrain X seqlen)\n",
    "print(f\"Number of training instances: {len(xtrain):,}\")\n",
    "\n",
    "# Reshaping into the form:\n",
    "# nfeatures X timesteps X features\n",
    "xtrain = np.reshape(xtrain, (-1, seqlen, 1))\n",
    "# Normalizing values\n",
    "#xtrain = xtrain / nvocab\n",
    "# Transorming output values to be\n",
    "# One-hot encoded\n",
    "ytrain = to_categorical(ytrain)\n",
    "xtrain.shape, ytrain.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First five instances in the training dataset: A `seqlen` number characters followed by the next character of the dinosaur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1  1  3  8  5 14 15 19  1 21]\n",
      "'aachenosau'\n",
      "'r'\n",
      "\n",
      "[ 1  3  8  5 14 15 19  1 21 18]\n",
      "'achenosaur'\n",
      "'u'\n",
      "\n",
      "[ 3  8  5 14 15 19  1 21 18 21]\n",
      "'chenosauru'\n",
      "'s'\n",
      "\n",
      "[ 8  5 14 15 19  1 21 18 21 19]\n",
      "'henosaurus'\n",
      "'\\n'\n",
      "\n",
      "[ 5 14 15 19  1 21 18 21 19  0]\n",
      "'enosaurus\\n'\n",
      "'a'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print(xtrain[i].ravel())\n",
    "    print(repr(decode_ix(xtrain[i])))\n",
    "    print(repr(ix_to_char[np.where(ytrain[i] == 1)[0][0]]))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "19903/19903 [==============================] - 3s 174us/step - loss: 2.4776\n",
      "Epoch 2/100\n",
      "19903/19903 [==============================] - 3s 166us/step - loss: 2.3135\n",
      "Epoch 3/100\n",
      "19903/19903 [==============================] - 3s 165us/step - loss: 2.2333\n",
      "Epoch 4/100\n",
      "19903/19903 [==============================] - 3s 165us/step - loss: 2.1625\n",
      "Epoch 5/100\n",
      "19903/19903 [==============================] - 3s 171us/step - loss: 2.1147\n",
      "Epoch 6/100\n",
      "19903/19903 [==============================] - 3s 170us/step - loss: 2.0597\n",
      "Epoch 7/100\n",
      "19903/19903 [==============================] - 3s 168us/step - loss: 2.0199\n",
      "Epoch 8/100\n",
      "19903/19903 [==============================] - 4s 182us/step - loss: 1.9787\n",
      "Epoch 9/100\n",
      "19903/19903 [==============================] - 3s 168us/step - loss: 1.9491\n",
      "Epoch 10/100\n",
      "19903/19903 [==============================] - 4s 177us/step - loss: 1.9196\n",
      "Epoch 11/100\n",
      "19903/19903 [==============================] - 3s 170us/step - loss: 1.8850\n",
      "Epoch 12/100\n",
      "19903/19903 [==============================] - 3s 168us/step - loss: 1.8603\n",
      "Epoch 13/100\n",
      "19903/19903 [==============================] - 4s 195us/step - loss: 1.8276\n",
      "Epoch 14/100\n",
      "19903/19903 [==============================] - 4s 176us/step - loss: 1.8073\n",
      "Epoch 15/100\n",
      "19903/19903 [==============================] - 3s 175us/step - loss: 1.7810\n",
      "Epoch 16/100\n",
      "19903/19903 [==============================] - 3s 159us/step - loss: 1.7554\n",
      "Epoch 17/100\n",
      "19903/19903 [==============================] - 3s 160us/step - loss: 1.7336\n",
      "Epoch 18/100\n",
      "19903/19903 [==============================] - 3s 166us/step - loss: 1.7059\n",
      "Epoch 19/100\n",
      "19903/19903 [==============================] - 3s 162us/step - loss: 1.6848\n",
      "Epoch 20/100\n",
      "19903/19903 [==============================] - 3s 162us/step - loss: 1.6622\n",
      "Epoch 21/100\n",
      "19903/19903 [==============================] - 3s 164us/step - loss: 1.6401\n",
      "Epoch 22/100\n",
      "19903/19903 [==============================] - 3s 172us/step - loss: 1.6218\n",
      "Epoch 23/100\n",
      "19903/19903 [==============================] - 3s 172us/step - loss: 1.6031\n",
      "Epoch 24/100\n",
      "19903/19903 [==============================] - 3s 165us/step - loss: 1.5803\n",
      "Epoch 25/100\n",
      "19903/19903 [==============================] - 3s 167us/step - loss: 1.5628\n",
      "Epoch 26/100\n",
      "19903/19903 [==============================] - 3s 169us/step - loss: 1.5400\n",
      "Epoch 27/100\n",
      "19903/19903 [==============================] - 3s 170us/step - loss: 1.5221\n",
      "Epoch 28/100\n",
      "19903/19903 [==============================] - 4s 180us/step - loss: 1.5100\n",
      "Epoch 29/100\n",
      "19903/19903 [==============================] - 3s 159us/step - loss: 1.4888\n",
      "Epoch 30/100\n",
      "19903/19903 [==============================] - 3s 159us/step - loss: 1.4718\n",
      "Epoch 31/100\n",
      "19903/19903 [==============================] - 3s 162us/step - loss: 1.4535\n",
      "Epoch 32/100\n",
      "19903/19903 [==============================] - 3s 159us/step - loss: 1.4325\n",
      "Epoch 33/100\n",
      "19903/19903 [==============================] - 3s 158us/step - loss: 1.4217\n",
      "Epoch 34/100\n",
      "19903/19903 [==============================] - 3s 162us/step - loss: 1.4067\n",
      "Epoch 35/100\n",
      "19903/19903 [==============================] - 3s 175us/step - loss: 1.3868\n",
      "Epoch 36/100\n",
      "19903/19903 [==============================] - 4s 179us/step - loss: 1.3757\n",
      "Epoch 37/100\n",
      "19903/19903 [==============================] - 3s 175us/step - loss: 1.3586\n",
      "Epoch 38/100\n",
      "19903/19903 [==============================] - 3s 166us/step - loss: 1.3479\n",
      "Epoch 39/100\n",
      "19903/19903 [==============================] - 3s 175us/step - loss: 1.3321\n",
      "Epoch 40/100\n",
      "19903/19903 [==============================] - 4s 184us/step - loss: 1.3196\n",
      "Epoch 41/100\n",
      "19903/19903 [==============================] - 4s 176us/step - loss: 1.3037\n",
      "Epoch 42/100\n",
      "19903/19903 [==============================] - 3s 169us/step - loss: 1.2928\n",
      "Epoch 43/100\n",
      "19903/19903 [==============================] - 3s 172us/step - loss: 1.2769\n",
      "Epoch 44/100\n",
      "19903/19903 [==============================] - 4s 198us/step - loss: 1.2719\n",
      "Epoch 45/100\n",
      "19903/19903 [==============================] - 4s 194us/step - loss: 1.2603\n",
      "Epoch 46/100\n",
      "19903/19903 [==============================] - 4s 178us/step - loss: 1.2521\n",
      "Epoch 47/100\n",
      "19903/19903 [==============================] - 4s 177us/step - loss: 1.2426\n",
      "Epoch 48/100\n",
      "19903/19903 [==============================] - 4s 186us/step - loss: 1.2250\n",
      "Epoch 49/100\n",
      "19903/19903 [==============================] - 3s 175us/step - loss: 1.2070\n",
      "Epoch 50/100\n",
      "19903/19903 [==============================] - 3s 161us/step - loss: 1.2038\n",
      "Epoch 51/100\n",
      "19903/19903 [==============================] - 4s 189us/step - loss: 1.1969\n",
      "Epoch 52/100\n",
      "19903/19903 [==============================] - 4s 178us/step - loss: 1.1907\n",
      "Epoch 53/100\n",
      "19903/19903 [==============================] - 4s 178us/step - loss: 1.1752\n",
      "Epoch 54/100\n",
      "19903/19903 [==============================] - 3s 174us/step - loss: 1.1731\n",
      "Epoch 55/100\n",
      "19903/19903 [==============================] - 4s 194us/step - loss: 1.1567\n",
      "Epoch 56/100\n",
      "19903/19903 [==============================] - 4s 186us/step - loss: 1.1500\n",
      "Epoch 57/100\n",
      "19903/19903 [==============================] - 4s 180us/step - loss: 1.1421\n",
      "Epoch 58/100\n",
      "19903/19903 [==============================] - 3s 172us/step - loss: 1.1250\n",
      "Epoch 59/100\n",
      "19903/19903 [==============================] - 3s 174us/step - loss: 1.1209\n",
      "Epoch 60/100\n",
      "19903/19903 [==============================] - 3s 175us/step - loss: 1.1122\n",
      "Epoch 61/100\n",
      "19903/19903 [==============================] - 3s 174us/step - loss: 1.1089\n",
      "Epoch 62/100\n",
      "19903/19903 [==============================] - 4s 180us/step - loss: 1.1066\n",
      "Epoch 63/100\n",
      "19903/19903 [==============================] - 3s 175us/step - loss: 1.0910\n",
      "Epoch 64/100\n",
      "19903/19903 [==============================] - 4s 179us/step - loss: 1.0951\n",
      "Epoch 65/100\n",
      "19903/19903 [==============================] - 4s 181us/step - loss: 1.0832\n",
      "Epoch 66/100\n",
      "19903/19903 [==============================] - 4s 185us/step - loss: 1.0774\n",
      "Epoch 67/100\n",
      "19903/19903 [==============================] - 4s 187us/step - loss: 1.0653\n",
      "Epoch 68/100\n",
      "19903/19903 [==============================] - 4s 193us/step - loss: 1.0724\n",
      "Epoch 69/100\n",
      "19903/19903 [==============================] - 4s 193us/step - loss: 1.0478\n",
      "Epoch 70/100\n",
      "19903/19903 [==============================] - 4s 178us/step - loss: 1.0390\n",
      "Epoch 71/100\n",
      "19903/19903 [==============================] - 4s 183us/step - loss: 1.0404\n",
      "Epoch 72/100\n",
      "19903/19903 [==============================] - 3s 162us/step - loss: 1.0422\n",
      "Epoch 73/100\n",
      "19903/19903 [==============================] - 4s 212us/step - loss: 1.0482\n",
      "Epoch 74/100\n",
      "19903/19903 [==============================] - 4s 180us/step - loss: 1.0298\n",
      "Epoch 75/100\n",
      "19903/19903 [==============================] - 3s 175us/step - loss: 1.0116\n",
      "Epoch 76/100\n",
      "19903/19903 [==============================] - 4s 179us/step - loss: 1.0198\n",
      "Epoch 77/100\n",
      "19903/19903 [==============================] - 4s 198us/step - loss: 1.0065\n",
      "Epoch 78/100\n",
      "19903/19903 [==============================] - 4s 202us/step - loss: 1.0195\n",
      "Epoch 79/100\n",
      "19903/19903 [==============================] - 4s 201us/step - loss: 0.9992\n",
      "Epoch 80/100\n",
      "19903/19903 [==============================] - 4s 187us/step - loss: 0.9947\n",
      "Epoch 81/100\n",
      "19903/19903 [==============================] - 3s 171us/step - loss: 0.9895\n",
      "Epoch 82/100\n",
      "19903/19903 [==============================] - 3s 175us/step - loss: 0.9872\n",
      "Epoch 83/100\n",
      "19903/19903 [==============================] - 3s 169us/step - loss: 0.9724\n",
      "Epoch 84/100\n",
      "19903/19903 [==============================] - 3s 174us/step - loss: 0.9806\n",
      "Epoch 85/100\n",
      "19903/19903 [==============================] - 3s 170us/step - loss: 0.9743\n",
      "Epoch 86/100\n",
      "19903/19903 [==============================] - 3s 166us/step - loss: 0.9573\n",
      "Epoch 87/100\n",
      "19903/19903 [==============================] - 3s 165us/step - loss: 0.9646\n",
      "Epoch 88/100\n",
      "19903/19903 [==============================] - 3s 163us/step - loss: 0.9737\n",
      "Epoch 89/100\n",
      "19903/19903 [==============================] - 3s 159us/step - loss: 0.9579\n",
      "Epoch 90/100\n",
      "19903/19903 [==============================] - 3s 164us/step - loss: 0.9506\n",
      "Epoch 91/100\n",
      "19903/19903 [==============================] - 3s 161us/step - loss: 0.9582\n",
      "Epoch 92/100\n",
      "19903/19903 [==============================] - 4s 183us/step - loss: 0.9526\n",
      "Epoch 93/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19903/19903 [==============================] - 3s 175us/step - loss: 0.9585\n",
      "Epoch 94/100\n",
      "19903/19903 [==============================] - 3s 170us/step - loss: 0.9546\n",
      "Epoch 95/100\n",
      "19903/19903 [==============================] - 3s 167us/step - loss: 0.9454\n",
      "Epoch 96/100\n",
      "19903/19903 [==============================] - 3s 163us/step - loss: 0.9261\n",
      "Epoch 97/100\n",
      "19903/19903 [==============================] - 3s 164us/step - loss: 0.9523\n",
      "Epoch 98/100\n",
      "19903/19903 [==============================] - 3s 157us/step - loss: 0.9354\n",
      "Epoch 99/100\n",
      "19903/19903 [==============================] - 4s 181us/step - loss: 0.9388\n",
      "Epoch 100/100\n",
      "19903/19903 [==============================] - 4s 202us/step - loss: 0.9323\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1818f8ff60>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "### Constructing an RNN model with keras ###\n",
    "# Input with shape of a single training instance, we\n",
    "# do not take into account the number of training examples\n",
    "xin = Input(shape=xtrain.shape[1:])\n",
    "# A recurrent neural network cell with tanh activation function\n",
    "# and 256 units\n",
    "x = SimpleRNN(256, activation=\"tanh\")(xin)\n",
    "# The output of each training example, after feeding\n",
    "# sqlen characters is a desne \"feedforward\" neural network\n",
    "# with softax activation function\n",
    "x = Dense(ytrain.shape[1], activation=\"softmax\")(x)\n",
    "\n",
    "model = Model(inputs=xin, outputs=x)\n",
    "model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\")\n",
    "model.fit(xtrain, ytrain, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed:'us\\nhuabeis'\n",
      "aurus\n",
      "araucanoraptor\n",
      "aratososaurus\n",
      "araucanoraptor\n",
      "aratososaurus\n",
      "araucanoraptor\n",
      "aratososaurus\n",
      "araucanoraptor\n",
      "aratososaurus\n",
      "araucanoraptor\n",
      "aratososaurus\n",
      "...Done...\n"
     ]
    }
   ],
   "source": [
    "# pick a random seed\n",
    "start = np.random.randint(0, len(xtrain)-1)\n",
    "pattern = xtrain[start].ravel()\n",
    "print(\"Seed:\", end=\"\")\n",
    "print(repr(''.join([ix_to_char[value] for value in pattern])))\n",
    "# generate characters\n",
    "for i in range(150):\n",
    "    x = np.reshape(pattern, (1, len(pattern), 1))\n",
    "    prediction = model.predict(x, verbose=0)\n",
    "    index = np.argmax(prediction)\n",
    "    result = ix_to_char[index]\n",
    "    seq_in = [ix_to_char[value] for value in pattern]\n",
    "    print(result, end=\"\")\n",
    "    pattern = np.append(pattern, index)\n",
    "    pattern = pattern[1:len(pattern)]\n",
    "print(\"\\n...Done...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training dataset: Outputs\n",
    "dinos_list = [[char_to_ix[c] for c in d + \"\\n\"] for d in dinos.split()]\n",
    "maxlen =  len(max(dinos_list, key=len))\n",
    "ndinos = len(dinos_list)\n",
    "nchars = len(characters)\n",
    "xtrain = np.zeros((ndinos, maxlen, nchars))\n",
    "ytrain = np.zeros((ndinos, maxlen, nchars))\n",
    "\n",
    "xtrain = np.zeros((ndinos, maxlen))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "xin = Input(shape=(maxlen,))\n",
    "x = SimpleRNN(10, activation=\"tanh\")(xin)\n",
    "x = Dense()\n",
    "x = Activation(\"softmax\")(x)\n",
    "\n",
    "model = Model(inputs=xin, outputs=x)\n",
    "model.compile(\"adam\", loss=\"categorical_crossentropy\")\n",
    "model.fit(xtrain, ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.03703704, 0.03703704, 0.03703704, 0.03703704, 0.03703704,\n",
       "       0.03703704, 0.03703704, 0.03703704, 0.03703704, 0.03703704,\n",
       "       0.03703704, 0.03703704, 0.03703704, 0.03703704, 0.03703704,\n",
       "       0.03703704, 0.03703704, 0.03703704, 0.03703704, 0.03703704,\n",
       "       0.03703704, 0.03703704, 0.03703704, 0.03703704, 0.03703704,\n",
       "       0.03703704, 0.03703704], dtype=float32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(np.zeros((1, maxlen, nchars)))[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
